# üîµ K-Means Clustering (Agrupamiento K-Medias)

**K-Means** es una t√©cnica de aprendizaje no supervisado utilizada para dividir un conjunto de datos en **K grupos (cl√∫steres)** distintos pero no superpuestos. El objetivo es que las observaciones dentro de cada grupo sean muy similares entre s√≠ y diferentes de las observaciones en otros grupos.

Para aplicarlo, es necesario **especificar previamente el n√∫mero de cl√∫steres K** que se desean encontrar.

---

## üéØ Objetivo de Minimizaci√≥n en Clustering

El prop√≥sito principal del algoritmo es que la **variaci√≥n interna de cada cl√∫ster** sea lo m√°s peque√±a posible. Se busca minimizar la **suma total de las variaciones internas** de los cl√∫steres.

Esto se expresa mediante la siguiente funci√≥n objetivo:

$\min_{C_1, \dots, C_K} \left\{ \sum_{k=1}^{K} W(C_k) \right\}$

Donde:

* $C‚ÇÅ, ..., C_K$ son los cl√∫steres formados.
* $W(C_k)$ representa la **variaci√≥n interna** del cl√∫ster * $C_k$.
* La suma total recorre todos los K cl√∫steres.

---

### üìè Definici√≥n de la Variaci√≥n Interna de un Cl√∫ster: W(C‚Çñ)

La variaci√≥n dentro de un cl√∫ster $W(C_k)$ se define como la suma de las **distancias euclidianas al cuadrado** entre cada punto del cl√∫ster y su centroide (la media de sus caracter√≠sticas)**(inercia)**:

$W(C_k) = \sum_{i \in C_k} \sum_{j=1}^{p} (x_{ij} - \bar{x}_{kj})^2$

Donde:

* $i ‚àà C_k$: Recorre todas las observaciones del cl√∫ster $C_k$.
* $j = 1,...,p$: Recorre todas las **p caracter√≠sticas** del conjunto de datos.
* $x_{ij}$: Valor de la caracter√≠stica **j** de la observaci√≥n **i**.
* $ùë•ÃÑ_{kj}$: Media de la caracter√≠stica **j** en el cl√∫ster $C_k$.

---

## ‚öôÔ∏è ¬øC√≥mo Funciona el Algoritmo?

Encontrar una soluci√≥n √≥ptima global para K-Means es computacionalmente dif√≠cil. Por eso, se utiliza un algoritmo **iterativo** que converge a un **√≥ptimo local**.

### Pasos del Algoritmo

1. **Inicializaci√≥n aleatoria**:
   Se asigna aleatoriamente un n√∫mero del 1 al K a cada observaci√≥n (asignaci√≥n inicial de cl√∫steres).

2. El Proceso Iterativo de K-Means

El algoritmo K-Means alterna entre dos pasos fundamentales hasta que converge (es decir, hasta que las asignaciones de cl√∫steres ya no cambian):

   1. **üß≠ Paso de Asignaci√≥n:**
        Cada punto de datos se asigna al **cl√∫ster cuyo centroide est√© m√°s cercano**, t√≠picamente utilizando la **distancia euclidiana** como medida de cercan√≠a.

   2. **üìç Paso de Actualizaci√≥n:**
        Se **recalcula la posici√≥n de cada centroide**. El nuevo centroide es simplemente la **media (promedio vectorial)** de todas las observaciones que fueron asignadas a ese cl√∫ster en el paso anterior.

Este proceso garantiza que la **variaci√≥n total dentro de los cl√∫steres disminuya (o al menos no aumente)** en cada iteraci√≥n, y finaliza cuando ya no hay cambios en las asignaciones: el algoritmo ha llegado a un **√≥ptimo local**.

---

## üß† Resumen del Algoritmo K-Means (Iterativo)

El algoritmo **K-Means** sigue un enfoque iterativo para encontrar grupos (cl√∫steres) en los datos. Su funcionamiento puede resumirse en los siguientes pasos:

1. **üìå Elegir K:**
   Se decide cu√°ntos **cl√∫steres (K)** se desean encontrar en los datos.

2. **üéØ Inicializar Centroides:**
   Se seleccionan **K puntos iniciales** como centroides. (Existen distintas estrategias para esto, como la inicializaci√≥n aleatoria o el m√©todo *k-means++*).

3. **üîÅ Repetir hasta convergencia:**

   * **Paso de Asignaci√≥n:**
     Cada punto se asigna al **cl√∫ster cuyo centroide est√© m√°s cercano**, generalmente usando la **distancia euclidiana**.

   * **Paso de Actualizaci√≥n:**
     Se **recalcula cada centroide** como la **media de todos los puntos asignados** a ese cl√∫ster.

4. **üõë Detener:**
   El algoritmo finaliza cuando:

   * Las asignaciones de los puntos ya no cambian, o
   * Los centroides apenas se mueven, o
   * Se alcanza un **n√∫mero m√°ximo de iteraciones**.

> üí° Este proceso garantiza que la variaci√≥n interna (dentro de los cl√∫steres) se reduzca en cada paso, hasta llegar a un **√≥ptimo local**.

---
### ‚ö†Ô∏è Precauci√≥n: √ìptimos Locales

El resultado final depende de la asignaci√≥n aleatoria inicial. El algoritmo puede converger a diferentes soluciones dependiendo del punto de partida.

**Soluci√≥n recomendada:**

* Ejecutar el algoritmo m√∫ltiples veces con diferentes inicializaciones.
* Elegir la soluci√≥n con menor variaci√≥n total.

üìå En R: la funci√≥n `kmeans()` incluye el argumento `nstart`.
Se recomienda usar valores altos como `nstart = 20` o `nstart = 50`.

---

## ‚ùì La Importancia de Elegir K

K-Means requiere que especifiques **de antemano** cu√°ntos cl√∫steres deseas encontrar (**valor de K**).
Elegir el valor √≥ptimo de K es un problema complejo y se analiza con m√°s profundidad en la secci√≥n **"Practical Issues in Clustering"**.


## üìö Aprendizaje Supervisado vs No Supervisado

### 1. ‚úÖ Aprendizaje Supervisado

(Lo que hemos hecho hasta ahora)

En el **aprendizaje supervisado**, el algoritmo aprende a partir de datos que **ya est√°n etiquetados**.
Veamos un ejemplo pr√°ctico:

> Supongamos que utilizamos algoritmos como **Regresi√≥n Log√≠stica**, **SVC** o **Random Forest** con el conjunto de datos de **c√°ncer de mama**.

#### üéØ Objetivo:

Predecir si un tumor es **maligno** o **benigno**.

#### üß† ¬øCon qu√© informaci√≥n entrena el modelo?

* **Caracter√≠sticas (X):** radio medio, textura media, etc.
* **Variable de respuesta (y):** si el tumor es benigno (0) o maligno (1).

El algoritmo **aprende la relaci√≥n entre X e y** y la utiliza para hacer predicciones sobre nuevos datos en los que solo conocemos X.

#### üß© ¬øQu√© significa ‚Äúsupervisado‚Äù?

Significa que el algoritmo **recibe una ‚Äúsupervisi√≥n‚Äù** en forma de etiquetas correctas (y). Aprende comparando sus predicciones con las respuestas verdaderas durante el entrenamiento.

> üìå En el caso del c√°ncer de mama, la variable de respuesta es la columna `target` del dataset `load_breast_cancer`, que contiene los valores 0 y 1.

---

### 2. üîç Aprendizaje No Supervisado

(Aqu√≠ entra K-Means)

En el **aprendizaje no supervisado**, **no se proporcionan etiquetas ni categor√≠as**.
El objetivo es **descubrir patrones ocultos o estructura natural** en los datos.

#### üì¶ Imagina:

Tienes un conjunto de datos sobre clientes de una tienda:

* Edad, gasto promedio, frecuencia de compra, productos visualizados...

Pero **no tienes una columna** que diga si el cliente es "VIP", "ocasional", o "en riesgo".
**No hay una variable de respuesta.**

#### üîç ¬øQu√© hace el algoritmo?

* Analiza las **caracter√≠sticas (X)**.
* Intenta **encontrar grupos o patrones** sin saber de antemano cu√°ntos grupos existen ni c√≥mo deber√≠an estar etiquetados.

> No hay una ‚Äúrespuesta correcta‚Äù para aprender. El modelo **descubre por s√≠ mismo** la estructura de los datos.

---

### üß≠ Conexi√≥n con el Clustering

El **clustering** (agrupamiento), como el algoritmo **K-Means**, es un ejemplo cl√°sico de aprendizaje no supervisado.

#### üéØ Objetivo del Clustering:

Agrupar observaciones **sin etiquetas previas**, de forma que:

* Las observaciones **dentro del mismo cl√∫ster** sean muy similares entre s√≠.
* Las observaciones de **cl√∫steres distintos** sean muy diferentes entre s√≠.

> El algoritmo **K-Means no sabe de antemano** cu√°ntos grupos hay ni a qu√© grupo pertenece cada punto.
> Su tarea es **descubrir** esa estructura.

---

## üéØ K-Means++: Una Inicializaci√≥n M√°s Inteligente

La **inicializaci√≥n aleatoria simple** en K-Means puede llevar a soluciones pobres o a convergencia lenta. Para mejorar esto, se utiliza una t√©cnica llamada **K-Means++**, que busca seleccionar los centroides iniciales de forma m√°s estrat√©gica.

### ¬øC√≥mo funciona K-Means++?

1. **üß© Elegir el primer centroide aleatoriamente** de entre los puntos de datos.
2. **üéØ Elegir los siguientes centroides** uno a uno, seleccionando preferentemente los puntos que est√°n **m√°s lejos** de los centroides ya elegidos.

   * La probabilidad de seleccionar un nuevo punto como centroide es **proporcional al cuadrado de su distancia** al centroide m√°s cercano.
3. Este procedimiento **distribuye mejor los centroides iniciales**, evitando que todos caigan muy cerca unos de otros.

### üß† ¬øPor qu√© es mejor?

* Mejora la **estabilidad** del algoritmo.
* Aumenta la **probabilidad de converger a una mejor soluci√≥n** (√≥ptimo m√°s cercano al global).
* Reduce la **sensibilidad a la aleatoriedad inicial**.

> üí° **Importante:**
> Los centroides iniciales no son los definitivos. Solo son **puntos de partida** para el proceso iterativo. A partir de ellos, el algoritmo K-Means se encarga de refinar su ubicaci√≥n mediante asignaciones y actualizaciones sucesivas.

---



## M√©todos principales de un objeto `KMeans`

| M√©todo                 | Descripci√≥n                                                              |
| ---------------------- | ------------------------------------------------------------------------ |
| `fit(X)`               | Ajusta el modelo a los datos `X` (entrena el clustering).                |
| `fit_predict(X)`       | Ajusta el modelo y devuelve las etiquetas de cl√∫ster para `X`.           |
| `predict(X)`           | Asigna nuevos datos `X` a los clusters ya entrenados.                    |
| `fit_transform(X)`     | Ajusta el modelo y devuelve la distancia de cada punto a cada centroide. |
| `transform(X)`         | Devuelve la distancia de los datos `X` a los centroides ya entrenados.   |
| `set_params(**params)` | Establece par√°metros del modelo.                                         |
| `get_params()`         | Obtiene los par√°metros actuales del modelo.                              |

---

## Atributos importantes (despu√©s de hacer `.fit`)

| Atributo           | Descripci√≥n                                                                     |
| ------------------ | ------------------------------------------------------------------------------- |
| `cluster_centers_` | Coordenadas de los centroides encontrados.                                      |
| `labels_`          | Etiquetas asignadas a cada muestra (cl√∫ster).                                   |
| `inertia_`         | Suma de las distancias al cuadrado a los centroides (criterio de optimizaci√≥n). |
| `n_iter_`          | N√∫mero de iteraciones que tom√≥ el algoritmo para converger.                     |

---

### Ejemplo r√°pido:

```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

print(kmeans.cluster_centers_)  # centroides
print(kmeans.labels_)           # etiquetas
print(kmeans.inertia_)          # suma de distancias al cuadrado
```


## üìä Inercia y el M√©todo del Codo

### üìå ¬øQu√© es la Inercia?

La **inercia** es una m√©trica utilizada en K-Means para evaluar qu√© tan bien se han agrupado los datos. Se define como:

> **La suma de las distancias al cuadrado** de cada punto a su **centroide asignado**.

* Valores **m√°s bajos** de inercia indican que los **clusters son m√°s compactos** y que los puntos est√°n m√°s cerca de su centroide.
* Sin embargo, la inercia **siempre disminuye** al aumentar el n√∫mero de clusters K (o, en el peor caso, permanece igual), lo que puede inducir a error si solo se minimiza esta m√©trica.

---

### üìà Gr√°fica Inercia vs. N√∫mero de Clusters (K)

Al graficar la inercia en funci√≥n de diferentes valores de K, ocurre lo siguiente:

* Inicialmente, la inercia **disminuye r√°pidamente** conforme agregamos m√°s clusters.
* Luego, la curva **comienza a aplanarse**: los beneficios de seguir aumentando K se vuelven **marginales**.
* Si K fuera igual al n√∫mero total de puntos, la inercia ser√≠a **cero** (cada punto ser√≠a su propio cluster).

---

# ü¶æ El M√©todo del Codo

El objetivo es encontrar un **punto √≥ptimo** donde obtener una buena segmentaci√≥n sin usar m√°s clusters de los necesarios.

* Este punto se llama **"el codo"** de la curva.
* Representa un **equilibrio entre compacidad y simplicidad**.
* M√°s all√° del codo, **a√±adir m√°s clusters no mejora significativamente la inercia** (ley de rendimientos decrecientes).

> üß† **Consejo pr√°ctico**:
> El m√©todo del codo es una gu√≠a visual. Siempre es buena idea **complementarlo con otros m√©todos** o con conocimiento del dominio para decidir el valor √≥ptimo de K.

## üéØ‚ú® **C√≥mo usar el M√©todo del Codo para encontrar el mejor K en KMeans**

Imaginemos que queremos agrupar datos en grupos (clusters) y no sabemos cu√°ntos grupos son los ideales. El **M√©todo del Codo** nos ayuda a descubrir ese n√∫mero perfecto, llamado **K**.

---

### Paso a paso para encontrar el "codo" ü¶∂

1. **Elegir un rango de K para probar**

   * Por ejemplo, probamos desde 1 hasta 10 grupos.
   * As√≠ evaluamos varias opciones y no nos quedamos con una sola suposici√≥n.

2. **Probar cada valor de K**

   * Para cada n√∫mero de grupos (K):

     * Creamos un modelo KMeans con esa cantidad de grupos.

       > *Consejo:* Usamos `init='k-means++'` y `random_state=42` para que los resultados sean consistentes cada vez.
     * Ajustamos el modelo a nuestros datos (X).
     * Calculamos la **inercia**, que es una medida de qu√© tan bien los datos est√°n agrupados:

       * Inercia baja = grupos compactos y bien definidos.
     * Guardamos el valor de inercia para cada K.

3. **Hacer un gr√°fico con los resultados**

   * En el eje horizontal (X) ponemos los valores de K (1, 2, 3, ‚Ä¶).
   * En el eje vertical (Y) ponemos la inercia calculada para cada K.

4. **Buscar el "codo" en la gr√°fica**

   * El "codo" es el punto donde la disminuci√≥n de la inercia se vuelve menos pronunciada, formando una curva parecida a un brazo doblado.
   * Ese punto indica que agregar m√°s grupos no mejora mucho la agrupaci√≥n.
   * Elegir ese K es la mejor opci√≥n para balancear simplicidad y precisi√≥n.

---

üé® **Ejemplo simple:**
Imagin√° que tir√°s bolas en cajas: al principio, cada caja est√° muy llena y juntar m√°s bolas en menos cajas mejora mucho. Pero llega un momento en que agregar m√°s cajas no ayuda casi nada a ordenar mejor las bolas. Ese momento es el "codo".

---

üìå **Recordatorio r√°pido:**

* El M√©todo del Codo es como un detective que busca el punto justo donde dejar de dividir en grupos.
* Usar la gr√°fica es clave, porque no hay f√≥rmula m√°gica: ¬°es cuesti√≥n de observar!

---
# üé®üîç **Entendiendo el Coeficiente de Silueta para evaluar grupos (clusters)**

Cuando agrupamos datos, queremos saber qu√© tan bien est√°n organizados esos grupos. El **Coeficiente de Silueta** es una forma simple y visual de medir esto para cada punto y para todo el conjunto.

---

### ¬øQu√© es el Coeficiente de Silueta? ü§î

* Es un n√∫mero que va desde **‚àí1 hasta +1**.
* Nos dice qu√© tan bien un punto est√° ubicado en su grupo comparado con otros grupos cercanos.

---

### ¬øQu√© significan los valores?

- **üü¢ Cerca de +1:**  
  El punto est√° muy bien ubicado dentro de su grupo y lejos de otros grupos. ¬°Perfecto! üéØ

- **üü° Cerca de 0:**  
  El punto est√° justo en el borde, entre dos grupos. No est√° claro a cu√°l pertenece. ü§∑

- **üî¥ Cerca de ‚àí1:**  
  El punto probablemente fue asignado al grupo equivocado, porque est√° m√°s cerca de otro grupo diferente. üö©


---

### ¬øQu√© mide el Coeficiente de Silueta para cada punto?

Se fija en dos cosas importantes:

1. **Cohesi√≥n dentro del grupo** $a_i$

   * Qu√© tan cerca est√° el punto de los otros puntos de su propio grupo.
   * Se calcula como la distancia promedio entre el punto $i$ y todos los dem√°s puntos en el mismo cluster.
   * Si $a_i$ es peque√±o, el punto est√° bien "metido" en su grupo.

2. **Separaci√≥n entre grupos** $b_i$

   * Qu√© tan lejos est√° el punto $i$ de los puntos del cluster vecino m√°s cercano que no es el suyo.
   * Se calcula como la distancia promedio m√°s peque√±a entre $i$ y todos los puntos de otro cluster.
   * Si $b_i$ es grande, el cluster del punto est√° bien separado de los dem√°s.

---

### La f√≥rmula del Coeficiente de Silueta para un punto $i$

$$
s_i = \frac{b_i - a_i}{\max(a_i, b_i)}
$$

* Si $b_i \gg a_i$, entonces $s_i \to +1$ (ideal, buen punto).
* Si $a_i \gg b_i$, entonces $s_i \to -1$ (mala asignaci√≥n).
* Si $a_i \approx b_i$, entonces $s_i \approx 0$ (punto en el l√≠mite).

---

### Silhouette Score general para todo el conjunto

El **Silhouette Score** para un n√∫mero dado de clusters $K$ es el promedio de los coeficientes de silueta de todos los puntos:

$$
S = \frac{1}{N} \sum_{i=1}^N s_i
$$

Donde:

* $N$ es el n√∫mero total de puntos.
* Un $S$ cercano a +1 indica clusters bien definidos.
* Un $S$ cercano a 0 o negativo indica agrupamientos pobres o mal definidos.

---

### üéØ Resumen f√°cil:

* **Alta cohesi√≥n ($a_i$ peque√±o) + buena separaci√≥n ($b_i$ grande) = buen clustering**
* El Coeficiente de Silueta nos da un n√∫mero para comprobarlo.
* Es una herramienta visual y num√©rica para decidir cu√°ntos grupos elegir y si est√°n bien formados.

---

üí° **Ejemplo simple:**
Imagin√° que est√°s en un recreo con varios grupos de amigos.

* Si est√°s muy cerca de tus amigos y lejos de otros grupos, tu silueta es cercana a +1.
* Si est√°s justo en medio, tu silueta est√° cerca de 0.
* Si est√°s m√°s cerca de otro grupo que del tuyo, tu silueta es negativa, y deber√≠as cambiar de grupo.

---


# üß© Implementando el C√°lculo del Coeficiente de Silueta

El **Coeficiente de Silueta** nos ayuda a elegir el n√∫mero ideal de grupos (K) cuando usamos el m√©todo de agrupamiento KMeans. El proceso es parecido al del M√©todo del Codo, pero con algunas diferencias importantes.

---

### Paso 1: Definir un rango de valores para K

* No podemos usar $K=1$ porque el coeficiente de silueta compara clusters entre s√≠.
* Por eso, probamos desde $K=2$ hasta un m√°ximo razonable, por ejemplo $K=10$.

---

### Paso 2: Iterar para cada valor de K

Para cada $K$ dentro del rango definido:

1. Crear y entrenar un modelo KMeans con:

   * `n_clusters=K`
   * `init='k-means++'` (mejor inicializaci√≥n)
   * `n_init='auto'` (n√∫mero de inicializaciones para estabilidad)
   * `random_state=42` (para reproducibilidad)

2. Obtener las etiquetas de grupo que asigna el modelo a cada punto (con `model.labels_` o `model.fit_predict(X)`).

3. Calcular el **Silhouette Score promedio** usando los datos y etiquetas con la funci√≥n `silhouette_score` de `sklearn.metrics`.

4. Guardar ese Silhouette Score para ese valor de $K$.

---

### Paso 3: Graficar y elegir el mejor K

* Hacer un gr√°fico de l√≠nea con:

  * Eje X: valores de $K$
  * Eje Y: Silhouette Score promedio para cada $K$

* Buscar el **valor de $K$ que tenga el puntaje m√°s alto**, porque indica la mejor agrupaci√≥n seg√∫n la silueta.

---

‚ú® **As√≠, con este proceso podemos elegir un n√∫mero de clusters que agrupe bien los datos, con grupos compactos y separados.**

---

# üöÄ Aplicaci√≥n de K-Means y M√©todos de Evaluaci√≥n en Datasets Sint√©ticos y Reales


## üìä 1. Aplicaci√≥n para el Dataset `make_blobs`

### Paso 1: Cargar y visualizar los datos

* Cargamos los datos del dataset sint√©tico `make_blobs` en variables `X` (caracter√≠sticas) y `y_true` (etiquetas reales).
* Imprimimos los puntos coloreados para distinguir visualmente los grupos reales.

### Paso 2: Entrenar K-Means con $n\_clusters=3$

* Elegimos arbitrariamente $K=3$ para probar.
* Entrenamos el modelo y extraemos:

  * `y_kmeans_pred = kmeans_model.labels_`: etiquetas asignadas a cada muestra.
  * `centroides_kmeans = kmeans_model.cluster_centers_`: coordenadas de los centroides (centros) de los grupos.
  * `inertia_kmeans = kmeans_model.inertia_`: suma de distancias cuadradas entre puntos y sus centroides (menor inercia = grupos m√°s compactos).

### Paso 3: Visualizar resultados

* Graficamos los puntos con colores seg√∫n su grupo asignado por K-Means.
* Mostramos los centroides con marcadores rojos grandes.
* Este gr√°fico ayuda a ver si los grupos est√°n bien separados y centrados.

![Datos y centroides KMeans para make\_blobs](<Data generated for make_blobs data.png>)
![Clusters y centroides KMeans make\_blobs](<Kmeans Clustering with Centroids for Makeblobs data.png>)

---

## üìà 2. Aplicaci√≥n del M√©todo del Codo (`Elbow method`)

* Probamos diferentes valores de $K$ para K-Means (por ejemplo, de 1 a 10).
* Calculamos la inercia para cada $K$ (suma de distancias cuadradas).
* Graficamos la inercia vs. $K$.
* Buscamos el ‚Äúcodo‚Äù en la curva: el punto donde a√±adir m√°s grupos no reduce mucho la inercia.

![M√©todo del Codo para make\_blobs](<Elbow Method for Makeblobs data.png>)

---

## üìê 3. C√°lculo del Coeficiente de Silueta

* Calculamos el coeficiente de silueta para valores de $K$ desde 2 en adelante.
* Este coeficiente mide la calidad del agrupamiento: qu√© tan bien cada punto encaja en su grupo y qu√© tan separado est√° de otros grupos.
* Valores:

  * üîπ Cerca de 1: excelente agrupamiento (puntos bien dentro de su grupo).
  * üî∏ Cerca de 0: puntos en el l√≠mite entre grupos.
  * üîª Cerca de -1: puntos posiblemente asignados mal.

![Coeficiente de Silueta para make\_blobs](<Silhouette Coefficient Method for Makeblobs set.png>)

---

## üå∏ 4. Aplicaci√≥n para el Dataset Iris

### Paso 1: Cargar datos

* Cargamos las caracter√≠sticas y etiquetas reales de las especies del dataset Iris.
* Imprimimos dimensiones y nombres para conocer el dataset.

### Paso 2: M√©todo del Codo en Iris

* Entrenamos modelos K-Means con $K = 1$ a $10$.
* Calculamos inercia para cada modelo y graficamos.
* Identificamos el codo para elegir $K$ √≥ptimo.

![M√©todo del Codo para Iris](<Elbow method for Iris Data.png>)

### Paso 3: Coeficiente de Silueta en Iris

* Calculamos y graficamos el coeficiente de silueta para varios $K$.
* Valor m√°ximo sugiere el mejor n√∫mero de grupos para Iris.

![Coeficiente de Silueta para Iris](<Silhouette Coefficient Method for Iris Data.png>)

---

## üéØ 5. Visualizaci√≥n y Comparaci√≥n de Resultados en Iris

* Entrenamos K-Means con $K=3$ (n√∫mero real de especies).

* Obtenemos etiquetas predichas, centroides y inercia.

* Graficamos dos subplots:

  1. Puntos coloreados seg√∫n etiquetas predichas por K-Means + centroides en rojo.
  2. Puntos coloreados seg√∫n etiquetas reales de especies.

* Esto permite comparar visualmente la agrupaci√≥n del modelo con las clases reales.

![Comparaci√≥n Clusters Iris](<Comparation Clusters.png>)

---

‚ú® **Resumen:**
Este recorrido muestra c√≥mo aplicar K-Means y evaluar la cantidad √≥ptima de grupos con m√©todos visuales y m√©tricas cuantitativas, usando datasets sint√©ticos (`make_blobs`) y reales (Iris). Los gr√°ficos y m√©tricas ayudan a entender y validar los resultados.

---