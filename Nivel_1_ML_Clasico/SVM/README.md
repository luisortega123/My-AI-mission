# ğŸ§  ImplementaciÃ³n de SVM con Scikit-learn

## âœ… DefiniciÃ³n de SVM:
Una MÃ¡quina de Vectores de Soporte (SVM) es un modelo de aprendizaje supervisado que encuentra el hiperplano Ã³ptimo que maximiza el margen entre las clases en un conjunto de datos etiquetados.

## âœ‚ï¸ Clasificador de MÃ¡ximo Margen

El **Clasificador de MÃ¡ximo Margen** es el nÃºcleo del algoritmo **SVM (Support Vector Machine)**. Su misiÃ³n es clara:
ğŸ‘‰ **Encontrar el hiperplano que separa dos clases con el mayor margen posible**, es decir, con la mÃ¡xima â€œzona de seguridadâ€ entre los puntos de cada clase.

---

## ğŸ”µğŸ”´ AnalogÃ­a intuitiva: separaciÃ³n entre dos grupos

Imagina dos grupos de puntos en un plano:

* ğŸ”´ Grupo de puntos **rojos**
* ğŸ”µ Grupo de puntos **azules**

Queremos trazar una **lÃ­nea de separaciÃ³n** que divida ambos grupos correctamente. En SVM, esta lÃ­nea se llama:

### âœ³ï¸ **Hiperplano Separador**

> Aunque en 2D es una lÃ­nea, en dimensiones superiores es un **hiperplano** (una superficie que generaliza la nociÃ³n de lÃ­nea recta).

Este **hiperplano** funciona como una **navaja** que corta el espacio en dos mitades.
ğŸ›¡ï¸ Cuanto mÃ¡s lejos estÃ© de los puntos cercanos de cada clase, mÃ¡s **robusta** serÃ¡ esta separaciÃ³n.


## ğŸ§± Â¿QuÃ© son las fronteras de decisiÃ³n o el Hiperplano en SVM?

### ğŸ“Œ DefiniciÃ³n sencilla:

> **Una frontera de decisiÃ³n es la lÃ­nea (o superficie) que separa las distintas clases en los datos.**

En el caso de una **SVM lineal en 2D**, es una **lÃ­nea recta**.
En una SVM no lineal (usando *kernels*), puede ser una **curva, superficie o forma mÃ¡s compleja**.

---

## ğŸ¯ Â¿Por quÃ© es importante?

La **frontera de decisiÃ³n es lo que el modelo aprende**. Su objetivo es:

* Separar las clases de forma que **se maximice el margen** entre ellas.
* **Predecir la clase de nuevos puntos** segÃºn de quÃ© lado de la frontera caen.

---

## ğŸ–¼ï¸ IntuiciÃ³n visual (2D)

ImaginÃ¡ dos grupos de puntos:

ğŸ”´ Rojos
ğŸ”µ Azules

Una SVM encuentra la **lÃ­nea (hiperplano en general)** que los separa **dejando el mayor espacio posible entre ambos grupos**.

AsÃ­:

```
ğŸ”´ğŸ”´ğŸ”´         ğŸ”µğŸ”µğŸ”µ
ğŸ”´ğŸ”´ğŸ”´  |     ğŸ”µğŸ”µğŸ”µ
ğŸ”´ğŸ”´ğŸ”´         ğŸ”µğŸ”µğŸ”µ
        â†‘
     Frontera
     de decisiÃ³n
```

* Todo lo que queda a la izquierda â†’ se predice como clase roja.
* Todo lo que queda a la derecha â†’ se predice como clase azul.

---

## ğŸ§  CaracterÃ­sticas clave de la frontera en SVM

| CaracterÃ­stica                    | ExplicaciÃ³n                                                              |
| --------------------------------- | ------------------------------------------------------------------------ |
| **Lineal o no lineal**            | Depende del kernel: lineal â†’ hiperplano; no lineal â†’ curva/superficie    |
| **Definida por vectores soporte** | Solo los puntos mÃ¡s cercanos (vectores soporte) influyen en su posiciÃ³n  |
| **MÃ¡ximo margen**                 | SVM la posiciona para maximizar la distancia con los puntos mÃ¡s cercanos |

---

## ğŸ” Frontera de decisiÃ³n vs. MÃ¡rgenes

* **Frontera de decisiÃ³n**: la lÃ­nea central que divide las clases.
* **MÃ¡rgenes**: las dos lÃ­neas paralelas que pasan por los vectores soporte (uno por cada clase).

```
   margen         frontera         margen
   -----        | (decisiÃ³n)        -----
ğŸ”´ğŸ”´ğŸ”´ğŸ”´        |                ğŸ”µğŸ”µğŸ”µğŸ”µ
```


---

## ğŸ“ Â¿QuÃ© es el **margen**?

ğŸ§© El **margen** es la distancia entre el hiperplano separador y los puntos mÃ¡s cercanos de cada clase.
Un margen amplio significa:

* âœ… Mayor tolerancia al ruido
* âœ… Mayor robustez ante nuevos datos
* âœ… Mejor generalizaciÃ³n

ğŸ¯ **El objetivo de SVM es maximizar este margen**.

---

## ğŸ§· Â¿QuÃ© son los **vectores de soporte**?

Los **vectores de soporte** son:

* ğŸ”º Los puntos **mÃ¡s cercanos** al hiperplano
* ğŸ§± Los que **definen los bordes del margen**
* ğŸ§­ Los que **determinan la posiciÃ³n exacta del hiperplano**

ğŸ› ï¸ Si mueves uno de estos puntos, el hiperplano y el margen cambiarÃ¡n.
Los demÃ¡s puntos no tienen influencia directa.

> ğŸ“Œ Son literalmente los que â€œsoportanâ€ (support) el hiperplano.

---

## ğŸ›£ï¸ VisualizaciÃ³n mental: una calle

Visualiza esto como una **calle**:

* ğŸŸ¢ El **hiperplano separador** es la **lÃ­nea central** de la calle
* ğŸŸ¥ y ğŸŸ¦ Los **vectores de soporte** estÃ¡n en **los bordes** de la calle
* ğŸ“ El **margen** es el **ancho total** de la calle

ğŸ“ El mejor hiperplano es el que estÃ¡ **a la misma distancia** de los puntos rojos y azules mÃ¡s cercanos.

---

## ğŸ§¾ En resumen

El **Clasificador de MÃ¡ximo Margen** busca:

âœ… Un **hiperplano** que separe correctamente las clases
ğŸ“ Que estÃ© **equidistante** de los vectores de soporte de cada clase
ğŸ“ Que **maximice el margen** para mayor estabilidad
ğŸ”§ Solo los vectores de soporte afectan su posiciÃ³n

ğŸ’¡ Esta estrategia ofrece una **separaciÃ³n Ã³ptima y robusta**, ideal para evitar errores con nuevos datos.



## ğŸŒ€ Â¿QuÃ© sucede si los datos son mÃ¡s complejos?

Imagina una nueva situaciÃ³n mÃ¡s desafiante:

* ğŸ”µ Tienes un grupo de **puntos azules en el centro** de tu hoja de papel.
* ğŸ”´ Y un grupo de **puntos rojos formando un anillo alrededor** de los puntos azules.

ğŸ¨ En este escenario, una simple lÃ­nea recta (hiperplano en 2D) **no puede separarlos** correctamente.

---

## ğŸ§  AquÃ­ es donde entra la **idea genial de los *Kernels*** âœ¨

Â¿QuÃ© pasarÃ­a si pudieras:

* ğŸ’« Tomar tus datos en ese plano 2D (la hoja de papel),
* ğŸ” Y **transformarlos** mÃ¡gicamente a un **espacio con mÃ¡s dimensiones**,
* Donde sÃ­ se puedan **separar con un plano recto**?

ğŸ© Eso es exactamente lo que hacen los **Kernels** en SVM.

---

## â›°ï¸ AnalogÃ­a Visual: Valle y Colina

Piensa en esto:

* ğŸ“„ En tu hoja 2D, un **cÃ­rculo** no se puede dividir con una lÃ­nea recta.
* Pero... Â¿y si los puntos del **centro (azules)** los pudieras **"elevar"** a una tercera dimensiÃ³n, como si estuvieran en una **colina**?
* Mientras tanto, los puntos del **anillo rojo** se mantienen abajo, en el **valle**.

ğŸ§± En ese espacio 3D, ahora puedes simplemente **cortar horizontalmente con un plano plano**,
y separar perfectamente los de la colina (azules) de los del valle (rojos).

> ğŸ” En 2D: separaciÃ³n imposible âŒ
> ğŸ›¸ En 3D con kernel: separaciÃ³n simple âœ”ï¸

---

## ğŸ” Â¿QuÃ© hace el **Kernel** entonces?

* Es una **funciÃ³n matemÃ¡tica** que transforma tus datos originales a un espacio de mayor dimensiÃ³n **sin que tÃº tengas que hacerlo manualmente**.
* Gracias a esta transformaciÃ³n, el algoritmo SVM puede encontrar un **hiperplano separador** incluso cuando los datos no eran separables antes.

ğŸ¯ Resultado:
SeparaciÃ³n lineal **en un espacio transformado**,
aunque en el espacio original **no lo parecÃ­a**.

---

## ğŸ’¬ En resumen

* ğŸ§© Algunos conjuntos de datos **no se pueden separar linealmente** en su forma original.
* ğŸ§  Los **kernels** permiten transformar esos datos a un **espacio donde sÃ­ lo son**.
* ğŸ§—â€â™‚ï¸ Como si **elevaras una parte del plano**, logrando una separaciÃ³n clara con un plano simple en esa nueva dimensiÃ³n.

---

## âœ¨ Esto es precisamente la magia de los **Kernels** en SVM:

### ğŸ”® TransformaciÃ³n ImplÃ­cita (el famoso *Kernel Trick*)

En lugar de tener que calcular manualmente nuevas coordenadas (por ejemplo, una altura `z` en el ejemplo de la colina ğŸ”ï¸), el **Kernel** es una funciÃ³n matemÃ¡tica que permite al **SVM** comportarse **como si** los datos ya estuvieran en un espacio transformado de mayor dimensiÃ³nâ€¦
Â¡pero **sin tener que calcular esas coordenadas explÃ­citamente**!

A esto se le conoce como el **`Kernel Trick`** ğŸ§™â€â™‚ï¸

> ğŸ’¡ Lo que hace el *Kernel Trick* es calcular **productos escalares** (medidas de similitud) **directamente en el espacio original**,
> de una forma que **equivale** a haber hecho las transformaciones complejas a otro espacio.

---

## ğŸ§ª Diferentes Tipos de Transformaciones (Kernels mÃ¡s comunes)

AsÃ­ como puedes imaginar diferentes maneras de â€œelevarâ€ o â€œdoblarâ€ tus datos,
tambiÃ©n existen distintos **tipos de kernels**, cada uno con su propia forma de transformaciÃ³n implÃ­cita:

---

### 1. ğŸ“ **Kernel Lineal**

* âœ… El mÃ¡s simple.
* No transforma nada: trabaja **en el espacio original**.
* Es el que usamos cuando los datos **ya son linealmente separables**.
* Ãštil para problemas simples donde una **lÃ­nea recta o un plano** bastan.

---

### 2. ğŸ“ **Kernel Polinomial**

* ğŸ§® Crea **combinaciones polinÃ³micas** de las variables originales.
* Ejemplo: transforma (xâ‚, xâ‚‚) en algo como (xâ‚, xâ‚‚, xâ‚Â², xâ‚‚Â², xâ‚Â·xâ‚‚).
* Esto permite generar **fronteras de decisiÃ³n curvas**: parÃ¡bolas, circunferencias, etc.
* ğŸ”„ Ideal para capturar relaciones **no lineales suaves**.

---

### 3. ğŸŒŠ **Kernel RBF (Radial Basis Function)**

* ğŸ¯ Es el mÃ¡s potente y mÃ¡s usado.
* Tiene una intuiciÃ³n parecida a una colina â›°ï¸:
  mide **la influencia de cada punto**, que **disminuye con la distancia** (como ondas en un estanque).
* Mapea los datos a un **espacio de dimensiones infinitas**. ğŸ˜®
* Permite que el SVM dibuje **fronteras muy complejas y precisas**.

---

## ğŸ¨ Â¿Entonces quÃ© hacen los Kernels?

Podemos pensarlos como:

> ğŸ§ª **Recetas matemÃ¡ticas** que le dicen al SVM cÃ³mo comparar puntos como si estuvieran en un espacio de mayor dimensiÃ³n.

Gracias a los kernels, el SVM puede encontrar una **frontera de decisiÃ³n lineal en un espacio invisible** y, cuando la proyectamos de vuelta a nuestro espacio original, vemos una **frontera curva o compleja**, como el **cÃ­rculo** que separa el centro del anillo.

---

## ğŸ§­ Resumen Visual

| Kernel        | Â¿Transforma los datos?    | Â¿CÃ³mo son las fronteras?            | Ideal para...                   |
| ------------- | ------------------------- | ----------------------------------- | ------------------------------- |
| ğŸ“ Lineal     | âŒ No                      | LÃ­neas rectas / planos              | Datos linealmente separables    |
| ğŸ“ Polinomial | âœ… SÃ­                      | Curvas suaves (parÃ¡bolas, cÃ­rculos) | RelaciÃ³n no lineal moderada     |
| ğŸŒŠ RBF        | âœ… SÃ­ (dimensiÃ³n infinita) | Fronteras complejas y adaptativas   | Problemas no lineales complejos |

---

# ğŸ› ï¸ ParÃ¡metros Clave en scikit-learn para SVM: C, kernel, degree, y gamma.
Cuando usamos SVC() de scikit-learn, hay varios parÃ¡metros importantes que determinan cÃ³mo se comporta el clasificador SVM. AquÃ­ te explico los mÃ¡s relevantes:

## ğŸ§ª ParÃ¡metro `kernel` en SVM (con `scikit-learn`)

> **Â¿QuÃ© le estÃ¡s diciendo al modelo cuando defines el parÃ¡metro `kernel`?**
> Le estÃ¡s diciendo **cÃ³mo transformar el espacio** para que una separaciÃ³n lineal sea posible... incluso si los datos **no lo son en el espacio original**.

---

### ğŸ§  Â¿QuÃ© es un *kernel*?

Un **kernel** es como una **receta matemÃ¡tica ğŸ§®** que define **cÃ³mo se calcula la similitud** entre puntos de datos.
DetrÃ¡s de escena, permite transformar (implÃ­citamente) los datos a un espacio de mayor dimensiÃ³n, **sin tener que hacer la transformaciÃ³n explÃ­cita**.
Eso es lo que se conoce como el famoso **kernel trick** ğŸª„.

---

### ğŸ“Œ Â¿QuÃ© opciones puedes darle a `kernel` en `scikit-learn`?

| Valor de `kernel`        | âœ¨ TransformaciÃ³n que aplica                         | ğŸ” CuÃ¡ndo usarlo                                                                             |
| ------------------------ | --------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| `"linear"`               | â– No transforma los datos. Usa el espacio original. | âœ… Ideal si tus datos ya son linealmente separables. RÃ¡pido y eficiente.                      |
| `"poly"`                 | ğŸ§© Aplica una transformaciÃ³n polinÃ³mica             | ğŸŒ€ Ãštil cuando las fronteras de decisiÃ³n tienen curvaturas suaves o patrones complejos.      |
| `"rbf"` (o `"gaussian"`) | ğŸŒŠ Usa funciones de base radial (como ondas)        | ğŸ”¥ Muy potente para datos no lineales. Puede crear fronteras muy complejas. Es el mÃ¡s comÃºn. |
| `"sigmoid"`              | ğŸ” Usa una funciÃ³n tipo tangente hiperbÃ³lica        | ğŸ§  Inspirado en redes neuronales. Poco usado, pero Ãºtil en contextos especÃ­ficos.            |
| `custom callable`        | ğŸ§  Puedes pasar tu propia funciÃ³n como kernel       | ğŸ‘¨â€ğŸ”¬ Para experimentaciÃ³n o necesidades muy particulares.                                   |

---

### ğŸ§© Â¿CÃ³mo lo usas en cÃ³digo?

```python
from sklearn.svm import SVC

# Ejemplo con kernel RBF (por defecto)
model = SVC(kernel='rbf')
```

---

### ğŸ“ ConclusiÃ³n rÃ¡pida

> El parÃ¡metro `kernel` define **la forma del mundo donde el SVM va a trabajar**.
> Elegir el kernel correcto es como elegir **los lentes adecuados** ğŸ‘“ para ver la estructura de tus datos.



## ParÃ¡metro C:
"C: ParÃ¡metro de regularizaciÃ³n (similar a 1/Î»). Un C pequeÃ±o implica mayor regularizaciÃ³n (margen mÃ¡s amplio, mÃ¡s errores de clasificaciÃ³n permitidos en el margen). Un C grande implica menor regularizaciÃ³n (margen mÃ¡s estrecho)."

### ğŸ§® ParÃ¡metro **C** en SVM: Controlando el equilibrio entre precisiÃ³n y generalizaciÃ³n

En Support Vector Machines, el parÃ¡metro **C** es como un **control deslizante** ğŸšï¸ que ajusta cuÃ¡nto le permitimos al modelo equivocarse durante el entrenamiento.

---

### âš–ï¸ Â¿QuÃ© significa ajustar **C**?

| Valor de **C**   | ğŸ” Comportamiento del modelo                                       | ğŸ¯ Consecuencias                                                                                                         |
| ---------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------ |
| ğŸ”¹ **C pequeÃ±o** | ğŸ”§ **Mayor regularizaciÃ³n**<br>ğŸ”„ Permite errores de clasificaciÃ³n | âœ… Mejor capacidad de generalizaciÃ³n<br>ğŸ“‰ Menor riesgo de *overfitting*<br>âš ï¸ Puede cometer mÃ¡s errores en entrenamiento |
| ğŸ”¸ **C grande**  | ğŸš« **Menor regularizaciÃ³n**<br>ğŸ” Penaliza fuertemente los errores | ğŸ¯ Alta precisiÃ³n en entrenamiento<br>ğŸ“ˆ Mayor riesgo de *overfitting*<br>âš ï¸ Puede memorizar ruido o *outliers*          |

---

### ğŸ§  Â¿CÃ³mo interpretarlo?

* ğŸ›¡ï¸ **C pequeÃ±o** â†’ El modelo busca un **margen amplio** aunque tenga que **ignorar algunos puntos** mal clasificados. MÃ¡s tolerancia, mÃ¡s robustez.
* ğŸ§· **C grande** â†’ El modelo intenta **clasificar todo perfectamente**, ajustando el margen para abarcar incluso casos extremos o ruidosos.

---

### ğŸ“ ConclusiÃ³n rÃ¡pida

> **C pequeÃ±o** = MÃ¡s tolerancia, mejor generalizaciÃ³n
> **C grande** = Menos tolerancia, mÃ¡s precisiÃ³n en entrenamiento (pero cuidado con el *overfitting*)

---

## ğŸ›ï¸ ParÃ¡metros `degree` y `gamma` en SVM

Estos parÃ¡metros controlan **la forma y complejidad** de la frontera de decisiÃ³n cuando usas ciertos tipos de kernel.

---

### ğŸ“ `degree` â€“ Grado del Polinomio (`kernel='poly'`)

Este parÃ¡metro **solo se aplica** si estÃ¡s utilizando el **kernel polinomial** (`'poly'`).

#### ğŸ§  Â¿QuÃ© hace?

Transforma tus datos al combinar y elevar las caracterÃ­sticas originales.
Por ejemplo, un vector como `(xâ‚, xâ‚‚)` se convierte en:
â†’ `(xâ‚, xâ‚‚, xâ‚Â², xâ‚‚Â², xâ‚Â·xâ‚‚, ...)`

#### ğŸ”¢ Â¿QuÃ© efecto tiene?

| Degree | Forma de la frontera | Ejemplo visual               |
| ------ | -------------------- | ---------------------------- |
| 1      | Lineal               | Una lÃ­nea recta              |
| 2      | CuadrÃ¡tica           | ParÃ¡bolas, cÃ­rculos, elipses |
| 3      | CÃºbica               | Ondas suaves o curvas        |
| >3     | Muy compleja         | Fronteras con muchas curvas  |

ğŸ“Œ **Cuanto mayor sea el `degree`**, mÃ¡s curvas y ondulaciones puede tener la frontera de decisiÃ³n. Pero cuidado: demasiada complejidad â†’ âš ï¸ sobreajuste(Overfitting).

---

### ğŸŒŒ `gamma` â€“ Alcance de la Influencia (`kernel='rbf'`)

#### ğŸ§  Â¿QuÃ© hace?

Controla **cuÃ¡nta influencia tiene cada punto de entrenamiento** sobre la forma de la frontera.
Es como si cada punto â€œirradiaraâ€ un campo alrededor de sÃ­ mismo:

* **Gamma alto** â†’ campo pequeÃ±o, muy localizado ğŸ”
* **Gamma bajo** â†’ campo grande, mÃ¡s extendido ğŸ›°ï¸

### ğŸ“ IntuiciÃ³n:
| Valor de `gamma`          | Significado         | Efecto                                                                                               |
| ------------------------- | ------------------- | ---------------------------------------------------------------------------------------------------- |
| **PequeÃ±o** (ej. `0.001`) | **Alcance grande**  | Cada punto afecta a muchos vecinos â†’ modelo mÃ¡s **suave**, generaliza mÃ¡s                            |
| **Grande** (ej. `10`)     | **Alcance pequeÃ±o** | Cada punto afecta solo a sÃ­ mismo â†’ modelo mÃ¡s **ajustado**, riesgo de **sobreajuste** (overfitting) |

---

#### ğŸ” Imagina esto:

* Cada punto "deforma" el espacio a su alrededor.
* Si gamma es alto, la deformaciÃ³n ocurre en un Ã¡rea pequeÃ±a.
* Si gamma es bajo, esa influencia llega mÃ¡s lejos.

#### ğŸ“Š Comparativo resumen:

| Gamma   | Alcance de Influencia ğŸŒ | Forma de la Frontera ğŸ§­ | Riesgo de Overfitting âš ï¸         |
| ------- | ------------------------ | ----------------------- | -------------------------------- |
| ğŸ”¹ Bajo | Amplio (global)          | Suave y generalizada    | Bajo (puede haber underfitting)  |
| ğŸ”¸ Alto | Localizado (fino)        | Muy ajustada al detalle | Alto (riesgo de memorizar ruido) |

---


| Kernel          | Usa `degree` | Usa `gamma` | Comentarios clave                                              |
| --------------- | ------------ | ----------- | -------------------------------------------------------------- |
| `'linear'`      | No           | No          | Kernel lineal, no usa ni `degree` ni `gamma`                   |
| `'poly'`        | SÃ­           | SÃ­          | Kernel polinomial, `degree` define el grado, `gamma` la escala |
| `'rbf'`         | No           | SÃ­          | Kernel RBF (radial), solo usa `gamma`                          |
| `'sigmoid'`     | No           | SÃ­          | Kernel sigmoide, usa `gamma` y `coef0`                         |
| `'precomputed'` | No           | No          | Se usa matriz de kernel precalculada, no usa estos parÃ¡metros  |
---

## âš™ï¸ ImplementaciÃ³n de un Clasificador SVC con `scikit-learn`

### 1. ğŸ“¥ ImportaciÃ³n del Dataset

Primero, importamos el dataset de **cÃ¡ncer de mama** usando `load_breast_cancer`. Luego, cargamos los datos en las variables `X` (caracterÃ­sticas) e `y` (etiquetas).

### 2. âœ‚ï¸ Â¿Por quÃ© dividir los datos?

Separar el dataset en **entrenamiento** y **prueba** tiene un propÃ³sito fundamental:

> ğŸ§ª **Evaluar la capacidad de generalizaciÃ³n del modelo.**

Esto nos permite saber si el modelo realmente aprendiÃ³ a clasificar correctamente o solo memorizÃ³ los datos.

---

### 3. ğŸ§ª DivisiÃ³n del Dataset

Usamos la funciÃ³n `train_test_split` para dividir los datos.
Nos devuelve 4 conjuntos:

* `X_train`: caracterÃ­sticas de entrenamiento
* `X_test`: caracterÃ­sticas de prueba
* `y_train`: etiquetas de entrenamiento
* `y_test`: etiquetas de prueba

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

#### âœ… ParÃ¡metros Ãºtiles:

* `test_size=0.2`: reserva el 20% de los datos para prueba.
* `random_state=42`: fija una **semilla** para garantizar que la divisiÃ³n sea **reproducible**.

---

### 4. ğŸ–¨ï¸ Buenas prÃ¡cticas

Es recomendable imprimir las formas de los conjuntos para verificar:

```python
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
```

---

### 5. ğŸ¤– Crear y entrenar el modelo

Creamos una instancia de un clasificador SVC lineal:

```python
from sklearn.svm import SVC

modelo_svc = SVC(kernel='linear')
```

Entrenamos el modelo con el mÃ©todo `.fit()`:

```python
modelo_svc.fit(X_train, y_train)
```

---

### 6. ğŸ”® Realizar predicciones

Utilizamos `.predict()` para predecir etiquetas:

```python
y_pred_test = modelo_svc.predict(X_test)   # predicciones en test
y_pred_train = modelo_svc.predict(X_train) # opcional: predicciones en entrenamiento
```

ğŸ“ `.predict()` recibe un array del mismo formato que `X_train` o `X_test`:
`(n_muestras, n_caracterÃ­sticas)`

---

### 7. ğŸ“ Medir la exactitud (accuracy)

#### MÃ©todo 1: Usando `accuracy_score` de `sklearn.metrics`

```python
from sklearn.metrics import accuracy_score

accuracy_test = accuracy_score(y_test, y_pred_test)
print("Exactitud en test:", accuracy_test)
```

#### MÃ©todo 2: Usando `.score()` del modelo

```python
print("Exactitud en test (con .score()):", modelo_svc.score(X_test, y_test))
```


## ğŸ” Paso 1 Bloque de Acciones Repetitivas 

Cada experimento con una configuraciÃ³n distinta de SVM sigue el mismo bloque de **tres pasos fundamentales**:

### 1. ğŸ§± Instanciar el modelo

Seleccionamos un kernel (por ejemplo, `'linear'`, `'rbf'`, `'poly'`) y establecemos los valores de sus **hiperparÃ¡metros**:

```python
modelo = SVC(kernel='rbf', C=1.0, gamma=0.01)
```

### 2. ğŸ§  Entrenar el modelo

Ajustamos el modelo a los datos de entrenamiento:

```python
modelo.fit(X_train, y_train)
```

### 3. ğŸ“Š Evaluar el modelo

Calculamos la exactitud en **ambos conjuntos**:

```python
accuracy_train = modelo.score(X_train, y_train)
accuracy_test = modelo.score(X_test, y_test)
```

Este bloque de tres pasos se **repite sistemÃ¡ticamente** para cada combinaciÃ³n de parÃ¡metros que deseemos evaluar.

---

## ğŸ”„ Paso 2: Identificar el Elemento Variable

En nuestra experimentaciÃ³n, los elementos que vamos a variar son:

| ParÃ¡metro | Â¿CuÃ¡ndo aplica?         | Efecto esperado                                 |
| --------- | ----------------------- | ----------------------------------------------- |
| `kernel`  | Siempre                 | Tipo de transformaciÃ³n del espacio de datos     |
| `C`       | Siempre                 | Controla la regularizaciÃ³n (rigidez del margen) |
| `gamma`   | Solo si `kernel='rbf'`  | Alcance de la influencia de cada muestra        |
| `degree`  | Solo si `kernel='poly'` | Grado del polinomio que define la curvatura     |

Esta identificaciÃ³n permite crear una **rejilla de combinaciones** que serÃ¡n exploradas experimentalmente.

---

## ğŸ” Paso 3: Evaluar Casos Aislados

### Ejemplo de EvaluaciÃ³n CrÃ­tica

> ConfiguraciÃ³n: `kernel='rbf'`, `C=10`, `gamma=0.1`
> Resultado:
>
> * Exactitud en entrenamiento: **100.00%**
> * Exactitud en prueba: **62.28%**

Este comportamiento es una **seÃ±al clara de sobreajuste (overfitting)**.

### Â¿Por quÃ© ocurre?

* **C=10**:
  Valor alto â†’ menor regularizaciÃ³n â†’ el modelo se esfuerza por clasificar perfectamente los datos de entrenamiento.
  Resultado: fronteras complejas que pueden capturar ruido.

* **gamma=0.1**:
  Valor relativamente alto â†’ la "influencia" de cada punto de entrenamiento es muy localizada.
  Resultado: fronteras muy sensibles a puntos individuales â†’ se generan ondulaciones innecesarias.

### ğŸ’¡ ReflexiÃ³n

No hay un valor universalmente "bueno" o "malo" para `C` o `gamma`.

> Todo depende del dataset y de cÃ³mo interactÃºan estos hiperparÃ¡metros.

Por eso es esencial experimentar **sistemÃ¡ticamente** con diferentes combinaciones:

> Solo asÃ­ se encuentra el equilibrio ideal entre ajuste y generalizaciÃ³n.


---

## ğŸ” Paso 4: Construir el Bucle Esqueleto

El objetivo de este paso es crear un **bucle anidado** que recorra distintas combinaciones de:

* Tipos de `kernel`
* Valores del parÃ¡metro `C`
* Valores de `gamma` (si el kernel es `'rbf'`)
* Valores de `degree` (si el kernel es `'poly'`)

Este bucle nos permite **experimentar con mÃºltiples configuraciones** para encontrar la "receta" Ã³ptima, es decir, la combinaciÃ³n de parÃ¡metros que proporcione el mejor rendimiento de generalizaciÃ³n.

TambiÃ©n creamos una **lista vacÃ­a** donde almacenaremos los resultados de cada experimento en forma de diccionario.

Durante la iteraciÃ³n, usamos estructuras `if` para decidir quÃ© parÃ¡metros usar segÃºn el tipo de `kernel`. Por ejemplo:

* Si `kernel == 'rbf'`, usamos distintos valores de `gamma`.
* Si `kernel == 'poly'`, usamos distintos valores de `degree`.
* Si `kernel == 'linear'`, no se necesitan ni `gamma` ni `degree`.

---

## ğŸ§© Paso 5: IntegraciÃ³n del Bucle y EvaluaciÃ³n

Dentro de cada `if`, seguimos siempre el **bloque de acciones repetitivas** definido previamente:

1. **Instanciamos** el modelo SVM con la configuraciÃ³n actual.
2. **Entrenamos** el modelo usando `fit()`.
3. **Evaluamos** el modelo con `.score()` tanto en el conjunto de entrenamiento como en el de prueba.

Para cada experimento, guardamos un diccionario con todos los parÃ¡metros utilizados y sus respectivas mÃ©tricas:

```python
{
  'kernel': 'rbf',
  'C': 10,
  'gamma': 0.1,
  'train_accuracy': 1.0,
  'test_accuracy': 0.6228
}
```

Este diccionario se aÃ±ade a la lista general de resultados.

---

## ğŸ“ˆ Paso 6: Manejo y ComparaciÃ³n de Resultados

Una vez almacenados todos los resultados, queremos **encontrar la mejor combinaciÃ³n** de parÃ¡metros segÃºn su rendimiento en el conjunto de prueba.

### MÃ©todo Manual

1. Creamos dos variables:

   * Una para guardar la **mejor exactitud** encontrada hasta el momento.
   * Otra para guardar el **diccionario completo** del experimento asociado.

2. Recorremos la lista de resultados y comparamos:

```python
for experimento_actual in lista_de_resultados:
    exactitud_actual_prueba = experimento_actual["Exactitud Prueba"] 
    if exactitud_actual_prueba > mejor_exactitud:
        mejor_exactitud = exactitud_actual_prueba
        mejor_experimento = experimento_actual
```

3. Finalmente, imprimimos el `mejor_experimento`.

### MÃ©todo con Pandas (mÃ¡s limpio y directo)

Si convertimos la lista de diccionarios en un `DataFrame`, el anÃ¡lisis se simplifica considerablemente:

```python
import pandas as pd

df = pd.DataFrame(lista_resultados)
mejor_experimento = df.loc[df["test_accuracy"].idxmax()]
print(mejor_experimento)
```

> Aunque ambos mÃ©todos son vÃ¡lidos, **pandas ofrece mayor eficiencia y legibilidad**, especialmente cuando se trabaja con grandes cantidades de experimentos.

---



# ğŸ§ª ImplementaciÃ³n de SVR y EvaluaciÃ³n de MÃ©tricas

## ğŸ¯ Paso 1: Clarificar la Meta

Cargamos y preparamos los datos del conjunto `california_housing`, con el objetivo de predecir precios de viviendas. Utilizaremos un modelo **SVR (Support Vector Regressor)** con una configuraciÃ³n bÃ¡sica de hiperparÃ¡metros para realizar predicciones y calcular el **MSE** tanto en el conjunto de entrenamiento como en el de prueba.

---

## âš–ï¸ Paso 2: Escalar las CaracterÃ­sticas

### Â¿Por quÃ© escalar para SVR?

El algoritmo **SVR**, especialmente con el kernel `RBF` (que es el valor por defecto), es **sensible a la escala de las caracterÃ­sticas**. Si los rangos de valores varÃ­an mucho entre columnas, el modelo puede:

* Favorecer caracterÃ­sticas con valores mÃ¡s grandes.
* Aprender patrones incorrectos.
* Ofrecer predicciones menos precisas.

### DiagnÃ³stico previo

Podemos visualizar la dispersiÃ³n de las caracterÃ­sticas con:

```python
df_train = pd.DataFrame(X_train, columns=datos.feature_names).describe().round(2)
print("DescripciÃ³n de las caracterÃ­sticas:\n", df_train)
```

Esto nos ayuda a detectar si existe disparidad en las escalas de las variables, lo cual justificarÃ­a el escalado.

### Â¿CÃ³mo se escalan los datos?

1. **`fit()`** calcula la media y desviaciÃ³n estÃ¡ndar de `X_train`.
2. **`transform()`** aplica esta transformaciÃ³n para que cada caracterÃ­stica tenga:

* ğŸ“ **Media 0**: los valores se centran en torno a 0.
* ğŸ“ˆ **DesviaciÃ³n estÃ¡ndar 1**: los datos tienen una dispersiÃ³n estÃ¡ndarizada.

Esto mejora significativamente el rendimiento de algoritmos sensibles a la escala.

> âš ï¸ **Importante:** Solo se usa `fit()` con `X_train`. No debe aplicarse en `X_test`, para evitar que el modelo "vea" datos que deben ser desconocidos.

---

## ğŸ§ª EvaluaciÃ³n de Modelos

### `.score()` en ClasificaciÃ³n vs RegresiÃ³n

| Modelo | MÃ©todo `.score()` devuelve |
| ------ | -------------------------- |
| `SVC`  | **Exactitud (Accuracy)**   |
| `SVR`  | **Coeficiente RÂ²**         |

Esto se debe a que los modelos de **clasificaciÃ³n** buscan asignar la categorÃ­a correcta, mientras que los de **regresiÃ³n** predicen valores numÃ©ricos.

### Â¿QuÃ© es el RÂ²?

* Mide **cuÃ¡nta varianza de la variable objetivo** puede ser explicada por el modelo.
* Un RÂ² cercano a **1.0** indica un modelo con muy buena capacidad predictiva.
* No debe confundirse con la "exactitud".

---

## âš™ï¸ Paso 3: Etapa de ExperimentaciÃ³n

Definimos un conjunto de hiperparÃ¡metros para explorar:

```python
lista_kernel = ["linear", "rbf", "poly"]
valores_C = [0.1, 1, 10, 100]
valores_gamma = [0.01, 0.1, 1]
valores_degree = [2, 3, 4]
```

Luego, construiremos un bucle para probar combinaciones y registrar mÃ©tricas (RÂ², MSE, MAE). Esto nos permitirÃ¡ encontrar el conjunto de hiperparÃ¡metros mÃ¡s efectivo para nuestro problema de regresiÃ³n.

---

## ğŸ“‰ MÃ©tricas de Error: MAE vs MSE

Ambas mÃ©tricas evalÃºan el **error** de las predicciones, pero de manera diferente:

| MÃ©trica | Sensibilidad a outliers | InterpretaciÃ³n         | PenalizaciÃ³n                    |
| ------- | ----------------------- | ---------------------- | ------------------------------- |
| MAE     | Baja                    | Error absoluto medio   | Lineal                          |
| MSE     | Alta                    | Error cuadrÃ¡tico medio | CuadrÃ¡tica (outliers pesan mÃ¡s) |

Ejemplo de cÃ¡lculo de **MAE**:

```python
# Paso a paso para MAE
diferencias = y_true - y_pred
errores_absolutos = np.abs(diferencias)
mae = np.mean(errores_absolutos)
```

---

## ğŸ§  Paso 4: MÃ©tricas Avanzadas para SVC

Una vez que hayas encontrado los mejores hiperparÃ¡metros para tu modelo `SVC` (clasificaciÃ³n), es importante complementar la **exactitud** con otras mÃ©tricas:

* âœ… **Matriz de ConfusiÃ³n**
* âœ… **PrecisiÃ³n**
* âœ… **Recall**
* âœ… **F1-score**

Esto se logra con:

```python
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
```

Estas mÃ©tricas son especialmente importantes si tienes un **desequilibrio de clases**, ya que la exactitud por sÃ­ sola puede ser engaÃ±osa.

---

## âœ… ConclusiÃ³n

* En tareas de **regresiÃ³n**, usamos mÃ©tricas como RÂ², MAE y MSE.
* En tareas de **clasificaciÃ³n**, usamos exactitud, matriz de confusiÃ³n y mÃ©tricas derivadas.
* Escalar los datos mejora el rendimiento de modelos sensibles como `SVR`.
* Separar correctamente los conjuntos de entrenamiento y prueba es esencial para una evaluaciÃ³n confiable.

---


## Resumen de  MÃ©tricas de EvaluaciÃ³n en ClasificaciÃ³n Binaria: Precision, Recall y F1-Score

| MÃ©trica   | Se enfoca en...                           | QuÃ© quiere evitar                 |
| --------- | ----------------------------------------- | --------------------------------- |
| Precision | Predicciones positivas correctas          | Falsos positivos                  |
| Recall    | Positivos reales correctamente detectados | Falsos negativos                  |
| F1        | Balance entre Precision y Recall          | Cuando uno de los dos falla mucho |


