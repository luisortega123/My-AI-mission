# √Ålgebra Lineal con Python Puro

Breve descripci√≥n del proyecto: Implementaci√≥n de operaciones b√°sicas de matrices (suma, multiplicaci√≥n por escalar, multiplicaci√≥n de matrices, traspuesta) usando √∫nicamente Python puro, junto con explicaciones de los conceptos fundamentales involucrados.

## üìÇ C√≥digo Python (`Algebra_lineal.py`)

Este archivo contiene las funciones desarrolladas para realizar las operaciones matriciales solicitadas: suma de matrices, multiplicaci√≥n por escalar, multiplicaci√≥n de matrices y trasposici√≥n.

## üìò Explicaciones Conceptuales

### 1. Vectores y Combinaciones Lineales

- **Vector:** Una lista ordenada de n√∫meros (componentes) que representa una magnitud con direcci√≥n en un espacio.
- **Combinaci√≥n Lineal:** Es una operaci√≥n en la que cada vector se multiplica por un escalar y luego se suman los vectores resultantes.

### 2. Multiplicaci√≥n de Matrices

* **¬øC√≥mo funciona?:** Cada elemento de la matriz resultante se calcula mediante el producto punto de una fila de la primera matriz y una columna de la segunda matriz (se multiplican los elementos correspondientes y se suman los resultados).
* **¬øPor qu√© importan las dimensiones?:** Son cruciales para determinar si la multiplicaci√≥n es posible y cu√°l ser√° el tama√±o del resultado.
    * **Condici√≥n:** Para multiplicar A (m x n) por B (p x q), es necesario que `n = p` (el n√∫mero de columnas de A debe ser igual al n√∫mero de filas de B).
    * **Tama√±o del Resultado:** Si la condici√≥n se cumple, la matriz resultado ser√° de tama√±o m x q (filas de A x columnas de B).
    * **Raz√≥n:** La condici√≥n `n = p` es necesaria para poder realizar la operaci√≥n fila-por-columna (producto punto).
      
### 3. Traspuesta de una Matriz

- **¬øC√≥mo se obtiene?:** Se intercambian filas por columnas. Es decir, la fila \( i \) de la matriz original se convierte en la columna \( i \) de la traspuesta.
  
- **¬øPara qu√© sirve?:** Facilita reorganizar datos, simplificar f√≥rmulas matem√°ticas y es muy usada en √°reas como Machine Learning para manipular vectores, pesos y operaciones matriciales..

### ‚ùì ¬øPor qu√© la multiplicaci√≥n de matrices *no* es conmutativa en general (\( AB \neq BA \))?

Las matrices representan transformaciones (como rotaciones o escalados), y aplicar una transformaci√≥n seguida de otra no necesariamente da el mismo resultado si se invierte el orden. Adem√°s:

- Puede que \( A \times B \) sea posible pero \( B \times A \) no, debido a la incompatibilidad de dimensiones.
- Incluso si ambas multiplicaciones son posibles, el tama√±o del resultado puede ser diferente.
- Y aun si el tama√±o coincide, el contenido generalmente **no ser√° el mismo**.

### ‚ùì ¬øCu√°l es la intuici√≥n geom√©trica detr√°s de la traspuesta?

- **Perspectiva:** Se puede ver como un cambio de enfoque: si las filas representan personas y las columnas caracter√≠sticas, al trasponer la matriz, ahora las filas representan caracter√≠sticas y las columnas personas que comparten esas caracter√≠sticas.
  
- **Reflejo:** Visualmente, es como reflejar la matriz sobre su **diagonal principal**, intercambiando filas por columnas.


# Preguntas de la Misi√≥n 1:

## ¬øPor qu√© la multiplicaci√≥n de matrices NO es conmutativa en general (AB != BA)?

**Porque** las matrices representan transformaciones, representan rotaciones, etc. **b√°sicamente** cuando multiplicas estas aplicando una **transformaci√≥n**, por lo tanto no puede ser conmutativa. **Tambi√©n** podemos pensar A\*B sea posible pero tal vez B\*A no, **porque** tal vez no sean compatibles (columna de la primera con filas de la segunda). **Aun as√≠**, en el caso de que sea posible A\*B y B\*A, puede que el tama√±o resultante no sea el mismo.

### ¬øCu√°l es la intuici√≥n geom√©trica (si la hay) detr√°s de la traspuesta?

* **Perspectiva:** Podemos pensarlo como un cambio de enfoque. Por ejemplo, si las filas representan personas y las columnas caracter√≠sticas, al trasponer la matriz, las filas pasar√≠an a representar las caracter√≠sticas, y las colvumnas a las personas que las poseen.
* **Reflejo:** Tambi√©n puede visualizarse como reflejar la matriz en su diagonal principal, intercambiando filas por columnas.

## ¬øQu√© es PCA?

PCA, por sus siglas **(An√°lisis de Componentes Principales)**, es una t√©cnica que se usa para reducir la cantidad de variables en un conjunto de datos, sin perder demasiada informaci√≥n.

## ¬øPara qu√© sirve?

* Nos permite **visualizar datos complejos.** Por ejemplo, podemos pasar de 4 dimensiones a 2 y graficarlos.
* **Tambi√©n** podemos eliminar variables que no aportan mucho o que no son tan importantes.

## ¬øC√≥mo se elige cu√°ntos componentes usar (el valor de $k$)?

Al hacer PCA para reducir dimensiones (por ejemplo, de 4 a $k$), la gran pregunta es: ¬øcu√°ntos componentes ($k$) debemos conservar para quedarnos con la informaci√≥n m√°s importante sin perder demasiado? Aqu√≠ entran los conceptos de **varianza explicada** y **varianza acumulada**.

* **Varianza Explicada:** Cada componente principal (CP) "explica" un cierto porcentaje de la variaci√≥n total de los datos. El primer CP explica la mayor parte, el segundo un poco menos, y as√≠ sucesivamente. Este porcentaje est√° directamente relacionado con el tama√±o de su valor propio (eigenvalue) o su valor singular al cuadrado ($s^2$) comparado con la suma total de todos ellos. Es como preguntarse: "¬øCu√°nto de la 'historia completa' de los datos me cuenta esta direcci√≥n principal?"

* **Varianza Acumulada:** Es simplemente ir sumando los porcentajes de varianza explicada de los primeros componentes. Por ejemplo, la varianza acumulada por los 2 primeros CP es (Varianza del CP1) + (Varianza del CP2).

**M√©todos para elegir $k$:**

1.  **Umbral de Varianza Acumulada (El m√°s com√∫n):**
    * Decidimos qu√© porcentaje de la varianza total original queremos conservar (un valor t√≠pico es entre 90% y 99%, por ejemplo, 95%).
    * Calculamos la varianza explicada acumulada al usar 1 componente, luego 2, luego 3...
    * Elegimos el **menor n√∫mero $k$** de componentes cuya varianza acumulada **alcance o supere** nuestro umbral (ej: el primer $k$ que explique al menos el 95% de la varianza).

2.  **M√©todo del "Codo" (Visual):**
    * Se grafica la varianza explicada por cada componente (ordenados de mayor a menor).
    * Se busca un punto en el gr√°fico donde la curva "se dobla" como un codo y empieza a aplanarse. El "codo" sugiere el punto donde a√±adir m√°s componentes ya no aporta una cantidad significativa de informaci√≥n nueva. El valor de $k$ se elige en ese codo.

## Pasos clave de mi implementaci√≥n

1.  **Cargamos** el dataset de Iris, que tiene 4 variables por flor.
2.  **Centramos** los datos, restando la media de cada caracter√≠stica.
3.  **Calculamos** la matriz de covarianza:
    * Aqu√≠ calculamos la matriz de covarianza, que **b√°sicamente** nos dice c√≥mo se mueven las variables entre s√≠. Es como una tabla que nos muestra si dos variables tienden a aumentar o disminuir juntas. Es clave para ver las relaciones entre las caracter√≠sticas.
4.  **Calculamos** los valores propios y vectores propios:
    * Los valores propios nos dicen **cu√°nta** "importancia" tiene cada direcci√≥n de variaci√≥n.
    * Los vectores propios nos indican en qu√© direcci√≥n ocurren esas variaciones.
5.  **Ordenamos** los vectores propios de mayor a menor seg√∫n la varianza que explican. As√≠ sabemos qu√© direcciones son las m√°s importantes y capturan la mayor parte de la informaci√≥n.
6.  **Seleccionamos** los dos primeros componentes principales para proyectar los datos, y as√≠ poder visualizarlos.
7.  **Aqu√≠ utilizamos** una alternativa usando SVD (Descomposici√≥n en Valores Singulares):
    * Como alternativa, usamos SVD, que es otra forma de descomponer los datos. En lugar de calcular la matriz de covarianza, directamente obtenemos las direcciones principales usando SVD. Los vectores fila de `Vh` nos dan las direcciones principales.
8.  **Proyecci√≥n final alternativa usando SVD**:
    * Proyectamos los datos usando los primeros dos componentes principales que conseguimos a trav√©s de SVD. Esto tambi√©n nos reduce la dimensionalidad y nos permite ver los datos en 2D.
9.  **Visualizaci√≥n**:
    * Finalmente, **graficamos** los datos proyectados en 2D, usando los dos componentes principales seleccionados. Esto nos da una visualizaci√≥n m√°s sencilla de los datos, para poder ver patrones o relaciones.

## Conexi√≥n: Matriz de Covarianza y Varianza

Pensemos en los **n√∫meros** de la diagonal principal (de arriba a la izquierda hacia abajo a la derecha), estos son la **varianza**, te dicen **cu√°nto var√≠a** cada variable por s√≠ sola; si es grande, cambia mucho entre los datos.
Los **n√∫meros** que quedan fuera de la diagonal son la **covarianza** entre pares de variables, estas nos dicen **c√≥mo var√≠an** juntas dos **caracter√≠sticas**.

* Si el n√∫mero es positivo y grande, cuando una sube, la otra tambi√©n tiende a subir.
* Si es negativo, cuando una sube, la otra baja.
* Si es cerca de cero, no hay mucha relaci√≥n.

### ¬øPor qu√© nos ayudan a entender la estructura de datos?

**Porque** nos dan pista de **c√≥mo est√°n** distribuidos los datos y sus relaciones.

* Si una varianza es muy grande, quiere decir que esa variable es muy dispersa, cambia mucho entre muestra y muestra. **Tal vez** sea importante.
* Si una covarianza fuera de la diagonal es grande y positiva, significa que esas dos variables **est√°n** relacionadas, se mueven ‚Äújuntas‚Äù. Eso es √∫til porque **podr√≠amos** reducir dimensiones, ya que **est√°n** diciendo cosas parecidas.

### B√ÅSICAMENTE

* Qu√© variables **cambian mucho** (varianzas).
* Qu√© variables **est√°n** conectadas entre s√≠ (covarianzas).

## Conexi√≥n: Eigenvectores/Eigenvalores y Varianza

Los **vectores propios** son como flechas que te dicen hacia d√≥nde se **extienden** los datos, o sea, por donde se dispersan. Los **valores propios** nos dicen **cu√°nta variaci√≥n** hay en la **direcci√≥n** de su **vector propio** correspondiente.

* Si el valor propio es grande, hay mucha varianza (los datos **est√°n** muy esparcidos en esa direcci√≥n).
* Si es peque√±o, hay poca varianza (los datos **est√°n** m√°s concentrados).

El **v√≠nculo** es: los vectores propios de la matriz de covarianza nos dan las **direcciones** donde la varianza es m√°xima, y los valores propios nos dicen **cu√°nta** varianza hay en cada una de esas direcciones.
En resumen: Los **vectores propios** te dicen por d√≥nde se **est√°n** moviendo m√°s los datos. Los **valores propios** te dicen **cu√°nto** se **est√°n** moviendo por esas direcciones.

## Conexi√≥n: SVD y Varianza

Cuando usamos `U, s, Vh = np.linalg.svd(X_centrado)` estamos haciendo algo muy parecido a lo que hicimos con la matriz de covarianza, pero m√°s directo y m√°s estable.
* La U nos dice **c√≥mo** se ven los datos originales sobre las nuevas direcciones (`Vh`).
* Las filas de `Vh` son las mismas direcciones principales que **hab√≠amos** encontrado con los vectores propios, o sea, por d√≥nde m√°s se esparcen los datos.
* Los valores de `s` (valores singulares) **est√°n** ligados a la varianza:
    * Si haces $s^2$ (s al cuadrado), eso te da una idea de **cu√°nta** varianza hay en cada direcci√≥n.

## En resumen (SVD):

* `Vh`: Hacia d√≥nde mirar (las direcciones principales).
* `s`: **Cu√°nta** importancia tiene cada direcci√≥n (relacionado con la varianza a trav√©s de $s^2$).
* `U`: **C√≥mo** cada punto del dataset se ve desde esas nuevas direcciones.

SVD te da otra forma (m√°s precisa y directa) de encontrar esas direcciones principales importantes (`Vh`) y **cu√°nta** info hay en cada una ($s^2$). Es como hacer PCA, pero sin tener que **calcular** la matriz de covarianza.

## C√≥mo Elegir $k$ (N√∫mero de Componentes)

Cuando hacemos PCA necesitamos preguntarnos "**¬øcu√°ntas direcciones** necesito para obtener lo **m√°s** importante?". **Aqu√≠** entramos en los **conceptos** de **varianza explicada** y **varianza acumulada**.

* **Varianza explicada:** Cada componente principal (esas nuevas direcciones que PCA encuentra) explica una parte de la variaci√≥n total que hay en tus datos. Es como decir: "**¬øCu√°nto** de la info original me **est√°** mostrando esta **direcci√≥n**?"
* **Varianza acumulada:** Es la suma de la varianza explicada de los primeros $k$ componentes. Nos ayuda a saber **cu√°nta informaci√≥n** estamos obteniendo si usamos solo $k$ componentes.

## ¬øC√≥mo saber cu√°ntos componentes necesito para conservar, por ejemplo, el 95% de la info (varianza)?

Primero, **calcul√°s** la varianza que explica cada componente, y **despu√©s** vas sumando una por una (eso se llama varianza acumulada).

* **Si us√°s los `valores_propios` (eigenvalues):**
    * **Sum√°s** todos los valores propios, eso te da la varianza total.
    * Para cada componente, **divid√≠s** su valor propio entre la varianza total; eso te da el porcentaje de varianza que explica ese componente.
    * Vas sumando esos porcentajes hasta que llegues o pases el umbral del 95%.
* **Si us√°s los `s` de SVD (valores singulares):**
    * **Elev√°s** al cuadrado cada n√∫mero de `s` ($s^2$), eso te dice **cu√°nta** "varianza" (informaci√≥n) tiene cada componente.
    * **Sum√°s** todos esos cuadrados; eso te da la informaci√≥n total que hay en los datos.
    * **Despu√©s**, para cada componente, **divid√≠s** su valor al cuadrado ($s^2$) entre la suma total ‚Üí as√≠ ves qu√© porcentaje de info aporta ese componente.
    * Vas sumando esos porcentajes, uno por uno, hasta que la suma llegue al 95%.

## Puedes explicar con una analog√≠a simple o geom√©trica qu√© representan los componentes principales? ¬øC√≥mo se relaciona la p√©rdida de informaci√≥n con la reducci√≥n de dimensiones?

**Componentes principales:**
Los componentes principales pueden entenderse como nuevas direcciones o ejes que nos permiten describir los datos de manera m√°s compacta.
Una forma de verlo es imaginar que las fotos representan informaci√≥n. Lo que los componentes principales buscan es organizar esta informaci√≥n de manera que no se pierda demasiado detalle.

En t√©rminos visuales, podr√≠amos pensar en dibujar una l√≠nea imaginaria que pase por el punto de mayor dispersi√≥n de los datos. Esta l√≠nea representar√≠a el primer componente principal, la direcci√≥n con mayor varianza. Es decir, la l√≠nea que captura la mayor parte de la variaci√≥n en los datos.

Despu√©s, podemos dibujar una segunda l√≠nea que debe ser ortogonal a la primera. Esta segunda l√≠nea captura la mayor cantidad restante de informaci√≥n, es decir, la segunda mayor varianza. As√≠ sucesivamente para cada componente principal.

**C√≥mo se relaciona la p√©rdida de informaci√≥n con la reducci√≥n de dimensiones:**
Reducir dimensiones es comparable a resumir una historia: retienes lo m√°s importante, pero inevitablemente se pierde parte de la informaci√≥n.
Siguiendo la analog√≠a de los datos como una nube de puntos, el primer componente principal captura la mayor parte de la informaci√≥n contenida en los datos. Sin embargo, como se descartan los componentes restantes, se pierde informaci√≥n.
Esta informaci√≥n perdida corresponde a la varianza explicada por los componentes principales que decidimos no conservar.