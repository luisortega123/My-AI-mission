# My-AI-Mission

# Misi√≥n 1: √Ålgebra Lineal con Python Puro

Breve descripci√≥n del proyecto: Implementaci√≥n de operaciones b√°sicas de matrices (suma, multiplicaci√≥n por escalar, multiplicaci√≥n de matrices, traspuesta) usando √∫nicamente Python puro, junto con explicaciones de los conceptos fundamentales involucrados.

## üìÇ C√≥digo Python (`Algebra_lineal.py`)

Este archivo contiene las funciones desarrolladas para realizar las operaciones matriciales solicitadas: suma de matrices, multiplicaci√≥n por escalar, multiplicaci√≥n de matrices y trasposici√≥n.

## üìò Explicaciones Conceptuales

### 1. Vectores y Combinaciones Lineales

- **Vector:** Una lista ordenada de n√∫meros (componentes) que representa una magnitud con direcci√≥n en un espacio.
- **Combinaci√≥n Lineal:** Es una operaci√≥n en la que cada vector se multiplica por un escalar y luego se suman los vectores resultantes.

### 2. Multiplicaci√≥n de Matrices

* **¬øC√≥mo funciona?:** Cada elemento de la matriz resultante se calcula mediante el producto punto de una fila de la primera matriz y una columna de la segunda matriz (se multiplican los elementos correspondientes y se suman los resultados).
* **¬øPor qu√© importan las dimensiones?:** Son cruciales para determinar si la multiplicaci√≥n es posible y cu√°l ser√° el tama√±o del resultado.
    * **Condici√≥n:** Para multiplicar A (m x n) por B (p x q), es necesario que `n = p` (el n√∫mero de columnas de A debe ser igual al n√∫mero de filas de B).
    * **Tama√±o del Resultado:** Si la condici√≥n se cumple, la matriz resultado ser√° de tama√±o m x q (filas de A x columnas de B).
    * **Raz√≥n:** La condici√≥n `n = p` es necesaria para poder realizar la operaci√≥n fila-por-columna (producto punto).
      
### 3. Traspuesta de una Matriz

- **¬øC√≥mo se obtiene?:** Se intercambian filas por columnas. Es decir, la fila \( i \) de la matriz original se convierte en la columna \( i \) de la traspuesta.
  
- **¬øPara qu√© sirve?:** Facilita reorganizar datos, simplificar f√≥rmulas matem√°ticas y es muy usada en √°reas como Machine Learning para manipular vectores, pesos y operaciones matriciales..

### ‚ùì ¬øPor qu√© la multiplicaci√≥n de matrices *no* es conmutativa en general (\( AB \neq BA \))?

Las matrices representan transformaciones (como rotaciones o escalados), y aplicar una transformaci√≥n seguida de otra no necesariamente da el mismo resultado si se invierte el orden. Adem√°s:

- Puede que \( A \times B \) sea posible pero \( B \times A \) no, debido a la incompatibilidad de dimensiones.
- Incluso si ambas multiplicaciones son posibles, el tama√±o del resultado puede ser diferente.
- Y aun si el tama√±o coincide, el contenido generalmente **no ser√° el mismo**.

### ‚ùì ¬øCu√°l es la intuici√≥n geom√©trica detr√°s de la traspuesta?

- **Perspectiva:** Se puede ver como un cambio de enfoque: si las filas representan personas y las columnas caracter√≠sticas, al trasponer la matriz, ahora las filas representan caracter√≠sticas y las columnas personas que comparten esas caracter√≠sticas.
  
- **Reflejo:** Visualmente, es como reflejar la matriz sobre su **diagonal principal**, intercambiando filas por columnas.


# Preguntas de la Misi√≥n 1:

## ¬øPor qu√© la multiplicaci√≥n de matrices NO es conmutativa en general (AB != BA)?

**Porque** las matrices representan transformaciones, representan rotaciones, etc. **b√°sicamente** cuando multiplicas estas aplicando una **transformaci√≥n**, por lo tanto no puede ser conmutativa. **Tambi√©n** podemos pensar A\*B sea posible pero tal vez B\*A no, **porque** tal vez no sean compatibles (columna de la primera con filas de la segunda). **Aun as√≠**, en el caso de que sea posible A\*B y B\*A, puede que el tama√±o resultante no sea el mismo.

### ¬øCu√°l es la intuici√≥n geom√©trica (si la hay) detr√°s de la traspuesta?

* **Perspectiva:** Podemos pensarlo como un cambio de enfoque. Por ejemplo, si las filas representan personas y las columnas caracter√≠sticas, al trasponer la matriz, las filas pasar√≠an a representar las caracter√≠sticas, y las colvumnas a las personas que las poseen.
* **Reflejo:** Tambi√©n puede visualizarse como reflejar la matriz en su diagonal principal, intercambiando filas por columnas.


# Misi√≥n 2

## ¬øQu√© es PCA?

PCA, por sus siglas **(An√°lisis de Componentes Principales)**, es una t√©cnica que se usa para reducir la cantidad de variables en un conjunto de datos, sin perder demasiada informaci√≥n.

## ¬øPara qu√© sirve?

* Nos permite **visualizar datos complejos.** Por ejemplo, podemos pasar de 4 dimensiones a 2 y graficarlos.
* **Tambi√©n** podemos eliminar variables que no aportan mucho o que no son tan importantes.

## ¬øC√≥mo se elige cu√°ntos componentes usar (el valor de $k$)?

Al hacer PCA para reducir dimensiones (por ejemplo, de 4 a $k$), la gran pregunta es: ¬øcu√°ntos componentes ($k$) debemos conservar para quedarnos con la informaci√≥n m√°s importante sin perder demasiado? Aqu√≠ entran los conceptos de **varianza explicada** y **varianza acumulada**.

* **Varianza Explicada:** Cada componente principal (CP) "explica" un cierto porcentaje de la variaci√≥n total de los datos. El primer CP explica la mayor parte, el segundo un poco menos, y as√≠ sucesivamente. Este porcentaje est√° directamente relacionado con el tama√±o de su valor propio (eigenvalue) o su valor singular al cuadrado ($s^2$) comparado con la suma total de todos ellos. Es como preguntarse: "¬øCu√°nto de la 'historia completa' de los datos me cuenta esta direcci√≥n principal?"

* **Varianza Acumulada:** Es simplemente ir sumando los porcentajes de varianza explicada de los primeros componentes. Por ejemplo, la varianza acumulada por los 2 primeros CP es (Varianza del CP1) + (Varianza del CP2).

**M√©todos para elegir $k$:**

1.  **Umbral de Varianza Acumulada (El m√°s com√∫n):**
    * Decidimos qu√© porcentaje de la varianza total original queremos conservar (un valor t√≠pico es entre 90% y 99%, por ejemplo, 95%).
    * Calculamos la varianza explicada acumulada al usar 1 componente, luego 2, luego 3...
    * Elegimos el **menor n√∫mero $k$** de componentes cuya varianza acumulada **alcance o supere** nuestro umbral (ej: el primer $k$ que explique al menos el 95% de la varianza).

2.  **M√©todo del "Codo" (Visual):**
    * Se grafica la varianza explicada por cada componente (ordenados de mayor a menor).
    * Se busca un punto en el gr√°fico donde la curva "se dobla" como un codo y empieza a aplanarse. El "codo" sugiere el punto donde a√±adir m√°s componentes ya no aporta una cantidad significativa de informaci√≥n nueva. El valor de $k$ se elige en ese codo.

## Pasos clave de mi implementaci√≥n

1.  **Cargamos** el dataset de Iris, que tiene 4 variables por flor.
2.  **Centramos** los datos, restando la media de cada caracter√≠stica.
3.  **Calculamos** la matriz de covarianza:
    * Aqu√≠ calculamos la matriz de covarianza, que **b√°sicamente** nos dice c√≥mo se mueven las variables entre s√≠. Es como una tabla que nos muestra si dos variables tienden a aumentar o disminuir juntas. Es clave para ver las relaciones entre las caracter√≠sticas.
4.  **Calculamos** los valores propios y vectores propios:
    * Los valores propios nos dicen **cu√°nta** "importancia" tiene cada direcci√≥n de variaci√≥n.
    * Los vectores propios nos indican en qu√© direcci√≥n ocurren esas variaciones.
5.  **Ordenamos** los vectores propios de mayor a menor seg√∫n la varianza que explican. As√≠ sabemos qu√© direcciones son las m√°s importantes y capturan la mayor parte de la informaci√≥n.
6.  **Seleccionamos** los dos primeros componentes principales para proyectar los datos, y as√≠ poder visualizarlos.
7.  **Aqu√≠ utilizamos** una alternativa usando SVD (Descomposici√≥n en Valores Singulares):
    * Como alternativa, usamos SVD, que es otra forma de descomponer los datos. En lugar de calcular la matriz de covarianza, directamente obtenemos las direcciones principales usando SVD. Los vectores fila de `Vh` nos dan las direcciones principales.
8.  **Proyecci√≥n final alternativa usando SVD**:
    * Proyectamos los datos usando los primeros dos componentes principales que conseguimos a trav√©s de SVD. Esto tambi√©n nos reduce la dimensionalidad y nos permite ver los datos en 2D.
9.  **Visualizaci√≥n**:
    * Finalmente, **graficamos** los datos proyectados en 2D, usando los dos componentes principales seleccionados. Esto nos da una visualizaci√≥n m√°s sencilla de los datos, para poder ver patrones o relaciones.

## Conexi√≥n: Matriz de Covarianza y Varianza

Pensemos en los **n√∫meros** de la diagonal principal (de arriba a la izquierda hacia abajo a la derecha), estos son la **varianza**, te dicen **cu√°nto var√≠a** cada variable por s√≠ sola; si es grande, cambia mucho entre los datos.
Los **n√∫meros** que quedan fuera de la diagonal son la **covarianza** entre pares de variables, estas nos dicen **c√≥mo var√≠an** juntas dos **caracter√≠sticas**.

* Si el n√∫mero es positivo y grande, cuando una sube, la otra tambi√©n tiende a subir.
* Si es negativo, cuando una sube, la otra baja.
* Si es cerca de cero, no hay mucha relaci√≥n.

### ¬øPor qu√© nos ayudan a entender la estructura de datos?

**Porque** nos dan pista de **c√≥mo est√°n** distribuidos los datos y sus relaciones.

* Si una varianza es muy grande, quiere decir que esa variable es muy dispersa, cambia mucho entre muestra y muestra. **Tal vez** sea importante.
* Si una covarianza fuera de la diagonal es grande y positiva, significa que esas dos variables **est√°n** relacionadas, se mueven ‚Äújuntas‚Äù. Eso es √∫til porque **podr√≠amos** reducir dimensiones, ya que **est√°n** diciendo cosas parecidas.

### B√ÅSICAMENTE

* Qu√© variables **cambian mucho** (varianzas).
* Qu√© variables **est√°n** conectadas entre s√≠ (covarianzas).

## Conexi√≥n: Eigenvectores/Eigenvalores y Varianza

Los **vectores propios** son como flechas que te dicen hacia d√≥nde se **extienden** los datos, o sea, por donde se dispersan. Los **valores propios** nos dicen **cu√°nta variaci√≥n** hay en la **direcci√≥n** de su **vector propio** correspondiente.

* Si el valor propio es grande, hay mucha varianza (los datos **est√°n** muy esparcidos en esa direcci√≥n).
* Si es peque√±o, hay poca varianza (los datos **est√°n** m√°s concentrados).

El **v√≠nculo** es: los vectores propios de la matriz de covarianza nos dan las **direcciones** donde la varianza es m√°xima, y los valores propios nos dicen **cu√°nta** varianza hay en cada una de esas direcciones.
En resumen: Los **vectores propios** te dicen por d√≥nde se **est√°n** moviendo m√°s los datos. Los **valores propios** te dicen **cu√°nto** se **est√°n** moviendo por esas direcciones.

## Conexi√≥n: SVD y Varianza

Cuando usamos `U, s, Vh = np.linalg.svd(X_centrado)` estamos haciendo algo muy parecido a lo que hicimos con la matriz de covarianza, pero m√°s directo y m√°s estable.
* La U nos dice **c√≥mo** se ven los datos originales sobre las nuevas direcciones (`Vh`).
* Las filas de `Vh` son las mismas direcciones principales que **hab√≠amos** encontrado con los vectores propios, o sea, por d√≥nde m√°s se esparcen los datos.
* Los valores de `s` (valores singulares) **est√°n** ligados a la varianza:
    * Si haces $s^2$ (s al cuadrado), eso te da una idea de **cu√°nta** varianza hay en cada direcci√≥n.

## En resumen (SVD):

* `Vh`: Hacia d√≥nde mirar (las direcciones principales).
* `s`: **Cu√°nta** importancia tiene cada direcci√≥n (relacionado con la varianza a trav√©s de $s^2$).
* `U`: **C√≥mo** cada punto del dataset se ve desde esas nuevas direcciones.

SVD te da otra forma (m√°s precisa y directa) de encontrar esas direcciones principales importantes (`Vh`) y **cu√°nta** info hay en cada una ($s^2$). Es como hacer PCA, pero sin tener que **calcular** la matriz de covarianza.

## C√≥mo Elegir $k$ (N√∫mero de Componentes)

Cuando hacemos PCA necesitamos preguntarnos "**¬øcu√°ntas direcciones** necesito para obtener lo **m√°s** importante?". **Aqu√≠** entramos en los **conceptos** de **varianza explicada** y **varianza acumulada**.

* **Varianza explicada:** Cada componente principal (esas nuevas direcciones que PCA encuentra) explica una parte de la variaci√≥n total que hay en tus datos. Es como decir: "**¬øCu√°nto** de la info original me **est√°** mostrando esta **direcci√≥n**?"
* **Varianza acumulada:** Es la suma de la varianza explicada de los primeros $k$ componentes. Nos ayuda a saber **cu√°nta informaci√≥n** estamos obteniendo si usamos solo $k$ componentes.

## ¬øC√≥mo saber cu√°ntos componentes necesito para conservar, por ejemplo, el 95% de la info (varianza)?

Primero, **calcul√°s** la varianza que explica cada componente, y **despu√©s** vas sumando una por una (eso se llama varianza acumulada).

* **Si us√°s los `valores_propios` (eigenvalues):**
    * **Sum√°s** todos los valores propios, eso te da la varianza total.
    * Para cada componente, **divid√≠s** su valor propio entre la varianza total; eso te da el porcentaje de varianza que explica ese componente.
    * Vas sumando esos porcentajes hasta que llegues o pases el umbral del 95%.
* **Si us√°s los `s` de SVD (valores singulares):**
    * **Elev√°s** al cuadrado cada n√∫mero de `s` ($s^2$), eso te dice **cu√°nta** "varianza" (informaci√≥n) tiene cada componente.
    * **Sum√°s** todos esos cuadrados; eso te da la informaci√≥n total que hay en los datos.
    * **Despu√©s**, para cada componente, **divid√≠s** su valor al cuadrado ($s^2$) entre la suma total ‚Üí as√≠ ves qu√© porcentaje de info aporta ese componente.
    * Vas sumando esos porcentajes, uno por uno, hasta que la suma llegue al 95%.

## Puedes explicar con una analog√≠a simple o geom√©trica qu√© representan los componentes principales? ¬øC√≥mo se relaciona la p√©rdida de informaci√≥n con la reducci√≥n de dimensiones?

**Componentes principales:**
Los componentes principales pueden entenderse como nuevas direcciones o ejes que nos permiten describir los datos de manera m√°s compacta.
Una forma de verlo es imaginar que las fotos representan informaci√≥n. Lo que los componentes principales buscan es organizar esta informaci√≥n de manera que no se pierda demasiado detalle.

En t√©rminos visuales, podr√≠amos pensar en dibujar una l√≠nea imaginaria que pase por el punto de mayor dispersi√≥n de los datos. Esta l√≠nea representar√≠a el primer componente principal, la direcci√≥n con mayor varianza. Es decir, la l√≠nea que captura la mayor parte de la variaci√≥n en los datos.

Despu√©s, podemos dibujar una segunda l√≠nea que debe ser ortogonal a la primera. Esta segunda l√≠nea captura la mayor cantidad restante de informaci√≥n, es decir, la segunda mayor varianza. As√≠ sucesivamente para cada componente principal.

**C√≥mo se relaciona la p√©rdida de informaci√≥n con la reducci√≥n de dimensiones:**
Reducir dimensiones es comparable a resumir una historia: retienes lo m√°s importante, pero inevitablemente se pierde parte de la informaci√≥n.
Siguiendo la analog√≠a de los datos como una nube de puntos, el primer componente principal captura la mayor parte de la informaci√≥n contenida en los datos. Sin embargo, como se descartan los componentes restantes, se pierde informaci√≥n.
Esta informaci√≥n perdida corresponde a la varianza explicada por los componentes principales que decidimos no conservar.

# Explicaciones Tarea 3

## Descripci√≥n de las operaciones realizadas con Pandas

En la primera parte del script, **utilic√©** la **librer√≠a** Pandas para realizar varias operaciones de **manipulaci√≥n** y **an√°lisis** sobre el conjunto de datos de **Iris**. A **continuaci√≥n**, se **describen** las **principales** operaciones que **realic√©**:

* **Carga de datos:** **Utilic√©** el **m√©todo** `pd.read_csv()` para cargar el archivo `Iris.csv` en un DataFrame llamado `df`. Este DataFrame contiene las **caracter√≠sticas** de las flores de **Iris**.

* **Visualizaci√≥n de las primeras y √∫ltimas filas:** **Us√©** `df.head()` para mostrar las primeras 5 filas del DataFrame y obtener una vista **r√°pida** de los primeros registros. **Tambi√©n** **utilic√©** `df.tail()` para ver las **√∫ltimas** 5 filas y verificar **c√≥mo** **terminan** los datos.

* **Resumen del DataFrame:** Con `df.info()`, **obtuve** un resumen general del DataFrame, incluyendo el **n√∫mero** de entradas, el **tipo** de datos de cada columna y la cantidad de valores no nulos.

* **Estad√≠sticas descriptivas:** **Utilic√©** `df.describe()` para obtener **estad√≠sticas** descriptivas de las columnas **num√©ricas** del DataFrame, como la media, **desviaci√≥n** **est√°ndar**, **m√≠nimos**, **m√°ximos** y percentiles. Posteriormente, **redonde√©** los resultados a tres decimales con `.round(3)`.

* **Conteo de categor√≠as:** Para saber **cu√°ntas** veces **aparece** cada valor en la columna `Species`, **utilic√©** `df['Species'].value_counts()`, lo que me **dio** la **distribuci√≥n** de las especies en el conjunto de datos.

* **Selecci√≥n de filas y columnas por posici√≥n:** **Us√©** `df.iloc[-1, [1, 3]]` para seleccionar la **√∫ltima** fila y las columnas en las **posiciones** 1 y 3 (correspondientes a `SepalWidthCm` y `PetalWidthCm`).

* **Selecci√≥n de filas y columnas por etiquetas:** Con `df.loc[149, ['SepalWidthCm', 'PetalWidthCm']]`, **seleccion√©** la fila con el **√≠ndice** 149 y las columnas `SepalWidthCm` y `PetalWidthCm` por sus nombres.

* **Filtrado de datos con condiciones combinadas:** **Apliqu√©** un filtro combinado con `df[(df['Species'] == 'Iris-setosa') & (df['SepalLengthCm'] < 5)]` para seleccionar las filas donde la **especie** es `Iris-setosa` y la **longitud** del **s√©palo** es menor a 5.

Estas **operaciones** son fundamentales para explorar y entender los datos antes de pasar a las siguientes etapas del **an√°lisis** o **modelado**.

## Naive Bayes

Podemos pensar en Naive Bayes como un detective que trata de resolver un caso: descubrir qu√© tipo de flor es una nueva flor que encontr√≥, bas√°ndose en la evidencia (sus medidas) y su experiencia previa.

### Pasos del Algoritmo (Implementaci√≥n)

1.  **Cargar Datos:** Es muy importante empezar importando los datos necesarios, en este caso, usando `load_iris()`. A este resultado le damos un nombre (ej: `datos`). Accedemos a los datos de dos maneras principales:
    * `.data` (guardado en `$X$`): Contiene las caracter√≠sticas num√©ricas (las 4 medidas de cada flor).
    * `.target` (guardado en `$Y$`): Contiene las etiquetas de clase (0, 1 o 2) para cada flor.

2.  **Separar Datos por Clase:** Tomamos la matriz `$X$` y, usando las etiquetas `$Y$`, la separamos en tres grupos: uno para cada clase (0, 1 y 2). As√≠ tenemos `X_clase0`, `X_clase1`, `X_clase2`.

3.  **Calcular Estad√≠sticas ("Entrenamiento"):** Una vez separados los datos por clase, calculamos para cada una:
    * **Media (`np.mean(..., axis=0)`):** El valor promedio de cada una de las 4 caracter√≠sticas para esa clase espec√≠fica.
    * **Desviaci√≥n Est√°ndar (`np.std(..., axis=0)`):** Cu√°nto var√≠an o se dispersan las medidas de cada caracter√≠stica alrededor de su media, para esa clase.
    * **Probabilidad Previa (Prior):** Qu√© tan com√∫n es cada especie en el conjunto total de datos (calculado como: n√∫mero de flores de la clase / n√∫mero total de flores).

4.  **Agrupar Estad√≠sticas:** Guardamos todas las medias, desviaciones est√°ndar y priors calculados en listas (`lista_medias`, `lista_stds`, `lista_priors`) para usarlas f√°cilmente en la predicci√≥n.

5.  **Funci√≥n `gaussian_pdf`:** Definimos una funci√≥n auxiliar importante. Esta calcula la Densidad de Probabilidad Gaussiana: dado un valor `$x$`, una media `$mu$` y una desviaci√≥n est√°ndar `$std$`, nos dice qu√© tan "probable" o "t√≠pico" es ese valor `$x$` si perteneciera a una distribuci√≥n normal (Campana de Gauss) con esa `$mu$` y `$std$`. Es como preguntarle a la campana: "¬øQu√© altura ten√©s en este punto `$x$`?".

6.  **Funci√≥n `predecir_clases_nb` (Predicci√≥n):** Esta es la funci√≥n principal. Recibe una flor nueva (`flor_nueva`) y las listas de estad√≠sticas. Para decidir la clase:
    * Crea una lista vac√≠a (`posteriors`) para guardar los "scores".
    * Usa un bucle `for` para recorrer cada clase posible (0, 1, 2).
    * **Dentro del bucle:**
        * Obtiene las estad√≠sticas (`media_actual`, `std_actual`, `prior_actual`) de la clase actual.
        * Calcula el **Likelihood**: Usa `gaussian_pdf` para obtener la probabilidad de cada una de las 4 caracter√≠sticas de la `flor_nueva` seg√∫n la media y `$std$` de la clase actual. Luego, multiplica estas 4 probabilidades (¬°la asunci√≥n "naive"!) usando `np.prod` para obtener la probabilidad total de observar esas caracter√≠sticas si la flor fuera de esta clase. (Nota: en el c√≥digo final usamos logaritmos para evitar problemas num√©ricos, sumando $\log(\text{PDFs})$ en lugar de multiplicar PDFs).
        * Calcula el **Score Posterior**: Multiplica el `likelihood` por el `prior_actual` (o suma sus logaritmos).
        * Guarda este `posterior_actual` en la lista `posteriors`.
    * **Despu√©s del bucle:** Compara los 3 scores guardados en `posteriors` y elige el **√≠ndice** (0, 1 o 2) del score m√°s alto usando `np.argmax`.
    * Devuelve ese √≠ndice como la clase predicha.

7.  **Probar Nuestro Clasificador:**
    * Hacemos un bucle que recorre todas las flores del dataset original (`$X$`).
    * Para cada flor, llamamos a `predecir_clases_nb` para obtener su predicci√≥n.
    * Guardamos todas las predicciones.
    * Calculamos la **Precisi√≥n (Accuracy)**: comparamos nuestras predicciones con las etiquetas reales (`$Y$`) y vemos el porcentaje de aciertos.

8.  **Comparar con Scikit-learn:**
    * Usamos la implementaci√≥n `GaussianNB` de Scikit-learn, la entrenamos (`fit`) y predecimos (`predict`) con los mismos datos `$X$` e `$Y$`.
    * Calculamos su precisi√≥n para tener una referencia y ver si nuestro modelo manual da resultados similares.


## Conceptos Clave Relacionados

### Teorema de Bayes (Regla General)

* **¬øQu√© es?:** Es una f√≥rmula matem√°tica para actualizar una probabilidad o creencia inicial (`Prior`) bas√°ndonos en nueva evidencia (`Datos`) para obtener una probabilidad final (`Posterior`).
* **Idea:** $P(\text{Clase} | \text{Datos}) \propto P(\text{Datos} | \text{Clase}) \times P(\text{Clase})$ (El posterior es proporcional al likelihood por el prior).
* **En resumen:** Nos dice c√≥mo combinar lo que ya sab√≠amos con la nueva evidencia.

### Naive Bayes (El Algoritmo de Clasificaci√≥n)

* **¬øQu√© es?:** Un m√©todo de clasificaci√≥n que **usa** el Teorema de Bayes para decidir a qu√© clase pertenece una nueva muestra.
* **La parte "Naive" (Ingenua):** Su caracter√≠stica principal es que **asume (ingenuamente)** que todas las caracter√≠sticas (las 4 medidas de la flor) son **independientes entre s√≠** dada la clase. Esto simplifica much√≠simo el c√°lculo del Likelihood ($P(\text{Datos}|\text{Clase})$), permitiendo multiplicar las probabilidades individuales de cada caracter√≠stica (o sumar sus logaritmos).
* **En resumen:** Aplica Bayes con una simplificaci√≥n clave (independencia) para clasificar.

### Campana de Gauss (Distribuci√≥n Normal)

* **¬øQu√© es?:** Una forma matem√°tica muy com√∫n (la curva en forma de campana) que describe c√≥mo se distribuyen muchos datos num√©ricos alrededor de una media, con una cierta dispersi√≥n (desviaci√≥n est√°ndar).
* **En Gaussian Naive Bayes:** Hacemos la **suposici√≥n** de que las caracter√≠sticas num√©ricas *dentro de cada clase* siguen esta distribuci√≥n Gaussiana. Esto nos permite usar la f√≥rmula de la Campana de Gauss (nuestra funci√≥n `gaussian_pdf`) para calcular las probabilidades $P(\text{caracter√≠stica}_k | \text{Clase})$ que necesitamos para el Likelihood.
* **En resumen:** Es el modelo de probabilidad que usamos para las caracter√≠sticas num√©ricas en esta versi√≥n espec√≠fica de Naive Bayes.

### Resumen de la Relaci√≥n

El **Teorema de Bayes** nos da el marco general. El algoritmo **Naive Bayes** lo aplica para clasificar, a√±adiendo la suposici√≥n "naive" de independencia. La **Campana de Gauss** es la herramienta que usamos en *Gaussian* Naive Bayes para calcular las probabilidades de las caracter√≠sticas num√©ricas dentro de ese marco.

# Explicaciones Tarea 4

## Regresi√≥n Lineal

### ¬øQu√© es?

La **regresi√≥n lineal** es una t√©cnica estad√≠stica que busca la relaci√≥n entre una variable cuantitativa ($Y$) y una o m√°s variables predictoras ($X$).
El **objetivo** es **predecir valores num√©ricos continuos**, bas√°ndose en la suposici√≥n de que existe una relaci√≥n lineal entre las variables explicativas ($X$) y la variable objetivo ($Y$).

---

### ¬øQu√© tipo de resultado produce?

**Variable cuantitativa continua:**
No clasifica en categor√≠as, sino que entrega un valor num√©rico que puede tomar cualquier valor dentro de un rango.

---

### Interpretaci√≥n de los coeficientes

Los coeficientes del modelo ($\theta$) indican c√≥mo cambia $Y$ ante variaciones en $X$.

---

## Funci√≥n Hip√≥tesis ($h_\theta(X) = X \theta$)

Ecuaci√≥n fundamental en la regresi√≥n lineal: $h_\theta(X) = X \theta$. Define la relaci√≥n entre las variables predictoras ($X$) y la variable objetivo ($Y$).

### Objetivo

Predecir valores num√©ricos continuos bas√°ndose en una combinaci√≥n lineal de caracter√≠sticas de entrada y coeficientes asociados.

### Elementos

1.  **$X$**: Matriz de caracter√≠sticas (inputs)
    * Cada fila representa una observaci√≥n (ejemplo de entrenamiento)
    * Cada columna corresponde a una caracter√≠stica (variable predictora)
    * Se agrega una columna de unos para incluir el intercepto ($\theta_0$)

2.  **$\theta$**: Vector de par√°metros (coeficientes)
    * Contiene los pesos que el modelo aprende durante el entrenamiento
    * Incluye el intercepto ($\theta_0$)
    * $\theta_1, \theta_2, \dots$ indican la influencia de cada caracter√≠stica

3.  **$h_\theta(X)$**: Predicci√≥n del modelo
    * Resultado del producto matricial $X\theta$
    * Cada valor $h_\theta(x^{(i)})$ es la predicci√≥n para la observaci√≥n $i$

---

## Funci√≥n de Coste (MSE)

**F√≥rmula:**
$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$

Mide el **promedio del error cuadr√°tico** entre las predicciones del modelo y los valores reales en todo el conjunto de datos.

* **$m$**: N√∫mero de observaciones
* **$h_\theta(x^{(i)})$**: Predicci√≥n para la observaci√≥n $i$
* **$y^{(i)}$**: Valor real para la observaci√≥n $i$

---

### ¬øPor qu√© se eleva al cuadrado la diferencia?

1.  **Evita errores negativos:** Las diferencias se vuelven positivas.
2.  **Penaliza errores grandes:** Un error de 2 pesa m√°s (4) que uno de 1 (1).
3.  **Facilita la optimizaci√≥n:** La funci√≥n cuadr√°tica es convexa y garantiza un m√≠nimo global.

---

## ¬øPor qu√© queremos minimizar $J(\theta)$?

Minimizar $J(\theta)$ significa ajustar los par√°metros $\theta$ para que las predicciones sean lo m√°s cercanas posible a los valores reales.

### M√©todos comunes:

* **M√≠nimos Cuadrados Ordinarios (OLS):** Soluci√≥n anal√≠tica.
* **Descenso de Gradiente (GD):** M√©todo iterativo.

---

## ¬øQu√© implica un $J(\theta)$ peque√±o?

* **Buen ajuste:** Las predicciones est√°n cerca de los valores reales.
* **Alta precisi√≥n:** El modelo generaliza bien.
* **Menor incertidumbre:** Los errores (residuos) tienen baja variabilidad.

---

# Descenso de Gradiente (Gradient Descent)

El **descenso de gradiente** busca minimizar $J(\theta)$, ajustando iterativamente los par√°metros $\theta$ para reducir el error.

### En resumen:

Es un m√©todo que permite a un modelo **aprender** los mejores valores de los par√°metros $\theta$, optimizando la predicci√≥n. El resultado del entrenamiento es el vector `theta_final`, que contiene los coeficientes √≥ptimos encontrados por el modelo.

---

## Regla de Actualizaci√≥n

$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)$ (Esta es la regla impl√≠cita que se describe)

### Componentes

**$\alpha$ (Tasa de Aprendizaje):**

* Controla el tama√±o del paso en cada iteraci√≥n.
* $\alpha$ alto ‚Üí puede hacer que el algoritmo no converja.
* $\alpha$ bajo ‚Üí puede hacer que la convergencia sea muy lenta.

**Derivada Parcial ($\frac{\partial}{\partial \theta_j} J(\theta)$):**

* Mide la direcci√≥n de mayor aumento de $J(\theta)$
* Al restarla, el modelo se mueve en direcci√≥n descendente (hacia el m√≠nimo).

---

## Gradiente Vectorizado

**F√≥rmula:**
$\nabla J(\theta) = \frac{1}{m} X^T (X\theta - y)$

### T√©rminos

1.  **$X\theta$**: Vector de predicciones para todas las observaciones.
2.  **$X\theta - y$**: Vector de errores residuales.
3.  **$X^T$**: Multiplicaci√≥n por la transpuesta de $X$ pondera los errores por cada caracter√≠stica.
4.  **$\frac{1}{m}$**: Promedia el gradiente sobre el conjunto de datos.

---

## ¬øQu√© representa el resultado?

El vector $\nabla J(\theta)$ nos dice:

* **Direcci√≥n:**
    * Si $\frac{\partial J}{\partial \theta_j} > 0$ = disminuir $\theta_j$
    * Si $\frac{\partial J}{\partial \theta_j} < 0$ = aumentar $\theta_j$

* **Magnitud:**
    * Cu√°nto influye ese $\theta_j$ en el error total

---

## Resumen

* **Objetivo:** Minimizar $J(\theta)$ ajustando $\theta$ para reducir errores
* **Tasa de Aprendizaje ($\alpha$):** Controla la velocidad de convergencia
* **Gradiente Vectorizado:** Forma eficiente de calcular el ajuste de todos los coeficientes a la vez

# üìò Pasos del Algoritmo de Regresi√≥n Lineal (California Housing)

## 1. üóÇÔ∏è Importaci√≥n de Datos

Importamos el dataset **California Housing** y las funciones necesarias de **NumPy**.

* `.data`: contiene las **caracter√≠sticas** (features), en forma de matriz ($X$).
* `.target`: contiene los **valores a predecir** (precios promedio de casas) ($Y$).

Se ajust√≥ la forma de $Y$ usando `np.reshape` para trabajar con matrices columna, sin alterar los datos.

---

## 2. ‚öñÔ∏è Normalizaci√≥n (Estandarizaci√≥n)

Calculamos la **media ($\mu$)** y **desviaci√≥n est√°ndar ($\sigma$)** de los datos originales para escalar las caracter√≠sticas. Esto es clave porque:

* Las variables originales tienen diferentes escalas.
* Sin escalar, el descenso de gradiente puede ser lento o ineficaz.
* El escalado mejora la velocidad y estabilidad del entrenamiento.
kv
**üîß Sin este paso, el modelo devolv√≠a `null` en `theta_final`, sin importar los hiperpar√°metros.**

---

## 3. ‚ûï Agregar Columna de Unos (Bias)

Se a√±adi√≥ una columna de unos a `X_scaled` para permitir que el modelo aprenda un **t√©rmino independiente** ($\theta_0$), haciendo que la recta **no tenga que pasar por el origen (0,0)**.
Esto da **flexibilidad** al modelo.

---

## 4. üîÆ Funci√≥n de Hip√≥tesis

Se implement√≥ la funci√≥n `calcular_hipotesis(X, theta)`:

* Predice valores continuos.
* Es la base de la f√≥rmula de **regresi√≥n lineal**:
    $h(\theta) = X \theta$

---

## 5. ‚ùå Funci√≥n de Coste (MSE)

Se implement√≥ la funci√≥n de coste: **Error Cuadr√°tico Medio (Mean Squared Error)**. $J(\theta)$.

Mide el **promedio del error al cuadrado** entre predicciones ($h_\theta(x^{(i)})$) y valores reales ($y^{(i)}$).

üìâ El objetivo es **minimizarla**:

* **MSE alta** ‚Üí el modelo se equivoca mucho.
* **MSE baja** ‚Üí el modelo est√° aprendiendo bien.

---

## 6. üîÅ Descenso de Gradiente

Se implement√≥ el **descenso de gradiente** para minimizar el error:

* Calcula predicciones, errores y gradiente ($\nabla J(\theta)$) en cada iteraci√≥n.
* Actualiza $\theta$ con la regla de aprendizaje:
    $\theta := \theta - \alpha \nabla J(\theta)$

Se definieron:

* `theta_inicial`: vector de ceros
* $\theta$ (tasa de aprendizaje)
* `n_iteraciones`

Tambi√©n se grafic√≥ el **historial de coste** para visualizar la convergencia del algoritmo.
![alt text](<Regresion_Lineal.py/Grafico Historial de coste.png>)
**La gr√°fica mostr√≥ que el coste convergi√≥ de forma estable a un valor aproximado de X.XX despu√©s de unas YYY iteraciones.** (<- **¬°RECUERDA REEMPLAZAR X.XX e YYY con tus valores!**)

---

## 7. ü§ñ Funci√≥n de Predicci√≥n

La funci√≥n `predecir(X_nuevos, theta_final, mu, sigma)` permite usar el modelo entrenado con **nuevos datos**:

1.  Escala los nuevos datos con $\mu$ y $\sigma$ del entrenamiento.
2.  A√±ade la columna de unos (bias).
3.  Aplica la f√≥rmula de regresi√≥n lineal ($h_\theta(X) = X \theta$) para predecir precios.

---

## ‚úÖ Conclusi√≥n

Este modelo permite predecir el precio promedio de casas en California usando regresi√≥n lineal multivariable, correctamente entrenada y escalada.
Con el descenso de gradiente y la MSE como gu√≠a, podemos ajustar $\theta$ hasta encontrar una soluci√≥n eficiente y precisa.

## Punto 1: Consolidar el Aprendizaje üß†

Dependiendo del valor de **alpha**, podemos observar cu√°nto tiempo tarda en **converger** el algoritmo.

* Si el **alpha es muy peque√±o**, el descenso de gradiente avanza muy lento.
* Si el **alpha es muy grande**, el algoritmo puede **omitir el aprendizaje** o incluso **divergir** (los valores crecen en lugar de estabilizarse).

üìà
*Figura 1*
![alt text](Regresion_Lineal.py/Figure_1.png)

Gracias a la comparaci√≥n de los valores en la gr√°fica, podemos encontrar un **alpha ideal**:


üìâ
*Figura 2*
![alt text](Regresion_Lineal.py/Figure_2.png)


Sin el **escalado**, el descenso de gradiente tarda m√°s o simplemente **no converge** (pueden aparecer datos 'null').
En cambio, cuando **escalamos las caracter√≠sticas**:

* Las variables tienen una magnitud parecida.
* El algoritmo avanza mejor.
* Se pueden usar valores de alpha m√°s grandes sin que se vuelva inestable.
* Todo converge de forma m√°s r√°pida y eficiente.


*Gr√°fico del historial de coste*
![alt text](<Regresion_Lineal.py/Grafico Historial de coste.png>)


### ‚úÖ En resumen:

* Escalar las caracter√≠sticas mejora la **eficiencia del algoritmo**.
* Elegir bien el valor de **alpha** hace que el modelo **converja m√°s r√°pido** sin salirse de control.

##  Punto 2: Evaluar el N√∫mero de Iteraciones ‚è±Ô∏è

Con distintos valores de alpha, podemos observar que, aproximadamente a partir de las 2500 iteraciones, las curvas comienzan a aplanarse. Esto indica que el algoritmo empieza a converger, ya que el coste deja de disminuir significativamente.

En mi experimento utilic√© 4000 iteraciones como n√∫mero total. Eleg√≠ este valor porque, al probar varios valores de alpha, quer√≠a asegurarme de observar con claridad en qu√© punto cada curva se aplanaba por completo. Esto me permiti√≥ identificar con mayor precisi√≥n cu√°ndo el algoritmo realmente comenzaba a converger en cada caso.

![alt text](Regresion_Lineal.py/Figure_1.2.png)



# Regresi√≥n Lineal: Ecuaci√≥n Normal vs Descenso de Gradiente  

## Estructura de la Matriz **X** y el Vector **y**  
### **Matriz X (Dise√±o)**  
- **Contenido**:  
  - Columna de **unos (1)** para el intercepto (`Œ∏‚ÇÄ`).  
  - Columnas de **caracter√≠sticas** (`X‚ÇÅ, X‚ÇÇ, ..., X‚Çô`).  
- **Dimensiones**:  
  `m √ó (n+1)`  *(m observaciones, n caracter√≠sticas)*  

### **Vector y (Objetivo)**  
- **Contenido**: Valores reales a predecir.  
- **Dimensiones**:  
  `m √ó 1`  

## üîç **Comparaciones Clave**  
### üìä Resultados Experimentales  
| **Escenario**            | Diferencia (Error) | Comparaci√≥n V√°lida |  
|--------------------------|--------------------|--------------------|  
| Theta GD (escalado) vs Theta EN (**sin escalar**) | ‚âà 111              | ‚ùå No (escalas distintas) |  
| Theta GD (escalado) vs Theta EN (**escalado**)    | ‚âà 9.9              | ‚úÖ S√≠               |  

### ‚ùì **Interpretaci√≥n**  
1. **Diferencia ‚âà 111**:  
   - Ilustra c√≥mo el **escalado afecta los valores absolutos de `Œ∏`**.  
   - **No es v√°lida t√©cnicamente** (comparar `Œ∏` en escalas diferentes no tiene sentido matem√°tico).  

2. **Diferencia ‚âà 9.9**:  
   - Muestra que el Descenso de Gradiente (**GD**) **no convergi√≥ totalmente** por falta de iteraciones.  

---

## üßÆ **Ecuaci√≥n Normal: F√≥rmula e Implementaci√≥n**  
### **F√≥rmula Anal√≠tica**  


Œ∏ = (X·µó X)‚Åª¬π X·µó y



### **Pasos de Implementaci√≥n**  
1. Calcular `X·µó X`.  
2. Invertir la matriz resultante.  
3. Multiplicar por `X·µó y`.  

## ‚öñÔ∏è **Pros y Contras**  
| **M√©todo**           | **Ecuaci√≥n Normal**                              | **Descenso de Gradiente**                     |  
|----------------------|--------------------------------------------------|-----------------------------------------------|  
| **Ventajas**         | - Soluci√≥n exacta en 1 paso.<br>- Sin hiperpar√°metros.<br>- No requiere escalado. | - Escalable a grandes `n`.<br>- Funciona incluso si `X·µó X` es singular. |  
| **Desventajas**      | - Coste `O(n¬≥)` (lento para `n > 10‚Å¥`).<br>- Falla si `X·µó X` no es invertible. | - Necesita ajustar `Œ±` e iteraciones.<br>- Requiere escalado para converger bien. |  

---

## üöÄ **¬øCu√°ndo Usar Cada M√©todo?**  
| **Criterio**               | **Ecuaci√≥n Normal**          | **Descenso de Gradiente**       |  
|----------------------------|------------------------------|---------------------------------|  
| **N√∫mero de caracter√≠sticas** | `n < 10‚Å¥`                 | `n ‚â• 10‚Å¥`                      |  
| **Estabilidad matricial**  | Evitar si `X·µó X` es singular | Funciona siempre               |  
| **Recursos computacionales** | Adecuado para CPU/GPU moderadas | Ideal para clusters distribuidos |  

---

**Notas Finales**:  
- Usar `np.linalg.pinv` en lugar de `inv` para manejar matrices singulares.  
- El escalado en GD es **cr√≠tico** para convergencia r√°pida y estable.  


# Tarea 5 Regresion Logisitica

con esta tarea vamos a comprender e implementar la Regresi√≥n Log√≠stica desde cero para la clasificaci√≥n binaria, entendiendo sus componentes matem√°ticos (funci√≥n sigmoide, hip√≥tesis, funci√≥n de coste de entrop√≠a cruzada), c√≥mo optimizarla con Descenso de Gradiente, y ser capaz de aplicarla y analizarla en un dataset.

# üìò Regresi√≥n Log√≠stica ‚Äì Conceptos Clave

## üîß Funciones a Implementar desde Cero
1. Funci√≥n Sigmoide g(z)
2. Funci√≥n de Hip√≥tesis h(X, Œ∏) (utiliza la sigmoide)
3. Funci√≥n de Coste J(X, y, Œ∏) (Entrop√≠a Cruzada Binaria)
4. Descenso de Gradiente (adaptado para clasificaci√≥n)
5. Funci√≥n de Predicci√≥n (aplica umbral 0.5 para clasificar en 0 o 1)

## üîÅ Funci√≥n Sigmoide

```math
g(z) = \frac{1}{1 + e^{-z}}
```

* Convierte cualquier n√∫mero (positivo o negativo) en un valor entre **0 y 1**.
* Tiene forma de **S**, y sus salidas son utiles por que pueden interpretarse como **probabilidades**.
* Por ejemplo, `g(0) = 0.5`, y si `z` es muy grande, `g(z)` se acerca a 1; si es muy peque√±o, se acerca a 0. 


---

## üß† Funci√≥n de Activaci√≥n Sigmoide en Regresi√≥n Log√≠stica

En la regresi√≥n log√≠stica, utilizamos la funci√≥n sigmoide como funci√≥n de activaci√≥n para modelar probabilidades. Este proceso se puede describir en los siguientes pasos:

1. **Calcular la Entrada `z`**

   Se calcula como el producto escalar entre los par√°metros y las caracter√≠sticas:

   $$
   z = \theta^T x
   $$

   > Este valor puede ser cualquier n√∫mero real: positivo, negativo o cero.

2. **Aplicar la Funci√≥n Sigmoide**

   La funci√≥n sigmoide toma `z` como entrada y devuelve un valor entre 0 y 1:

   $$
   g(z) = \frac{1}{1 + e^{-z}}
   $$

3. **Interpretar la Salida como Probabilidad**

   La salida de la funci√≥n sigmoide se interpreta como la **probabilidad estimada** de que la observaci√≥n pertenezca a la clase positiva (clase 1):

   $$
   h_\theta(x) = g(\theta^T x) \approx P(y = 1 \mid x; \theta)
   $$

---


## üß† Hip√≥tesis del Modelo

```math
h_\theta(x) = g(\theta^T x)
```

* Esta f√≥rmula se encarga de **hacer predicciones**.
* Multiplicamos los datos de entrada por los par√°metros (`Œ∏`) y aplicamos la funci√≥n sigmoide.
* El resultado es una **probabilidad** de que la salida sea `1`.
  Ejemplo: si `hŒ∏(x) = 0.8`, el modelo predice un **80% de probabilidad** de que `y = 1`.

---

## üí∞ Funci√≥n de Coste (Binary Cross-Entropy)

```math
J(\theta) = -\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right]
```

* Nos dice **qu√© tan mal est√° funcionando el modelo**.
* Penaliza m√°s fuerte cuando el modelo est√° seguro y se equivoca.
* Evitamos usar el **Error Cuadr√°tico Medio (MSE)**, porque no se adapta bien a clasificaci√≥n.

---

## üìâ Descenso de Gradiente

* Es el m√©todo que usamos para **encontrar los mejores par√°metros** (`Œ∏`).
* Calcula **qu√© tan lejos estamos** del m√≠nimo de la funci√≥n de coste.
* Da pasos peque√±os en la direcci√≥n correcta para **mejorar el modelo**.
* Aunque usamos la sigmoide, la f√≥rmula del gradiente se mantiene **muy parecida** a la de regresi√≥n lineal, lo cual simplifica la implementaci√≥n.

---

## üß≠ L√≠mite de Decisi√≥n

* Es la **frontera que separa las dos clases** (por ejemplo, spam vs no spam).
* Si `hŒ∏(x) ‚â• 0.5`, clasificamos como **1**; si es menor, como **0**.
* En un espacio 2D, es una **l√≠nea recta**; en espacios con m√°s dimensiones, es un **hiperplano**.

---

## ‚öôÔ∏è Consideraciones Pr√°cticas

* üîß **Umbral ajustable**: El valor de 0.5 puede cambiarse seg√∫n el problema (por ejemplo, para priorizar sensibilidad en medicina).
* üßØ **Regularizaci√≥n**: Podemos a√±adir t√©rminos (L1 o L2) a la funci√≥n de coste para **evitar el sobreajuste** (*overfitting*).
* üéØ **Clasificaci√≥n multiclase**: Se puede extender usando **Softmax** o estrategias **One-vs-Rest**.





### Comparaci√≥n con Otros M√©todos

| Caracter√≠stica              | Regresi√≥n Log√≠stica          | LDA / QDA                                                         |
| --------------------------- | ---------------------------- | ----------------------------------------------------------------- |
| Supuestos sobre los datos   | No hace suposiciones fuertes | Asume que los datos tienen forma de campana (distribuci√≥n normal) |
| Frontera de decisi√≥n        | Recta (lineal)               | Recta o curva (cuadr√°tica)                                        |
| C√≥mo calcula probabilidades | Directamente con la sigmoide | Basado en f√≥rmulas estad√≠sticas m√°s complejas                     |
 
## Pasos a seguir en la interacion de GD: 



### üîÑ Ciclo del Descenso de Gradiente

En cada iteraci√≥n del algoritmo de optimizaci√≥n se repiten los siguientes pasos:

1. **Calcular la Hip√≥tesis**
   Se calcula $z = X\theta$ (o $\theta^T X$ si $X$ es una sola muestra), y luego se aplica la funci√≥n sigmoide:

   $$
   h_\theta(X) = g(z)
   $$

   Esto nos da las probabilidades estimadas para cada muestra.

2. **Calcular el Error**
   Se obtiene la diferencia entre las predicciones y los valores reales:

   $$
   \text{errores} = h_\theta(X) - y
   $$

3. **Calcular el Gradiente**
   Se calcula usando la f√≥rmula vectorizada:

   $$
   \nabla J(\theta) = \frac{1}{m} X^T \cdot \text{errores}
   $$

4. **Actualizar los Par√°metros $\theta$**
   Se ajustan los par√°metros para minimizar la funci√≥n de coste:

   $$
   \theta := \theta - \alpha \cdot \nabla J(\theta)
   $$



# üìò Pasos del Algoritmo de Regresi√≥n Log√≠stica (`load_breast_cancer`)


## üî¢ Funci√≥n sigmoide

Para empezar, definimos la **funci√≥n sigmoide**, que convierte cualquier n√∫mero en un valor entre 0 y 1. Esto es muy √∫til para interpretar resultados como **probabilidades**.

Hice una lista de valores $z$ y apliqu√© la sigmoide para ver los resultados. Algunos puntos clave que me tengo que acordar:

* Si $z = 0$, la sigmoide da $0.5$.
* Si $z$ es muy grande, se acerca a $1.0$.
* Si $z$ es muy negativo, se acerca a $0.0$.

F√≥rmula:

$$
g(z) = \frac{1}{1 + e^{-z}}
$$

---

## üìà Funci√≥n de hip√≥tesis $h_\theta(x)$

Ya hab√≠amos visto esta funci√≥n antes, pero ahora la usamos junto con la sigmoide para obtener una **matriz de probabilidades**.

La f√≥rmula general es:

$$
h_\theta(x) = g(\theta^T x)
$$

---

## üí∞ Funci√≥n de coste (entrop√≠a cruzada binaria)

Para medir qu√© tan bien est√° aprendiendo el modelo, usamos la **entrop√≠a cruzada**, que castiga m√°s cuando el modelo se equivoca con confianza.

$$
J(\theta) = -\frac{1}{m} \sum \left[ y \log(h_\theta(x)) + (1 - y) \log(1 - h_\theta(x)) \right]
$$

Agregamos un peque√±o valor $\varepsilon$ para evitar errores como dividir entre cero o calcular $\log(0)$. Ese valor es tan peque√±o que no afecta el resultado final, pero ayuda a evitar problemas num√©ricos.

---

## üìâ Descenso de Gradiente (GD)

Esta funci√≥n sirve para ajustar los par√°metros $\theta$ y minimizar el error.

Primero calculamos el **gradiente**:

$$
\nabla J(\theta) = \frac{1}{m} X^T (h_\theta(X) - y)
$$

Y luego actualizamos los par√°metros con:

$$
\theta := \theta - \alpha \cdot \nabla J(\theta)
$$

Prob√© con varios valores de $\alpha$ (la tasa de aprendizaje) y vi cu√°l hac√≠a que la curva de p√©rdida bajara m√°s r√°pido y luego se estabilizara. Ese fue el mejor.

---

## üöÄ Empieza el entrenamiento

Cargu√© los datos desde `sklearn.datasets.load_breast_cancer` y segu√≠ estos pasos:

* Escal√© todas las caracter√≠sticas para que el modelo aprenda mejor.
* Agregu√© una columna de unos al dataset para que el modelo aprenda tambi√©n el **intercepto** $\theta_0$, lo que le da m√°s libertad para ajustar la curva.
* Us√© un valor de $\alpha$ que funcionara bien y un n√∫mero razonable de iteraciones (basado en c√≥mo se ve la curva de p√©rdida).

Todo esto me permiti√≥ entrenar el modelo y practicar la funci√≥n `predict`.

### Visualizaci√≥n del entrenamiento

Compar√© la evoluci√≥n del error y el efecto de distintos valores de $\alpha$:

![Curva de p√©rdida vs iteraciones](Regresion_Logisitica/Figure_2.png)

![Comparaci√≥n de tasas de aprendizaje](Regresion_Logisitica/Figure_1.png)

---

## ‚úÖ Funci√≥n predecir

Con la hip√≥tesis $h_\theta(x)$, calculamos probabilidades y luego usamos un **umbral** de 0.5 para convertir eso en una decisi√≥n:

* Si $h_\theta(x) \geq 0.5$ ‚Üí predice clase **1**.
* Si $h_\theta(x) < 0.5$ ‚Üí predice clase **0**.

Esto nos da una predicci√≥n binaria clara.

---

## üéØ Accuracy del modelo

Para saber qu√© tan bien aprendi√≥ el modelo, calcul√© el **accuracy**, que es el porcentaje de predicciones correctas.

En este caso, obtuve:

$$
\text{Accuracy} = 97.01\%
$$

Tambi√©n prob√© una forma alternativa de calcularlo con menos pasos, solo para recordar que se puede hacer lo mismo de distintas maneras.

---




## ü§î ¬øPor qu√© usamos la entrop√≠a cruzada binaria? (BCE vs MSE)

Usamos la **entrop√≠a cruzada binaria** (BCE) en regresi√≥n log√≠stica porque se ajusta muy bien al funcionamiento de la **funci√≥n sigmoide**, que nos da una probabilidad entre 0 y 1. En problemas de clasificaci√≥n binaria, como este, donde solo existen dos posibles clases (0 o 1), la BCE se adapta perfectamente, ya que estamos modelando **probabilidades**.

La BCE tiene la ventaja de penalizar m√°s fuertemente cuando el modelo se equivoca, especialmente cuando est√° muy seguro de su predicci√≥n y se equivoca. Esto ayuda a que el modelo aprenda m√°s r√°pido y mejor. En cambio, el **error cuadr√°tico medio** (MSE) no penaliza de la misma manera y no se comporta tan bien cuando estamos trabajando con **probabilidades**, ya que no mide la calidad de las predicciones de manera tan eficiente como la BCE.

En resumen, la BCE es m√°s adecuada para este tipo de problemas, porque no solo mide la diferencia entre las predicciones y las clases reales, sino que tambi√©n penaliza m√°s fuertemente los errores cuando el modelo est√° muy confiado y equivocado.

## Cuadro comparativo entre BCE y MSE

| **Caracter√≠stica**          | **Entrop√≠a Cruzada Binaria (BCE)**                                                              | **Error Cuadr√°tico Medio (MSE)**                                                                |
| --------------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| **Uso principal**           | Problemas de clasificaci√≥n binaria (0 o 1)                                                      | Problemas de regresi√≥n (predicciones continuas)                                                 |
| **Salida del modelo**       | Probabilidades (0 a 1)                                                                          | Cualquier valor real (n√∫meros continuos)                                                        |
| **F√≥rmula**                 | $-y \log(h) - (1 - y) \log(1 - h)$                                                              | $\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$                                                  |
| **Qu√© mide**                | Cu√°nta "sorpresa" hay entre la predicci√≥n y el valor real                                       | La diferencia entre la predicci√≥n y el valor real                                               |
| **Penalizaci√≥n de errores** | Penaliza fuertemente los errores de alta certeza (predicciones incorrectas con mucha confianza) | Penaliza m√°s los errores grandes, pero no lo suficiente para problemas de clasificaci√≥n binaria |
| **Ventajas**                | Se ajusta a problemas binarios, es estad√≠sticamente coherente, y ayuda al aprendizaje eficiente | Es simple y f√°cil de calcular, pero no es adecuado para probabilidades                          |
| **Desventajas**             | No es adecuado para regresi√≥n, y puede ser sensible a valores muy extremos                      | No es ideal para clasificaci√≥n binaria, ya que no maneja bien las probabilidades                |

---

## **Resumen f√°cil**:

* **BCE** es la mejor opci√≥n cuando est√°s trabajando con **probabilidades y clasificaci√≥n binaria** (0 o 1).
* **MSE** es mejor para **predicciones continuas** (por ejemplo, en regresi√≥n), pero no se adapta bien a los problemas de probabilidad.
---

## ü§î ¬øQu√© significa que una funci√≥n de coste sea "no convexa"?

Si usamos **MSE** (Error Cuadr√°tico Medio) en lugar de **BCE** (Entrop√≠a Cruzada Binaria), la funci√≥n de coste puede volverse **no convexa**. Esto sucede porque el **MSE** no se ajusta tan bien a la funci√≥n sigmoide, y puede generar una funci√≥n de coste con **m√∫ltiples m√≠nimos locales**. Esto dificulta encontrar el mejor valor para los par√°metros del modelo.

El **descenso de gradiente** es un algoritmo que busca minimizar la funci√≥n de coste, es decir, encuentra el m√≠nimo de la funci√≥n para que el modelo sea lo m√°s preciso posible.

### ¬øQu√© significa que una funci√≥n de coste sea "convexa"?

Cuando una funci√≥n es **convexa**, tiene una forma de **cuenco** o "U". En este caso, la funci√≥n solo tiene un **m√≠nimo global** (el fondo del cuenco), y no hay otros **picos** o "colinas" que distraigan el proceso de b√∫squeda del m√≠nimo.

Cuando la funci√≥n es convexa, **el descenso de gradiente** siempre llevar√° al **m√≠nimo global**. No importa desde qu√© punto empieces, siempre ir√°s hacia el punto m√°s bajo de la funci√≥n.

### ¬øQu√© pasa si la funci√≥n de coste no es convexa?

Si la funci√≥n **no es convexa** (como sucede con el **MSE** en regresi√≥n log√≠stica), entonces la funci√≥n de coste puede tener **m√∫ltiples m√≠nimos locales** (como monta√±as y valles). El **descenso de gradiente** podr√≠a quedarse atrapado en un **m√≠nimo local** y no encontrar el mejor valor (m√≠nimo global).

---

### üèûÔ∏è Ejemplo Visual

**Funci√≥n Convexa (como BCE):**

Imagina que est√°s en un campo con una sola gran colina que desciende en todas direcciones (funci√≥n convexa). No importa en qu√© punto empieces, siempre **descender√°s** hacia el punto m√°s bajo, que es el **m√≠nimo global**.

**Funci√≥n No Convexa (como MSE):**

Ahora imagina un campo con varias monta√±as y valles (funci√≥n no convexa). Si te encuentras en un valle peque√±o (m√≠nimo local), podr√≠as pensar que has encontrado el mejor lugar. Sin embargo, hay un valle m√°s profundo en otro lugar, el **m√≠nimo global**. Si el descenso de gradiente se queda atrapado en el primer valle, no podr√° encontrar el m√≠nimo global.

---

### üìù Resumen en palabras sencillas:

El **descenso de gradiente** busca el punto m√°s bajo (m√≠nimo) de una **funci√≥n de coste** ajustando los par√°metros del modelo.

* Si la funci√≥n es **convexa** (como la BCE), el descenso de gradiente siempre encontrar√° el **m√≠nimo global**.
* Si la funci√≥n es **no convexa** (como con MSE en regresi√≥n log√≠stica), el descenso de gradiente podr√≠a quedarse atrapado en **m√≠nimos locales** y no encontrar el mejor m√≠nimo global.

---




## üìå ¬øPor la que la Entrop√≠a Cruzada Binaria (BCE) es "la elegida" para modelos como la Regresi√≥n Log√≠stica.  (Conexi√≥n con MLE)

Una de las razones m√°s importantes para usar la **Entrop√≠a Cruzada Binaria (BCE)** en regresi√≥n log√≠stica es que **est√° directamente relacionada con un principio estad√≠stico muy fuerte llamado *Estimaci√≥n de M√°xima Verosimilitud (MLE)*.**

---

### üß† ¬øQu√© busca la MLE?

Queremos encontrar los par√°metros del modelo, representados como **Œ∏**, que hagan que los **datos de entrenamiento que ya observamos** (las verdaderas etiquetas `y`) sean **lo m√°s probables posible** seg√∫n el modelo. Es decir, que nuestro modelo diga:

> "¬°S√≠, con estos par√°metros, es muy probable que haya visto exactamente estos datos!"

---

### üìä ¬øC√≥mo se calcula esa probabilidad?

Para una sola observaci√≥n $(x^{(i)}, y^{(i)})$, la probabilidad seg√∫n el modelo es:

* Si $y^{(i)} = 1$, entonces la probabilidad es $h_{\theta}(x^{(i)})$
* Si $y^{(i)} = 0$, entonces la probabilidad es $1 - h_{\theta}(x^{(i)})$

Todo esto se puede escribir as√≠:

$$
P(y^{(i)}|x^{(i)};\theta) = (h_{\theta}(x^{(i)}))^{y^{(i)}} (1 - h_{\theta}(x^{(i)}))^{1 - y^{(i)}}
$$

> *Compru√©balo t√∫ mismo: si y = 1, queda solo hŒ∏(x); si y = 0, queda 1 ‚àí hŒ∏(x)*.

---

### üì¶ Verosimilitud total (Likelihood)

Ya que asumimos que las observaciones son independientes, multiplicamos todas las probabilidades:

$$
L(\theta) = \prod_{i=1}^{m} P(y^{(i)}|x^{(i)};\theta)
$$

---

### üìà Log-Verosimilitud

Trabajar con productos es inc√≥modo, as√≠ que tomamos el logaritmo (para convertir productos en sumas):

$$
\log L(\theta) = \sum_{i=1}^{m} \left[ y^{(i)} \log(h_{\theta}(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\theta}(x^{(i)})) \right]
$$

---

### üí° ¬°Sorpresa! ¬°Esta f√≥rmula ya la conoces!

La funci√≥n de coste de **Entrop√≠a Cruzada Binaria (BCE)** es exactamente la **negaci√≥n** del promedio de esa log-verosimilitud:

$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_{\theta}(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\theta}(x^{(i)})) \right]
$$

---

### üß† En resumen:

* Maximizar la log-verosimilitud (objetivo de MLE) es **equivalente a minimizar la funci√≥n BCE**.
* El signo negativo y el factor $\frac{1}{m}$ solo convierten el problema de maximizar en uno de **minimizaci√≥n promedio**, que es justo lo que usa el **descenso de gradiente**.
* Esto le da a la BCE una base te√≥rica muy s√≥lida, **adem√°s de que es convexa** (lo cual es genial para evitar m√≠nimos locales).

---

### üìå Relaci√≥n entre la teor√≠a y la implementaci√≥n

#### 1. **Funci√≥n de hip√≥tesis `hŒ∏(x)`**

```python
def calcular_hipotesis(X, theta):
    Z_vector = X @ theta
    Z_vector_prob = sigmoid(Z_vector)
    return Z_vector_prob
```

Esta funci√≥n calcula **hŒ∏(x)**, que representa la **probabilidad** de que una muestra pertenezca a la clase 1. Esto es precisamente lo que necesita el MLE: una funci√≥n que d√© **probabilidades condicionales P(y|x;Œ∏)**.

---

#### 2. **Funci√≥n de coste `calcular_coste`**

```python
def calcular_coste(X, y, theta):
    ...
    coste = - (1 / m) * sum_total
    return coste
```

Esta es **exactamente** la f√≥rmula de la **Entrop√≠a Cruzada Binaria (BCE)**, que como dijimos en la teor√≠a, es la **forma negativa y promedio de la log-verosimilitud**:

* **MLE:** maximiza la log-verosimilitud.
* **BCE:** minimiza el coste (‚àílog-verosimilitud promedio).

Por eso, esta funci√≥n de coste **implementa MLE en forma negativa**, adaptada para optimizaci√≥n v√≠a descenso de gradiente.

---

#### 3. **Descenso de Gradiente**

```python
theta = theta - alpha * gradiente
```

### üß† En resumen 

### Justificaci√≥n estad√≠stica de la funci√≥n de coste

Una raz√≥n fundamental para utilizar la **Entrop√≠a Cruzada Binaria** en regresi√≥n log√≠stica es su s√≥lida base te√≥rica en la **Estimaci√≥n de M√°xima Verosimilitud (MLE)**. En este modelo, queremos encontrar los par√°metros Œ∏ que **maximicen la probabilidad de haber observado las etiquetas reales del entrenamiento**, dado nuestro modelo. Esto se logra **maximizando la log-verosimilitud**, la cual, al tomar su forma negativa y promedio, **se convierte en la funci√≥n de coste que usamos: la BCE**.

Por tanto, el proceso de entrenamiento (con `calcular_coste` y `descenso_gradiente`) **no solo busca minimizar un error arbitrario, sino que est√° directamente fundamentado en probabilidad y estad√≠stica**: est√° **maximizando la verosimilitud de los datos observados**.

---

# ADELANTO INVESTIGACION PARA SIGUIENTE TAREA:

---

## üß† ¬øEs la exactitud siempre la mejor m√©trica?

No. La **exactitud (accuracy)** solo mide el porcentaje de predicciones correctas. Pero en casos de **clases desbalanceadas**, puede dar una **falsa sensaci√≥n de buen rendimiento**.

### üìå Ejemplo cl√°sico:

Sup√≥n que estamos dise√±ando un test para una **enfermedad rara** que afecta al 1% de la poblaci√≥n.
De 1,000 personas, solo 10 la tienen.

Un modelo que **siempre predice "no tiene la enfermedad"** acertar√° en 990 casos.

* Exactitud = (990 aciertos) / 1000 = **99%**

¬°Parece genial! Pero‚Ä¶

* No detect√≥ **ni un solo caso verdadero**.
* **Recall = 0%**

Esto lo vuelve **in√∫til** para el prop√≥sito real: **detectar la enfermedad**.

---

## üß© Matriz de Confusi√≥n: ¬øQu√© significa cada caso?

Cuando entrenas un modelo para clasificar entre dos opciones (por ejemplo, **"enfermo"** o **"no enfermo"**), hay cuatro formas posibles en las que tu predicci√≥n puede coincidir (o no) con la realidad:

| Nombre üìå                     | Realidad üß† | Predicci√≥n ü§ñ       | ¬øQu√© pas√≥?                                                                                        |
| ----------------------------- | ----------- | ------------------- | ------------------------------------------------------------------------------------------------- |
| ‚úÖ **Verdadero Positivo (TP)** | 1 (Enfermo) | 1 (Predijo enfermo) | El paciente **ten√≠a la enfermedad** y el modelo **lo detect√≥ correctamente**. Perfecto.           |
| ‚úÖ **Verdadero Negativo (TN)** | 0 (Sano)    | 0 (Predijo sano)    | El paciente **no ten√≠a la enfermedad** y el modelo **tambi√©n dijo que no**. Muy bien.             |
| ‚ö†Ô∏è **Falso Positivo (FP)**    | 0 (Sano)    | 1 (Predijo enfermo) | El paciente **estaba sano**, pero el modelo **dijo que estaba enfermo**. Una **falsa alarma**.    |
| ‚ùå **Falso Negativo (FN)**     | 1 (Enfermo) | 0 (Predijo sano)    | El paciente **s√≠ ten√≠a la enfermedad**, pero el modelo **no la detect√≥**. El error **m√°s grave**. |

---

### üß† ¬øPor qu√© son importantes?

* **TP y TN** son los **aciertos** del modelo.
* **FP y FN** son los **errores**.
* A partir de ellos, se calculan m√©tricas como **precisi√≥n**, **recall** y **F1-score**, que permiten entender mejor c√≥mo se comporta el modelo en **situaciones cr√≠ticas**.

---

¬øQuieres que agregue una visualizaci√≥n estilo matriz con estos valores colocados en una tabla tipo cuadr√≠cula (como un diagrama)?


## üìå M√©tricas clave

### üéØ Precisi√≥n (Precision)

> ¬øDe los que dije que eran positivos, cu√°ntos lo eran realmente?

**F√≥rmula:**
**Precisi√≥n = TP / (TP + FP)**

**Importante cuando:** El coste de un **falso positivo** es alto.
**Ejemplos:**

* Clasificaci√≥n de spam
* Recomendaciones de productos
* Sistema judicial (condenar a un inocente)

---

### üîç Recall (Sensibilidad, Exhaustividad)

> ¬øDe todos los que realmente eran positivos, cu√°ntos detect√©?

**F√≥rmula:**
**Recall = TP / (TP + FN)**

**Importante cuando:** El coste de un **falso negativo** es alto.
**Ejemplos:**

* Detecci√≥n de enfermedades graves
* Fraude bancario
* Alerta temprana de incendios o cat√°strofes

---

### ‚öñÔ∏è F1-Score (Balance entre precisi√≥n y recall)

> ¬øC√≥mo consigo un equilibrio justo entre precisi√≥n y recall?

**F√≥rmula:**
**F1 = 2 \* (Precision \* Recall) / (Precision + Recall)**

* Es la **media arm√≥nica**: si una m√©trica es baja, el F1 tambi√©n ser√° bajo.
* √ötil con **clases desbalanceadas**, o cuando es importante tener un **buen balance**.

---


Siguiendo con el ejemplo de la **enfermedad rara** (donde el 1% tiene la enfermedad y el 99% no):

Imagina que tenemos un modelo que **siempre predice "no tiene la enfermedad"**:

| **Resultado**               | **Realidad** | **Predicci√≥n** | **Cantidad** |
| --------------------------- | ------------ | -------------- | ------------ |
| **Verdadero Positivo (TP)** | 1            | 1              | 0            |
| **Falso Positivo (FP)**     | 0            | 1              | 0            |
| **Falso Negativo (FN)**     | 1            | 0              | 10           |
| **Verdadero Negativo (TN)** | 0            | 0              | 990          |

### **Accuracy**:

La **Accuracy** se calcula como:

**Accuracy** = (TP + TN) / Total = (0 + 990) / 1000 = **99%**
¬°Una **Accuracy** del 99%, que parece excelente!

---

Sin embargo, si nos fijamos en **Recall** para la clase **"tiene la enfermedad"**, vemos lo siguiente:

### **Recall (Sensibilidad)**:

**Recall** = TP / (TP + FN) = 0 / (0 + 10) = **0%**
Esto significa que el modelo **no detecta ninguna persona enferma**, lo cual hace que **no sea √∫til para el diagn√≥stico** de la enfermedad.

---
## ‚úÖ Conclusi√≥n

* Usa **Accuracy** solo si las clases est√°n balanceadas.
* Usa **Precisi√≥n** si **falsos positivos** son costosos.
* Usa **Recall** si **falsos negativos** son peligrosos.
* Usa **F1-Score** cuando **ambos errores son cr√≠ticos** o cuando hay **desequilibrio de clases**.
---


### ‚úÖ **¬øC√≥mo resumir la utilidad de Precisi√≥n, Recall y F1-Score?**

* **Precisi√≥n** te dice:

  > ‚Äú¬øCu√°ntos de los que el modelo **dijo que eran positivos**, **realmente lo eran**?‚Äù
  > Es √∫til cuando **no quieres dar falsas alarmas** (falsos positivos).
  > Ejemplo: Un filtro de spam ‚Äî mejor no meter correos importantes en la carpeta de spam.

* **Recall** te dice:

  > ‚Äú¬øCu√°ntos de los que **realmente eran positivos**, **logramos detectar**?‚Äù
  > Es √∫til cuando **no quieres dejar pasar casos importantes** (falsos negativos).
  > Ejemplo: Diagn√≥stico de una enfermedad ‚Äî mejor detectar todos los casos posibles, aunque te equivoques con algunos sanos.

* **F1-Score**:

  > Es una media entre precisi√≥n y recall.
  > Es √∫til cuando hay **desbalance de clases** o cuando **necesitas un equilibrio** entre no dar falsas alarmas y no dejar pasar casos.
  > Ejemplo: Detecci√≥n de fraude ‚Äî necesitas capturar la mayor√≠a de fraudes (recall), pero tambi√©n evitar acusar a gente inocente (precisi√≥n).

---

### üß† **¬øPor qu√© el F1-Score intenta balancearlas?**

Porque en muchos problemas **no basta con solo precisi√≥n o solo recall**. Si una es muy alta y la otra muy baja, el modelo puede estar fallando en algo importante.
**F1 te obliga a que ambas sean razonablemente buenas.**

---


# Tarea 6 Diagn√≥stico y Control del Modelo: Overfitting y Regularizaci√≥n

## ¬øQu√© es el Overfitting (Sobreajuste)?

El **overfitting** ocurre cuando un modelo aprende *demasiado bien* los datos con los que fue entrenado. No solo aprende los **patrones generales**, sino tambi√©n las **particularidades, errores o ruido** de esos datos. Como consecuencia, **pierde capacidad para generalizar** a nuevos datos: **memoriza** en lugar de *entender*.

> üìå **Definici√≥n simple**: El modelo rinde bien en los datos de entrenamiento, pero falla con datos nuevos porque ha memorizado en lugar de aprender.

---

## Causas Comunes del Overfitting

| Causa                            | Explicaci√≥n                                                                                      | Ejemplo                                                                                          |
| -------------------------------- | ------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------ |
| **Modelo demasiado complejo**    | Tiene demasiados par√°metros o flexibilidad en relaci√≥n con la cantidad/simplicidad de los datos. | Ajustar un polinomio de grado 10 a 15 puntos que siguen una l√≠nea recta.                         |
| **Pocos datos de entrenamiento** | No hay suficiente informaci√≥n para aprender patrones generalizables. El modelo ajusta el ruido.  | Con solo 5 ejemplos, el modelo puede "pasar por todos los puntos", pero fallar con datos nuevos. |
| **Ruido en los datos**           | El modelo aprende errores o anomal√≠as como si fueran patrones reales.                            | Datos mal etiquetados o con errores que el modelo intenta memorizar.                             |
| **Entrenamiento excesivo**       | Aun si el modelo es adecuado, entrenarlo demasiado tiempo hace que memorice.                     | Despu√©s de muchas √©pocas, el modelo deja de aprender y empieza a copiar el entrenamiento.        |

---

## Sobre la cantidad de datos

* Si **tienes pocos datos** y un **modelo muy complejo**, este podr√≠a *ajustarse perfectamente* a esos pocos puntos.
* Pero eso no implica que **haya aprendido bien**.
* Al llegar nuevos datos, ese ajuste perfecto puede resultar **muy pobre**.

> üéØ **Conclusi√≥n**: Con pocos datos, un modelo complejo **no tiene suficiente evidencia** para distinguir entre **se√±al** (patr√≥n general) y **ruido** (casualidades del conjunto de entrenamiento).

---

## üß† Resumen de las causas del Overfitting

* Modelo demasiado complejo para los datos o la tarea.
* Conjunto de datos de entrenamiento muy peque√±o.
* Presencia excesiva de ruido en los datos.
* Entrenamiento durante demasiadas iteraciones (√©pocas).

---

## üß© Underfitting (Subajuste)

El **underfitting** ocurre cuando un modelo es **demasiado simple** para captar la complejidad real de los datos de entrenamiento. Como resultado:

* **No aprende bien** los patrones presentes.
* **Comete muchos errores**, incluso con los datos con los que fue entrenado.
* Falla en generalizar a nuevos datos porque **ni siquiera ha logrado aprender los datos originales**.

> üìå **Definici√≥n simple**: El modelo no est√° aprendiendo ni siquiera los patrones de entrenamiento, y por eso comete errores altos *en todo*.

---

## ¬øC√≥mo se ve el underfitting?

| Tipo de error                  | Resultado | ¬øPor qu√© ocurre?                                                                     |
| ------------------------------ | --------- | ------------------------------------------------------------------------------------ |
| **Error en entrenamiento**     | **Alto**  | El modelo no logra ajustarse a los patrones presentes en los datos.                  |
| **Error en prueba/validaci√≥n** | **Alto**  | Si no entendi√≥ los datos de entrenamiento, dif√≠cilmente podr√° entender datos nuevos. |

> ‚ùó El rendimiento es pobre de forma consistente, tanto en entrenamiento como en validaci√≥n.

---

## Causas Comunes del Underfitting

| Causa                                  | Explicaci√≥n                                                                                       | Ejemplo                                                           |
| -------------------------------------- | ------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **Modelo demasiado simple**            | Tiene pocos par√°metros o una estructura r√≠gida que no puede capturar la complejidad de los datos. | Usar una l√≠nea recta para datos que tienen una forma curva.       |
| **Datos de entrada poco informativos** | Las variables (features) no contienen suficiente informaci√≥n relevante.                           | Predecir precios de casas solo con el n√∫mero de ventanas.         |
| **Entrenamiento insuficiente**         | El modelo no tuvo suficiente tiempo o ciclos de entrenamiento para aprender los patrones.         | Cortar el entrenamiento antes de que el error baje lo suficiente. |

---

## üß† Resumen

* El underfitting es lo **opuesto** al overfitting.
* El modelo **no aprende bien** ni siquiera los datos de entrenamiento.
* Puede deberse a una arquitectura demasiado simple, mala calidad de datos o entrenamiento insuficiente.
* Los **errores ser√°n altos en todas las fases**: tanto en entrenamiento como en prueba.


# üìö Bias-Variance Tradeoff (Compromiso Sesgo-Varianza)



## üéØ ¬øQu√© es el Bias-Variance Tradeoff?

Es el equilibrio que buscamos entre dos fuentes de error en los modelos de Machine Learning:

* **Sesgo (Bias)**: Error por suposiciones demasiado simplistas.
* **Varianza (Variance)**: Error por sensibilidad excesiva a los datos de entrenamiento.

Nuestro objetivo es **minimizar el error total** que un modelo comete en datos que nunca ha visto antes.

---

## üß† Tipos de Bias en Machine Learning

| Concepto                 | ¬øQu√© es?                                  | ¬øD√≥nde aparece?        |
| ------------------------ | ----------------------------------------- | ---------------------- |
| **Bias como intercepto** | Columna de unos ‚Üí par√°metro Œ≤‚ÇÄ            | Modelos lineales, RN   |
| **Bias como sesgo**      | Suposiciones err√≥neas ‚Üí error sistem√°tico | Bias-Variance Tradeoff |

---

## üîç 1. **Bias como Par√°metro (Intercepto)**

* Se refiere al t√©rmino independiente en modelos lineales:

  Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + Œ≤‚ÇÇX‚ÇÇ + ‚Ä¶

* Es un **par√°metro aprendido** por el modelo.

* Se introduce agregando una **columna de unos** a la matriz de entrada X.

---

## üîç 2. **Bias como Error Sistem√°tico**

* Error causado por suposiciones r√≠gidas (por ejemplo, que todo es lineal).
* Se define como la **diferencia entre la predicci√≥n promedio del modelo y la realidad**.
* Es una **medida de error te√≥rico**, no un par√°metro.

---

## üìà ¬øQu√© ocurre con Sesgo y Varianza?

| Tipo de Modelo           | Sesgo | Varianza | Resultado             |
| ------------------------ | ----- | -------- | --------------------- |
| Muy simple               | Alto  | Bajo     | Underfitting          |
| Muy complejo             | Bajo  | Alto     | Overfitting           |
| Equilibrado (sweet spot) | Medio | Medio    | Generalizaci√≥n √≥ptima |

---

## üìâ Error Total

El error total en un modelo puede expresarse como:

**Error total = Sesgo¬≤ + Varianza + Error irreducible**

* **Sesgo¬≤**: Error por suposiciones err√≥neas (underfitting).
* **Varianza**: Error por sobreajuste al conjunto de entrenamiento (overfitting).
* **Error irreducible**: Ruido inherente al problema. No se puede eliminar.

---

## ‚öñÔ∏è El Compromiso

* Reducir **sesgo** suele **aumentar varianza**.
* Reducir **varianza** suele **aumentar sesgo**.
* El punto √≥ptimo (üí° *sweet spot*) es donde el error total es m√≠nimo y el modelo **generaliza bien**.

---

## üõ†Ô∏è ¬øC√≥mo controlar la complejidad?

T√∫ eliges la complejidad del modelo con las siguientes "perillas":

* **Tipo de modelo**: lineal vs red neuronal, √°rbol de decisi√≥n, etc.
* **Hiperpar√°metros**:

  * Grado del polinomio
  * Profundidad del √°rbol
  * Capas y neuronas en redes
* **Regularizaci√≥n**: penaliza la complejidad (controla el overfitting).

---

## üìä ¬øC√≥mo encontrar el sweet spot?

1. **Divisi√≥n de Datos**:

   * Entrenamiento: aprende los par√°metros.
   * Validaci√≥n: elige el mejor modelo/hiperpar√°metro.
   * Prueba: eval√∫a el modelo final.

2. **Curvas de Aprendizaje**:

   * Gr√°fica de error de entrenamiento y validaci√≥n al aumentar la complejidad.
   * El sweet spot suele estar donde el error de validaci√≥n es m√≠nimo.

3. **Validaci√≥n Cruzada (Cross-Validation)**:

   * Eval√∫a el rendimiento de forma m√°s robusta.
   * Recomendado para seleccionar hiperpar√°metros con mayor confianza.

---

## üß© Conclusi√≥n

* El **bias-variance tradeoff** es uno de los conceptos m√°s fundamentales para entender por qu√© un modelo no est√° funcionando bien.
* **No hay una f√≥rmula m√°gica** para saber cu√°nta complejidad es ideal: lo descubrimos **experimentando** y validando.
* Tu tarea como modelador es ajustar esa complejidad para que el modelo **aprenda lo suficiente pero no memorice**.

Este resumen sobre *underfitting* y *overfitting* organiza de forma clara las estrategias clave para manejar ambos problemas, equilibrando la complejidad y la generalizaci√≥n del modelo. Aqu√≠ est√° embellecido y estructurado para tu `README.md`, con t√≠tulos, listas, preguntas ret√≥ricas y una redacci√≥n clara:

---

# Estrategias Generales para Combatir el Underfitting y el Overfitting

En el entrenamiento de modelos de machine learning, uno de los principales desaf√≠os es encontrar el equilibrio entre **subajuste (underfitting)** y **sobreajuste (overfitting)**. A continuaci√≥n, se presentan estrategias pr√°cticas y razonadas para abordar cada caso.

---

## ¬øC√≥mo combatir el UNDERFITTING?

El underfitting ocurre cuando un modelo es demasiado simple para capturar los patrones subyacentes de los datos. Algunas estrategias efectivas incluyen:

### 1. Aumentar la complejidad del modelo

* **Elegir un modelo m√°s expresivo**:

  * Si usas regresi√≥n lineal, prueba con regresi√≥n polin√≥mica (a√±adiendo t√©rminos como x¬≤, x¬≥, etc.).
    üëâ *¬øQu√© hiperpar√°metro controlar√≠as aqu√≠?* El grado del polinomio.
  * Si usas √°rboles de decisi√≥n, permite que crezcan m√°s profundos.
  * Considera modelos m√°s complejos como redes neuronales o SVM con kernel no lineal.

### 2. Ingenier√≠a de caracter√≠sticas (Feature Engineering)

* Agrega nuevas caracter√≠sticas relevantes.
* Introduce combinaciones de variables (interacciones).
* Aseg√∫rate de incluir representaciones adecuadas del dominio del problema.

### 3. Asegurar entrenamiento suficiente

* Aumenta el n√∫mero de √©pocas o iteraciones.
* Verifica que el algoritmo haya tenido oportunidad de converger.

### 4. Ajustar la regularizaci√≥n

* Si est√°s aplicando regularizaci√≥n (por ejemplo, con par√°metro Œª), revisa que **no sea excesiva**.
  Un Œª demasiado alto puede hacer que el modelo sea demasiado simple.
  üëâ *Reducir Œª puede permitirle aprender m√°s patrones reales.*

---

## ¬øC√≥mo combatir el OVERFITTING?

El overfitting ocurre cuando el modelo aprende demasiado bien los datos de entrenamiento, incluyendo el ruido o las particularidades del conjunto, y falla al generalizar. Estas estrategias ayudan a evitarlo:

### 1. Regularizaci√≥n

* Penaliza los valores grandes de los par√°metros del modelo (Œ∏) para evitar que se ajusten demasiado a los datos.

  * **L1 (Lasso)**: puede llevar a modelos m√°s escuetos (sparse).
  * **L2 (Ridge)**: reduce gradualmente todos los pesos.

  üëâ *¬øC√≥mo ayuda esto?* Reduce la complejidad efectiva del modelo sin cambiar su estructura base.

### 2. Selecci√≥n de caracter√≠sticas o reducci√≥n de dimensionalidad

* Elimina variables irrelevantes o ruidosas.
* Aplica t√©cnicas como **PCA (An√°lisis de Componentes Principales)** o m√©todos de selecci√≥n automatizada para reducir la dimensionalidad.

### 3. Early Stopping (Detenci√≥n Temprana)

* Monitorea el error en el conjunto de validaci√≥n durante el entrenamiento.
* Si el error de validaci√≥n comienza a aumentar mientras el error de entrenamiento sigue bajando, det√©n el entrenamiento.
  üëâ *Esto previene que el modelo se "memorice" los datos.*

### 4. M√°s datos de entrenamiento

* Cuantos m√°s ejemplos diversos tengas, m√°s robusto ser√° el modelo.
* Ayuda a reducir el sesgo inducido por un conjunto peque√±o o no representativo.

### 5. Filtrar o limpiar datos (con cuidado)

* Identifica y elimina outliers si est√°n claramente afectando el modelo.
  ‚ö†Ô∏è *Hazlo solo si puedes justificarlo bien*, ya que podr√≠as introducir sesgo si te excedes.

### 6. M√©todos de ensamblaje (Ensemble Methods)

* Combina m√∫ltiples modelos para obtener predicciones m√°s estables y precisas:

  * **Bagging** (como Random Forests) reduce la varianza.
  * **Boosting** (como XGBoost) puede mejorar el sesgo y la varianza a la vez.

---


### Comparaci√≥n de Estrategias contra Underfitting y Overfitting

| Categor√≠a                                | Combatir Underfitting                                                               | Combatir Overfitting                                                |
| ---------------------------------------- | ----------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| **Complejidad del Modelo**               | Aumentar complejidad (p. ej., redes m√°s grandes, polinomios, √°rboles m√°s profundos) | Reducir complejidad (modelos m√°s simples, limitar profundidad)      |
| **Caracter√≠sticas**                      | A√±adir o transformar caracter√≠sticas relevantes                                     | Eliminar caracter√≠sticas irrelevantes o ruidosas                    |
| **Entrenamiento**                        | Aumentar n√∫mero de iteraciones/√©pocas                                               | Early stopping (detener cuando se sobreajusta al set de validaci√≥n) |
| **Regularizaci√≥n**                       | Reducir regularizaci√≥n (bajar Œª)                                                    | A√±adir o aumentar regularizaci√≥n L1 / L2                            |
| **Datos**                                | No suele ser la primera opci√≥n, pero ayuda                                          | A√±adir m√°s datos de entrenamiento                                   |
| **Dimensionalidad**                      | No aplica directamente                                                              | Reducci√≥n de dimensionalidad (PCA, selecci√≥n de caracter√≠sticas)    |
| **T√©cnicas avanzadas**                   | ‚Äî                                                                                   | T√©cnicas de ensamblaje (Bagging, Boosting)                          |
| **Aumento de Datos (Data Augmentation)** | ‚Äî                                                                                   | √ötil para generalizar mejor (im√°genes, texto, audio)                |

---

## Regularizaci√≥n: Previniendo el Sobreajuste sin Perder Capacidad de Aprendizaje

### Formula

$$\text{T√©rmino de Regularizaci√≥n L2} = \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2$$

### ¬øQu√© es la Regularizaci√≥n?

La **regularizaci√≥n** es una t√©cnica fundamental que modifica la funci√≥n de coste de un modelo para reducir su complejidad y evitar el sobreajuste (*overfitting*). Lo hace penalizando los **par√°metros grandes** del modelo (Œ∏), lo que tiende a producir modelos m√°s simples y generalizables.

---

### ¬øPor qu√© se necesita?

* Queremos minimizar el **error de predicci√≥n** en los datos de entrenamiento.
* Pero si el modelo es demasiado complejo (por ejemplo, tiene par√°metros Œ∏ muy grandes), puede **memorizar los datos** en lugar de aprender patrones generales.
* Esto causa **sobreajuste**, es decir, bajo error en entrenamiento pero alto error en datos nuevos.

La regularizaci√≥n combate esto a√±adiendo una **penalizaci√≥n por complejidad** directamente en la funci√≥n de coste.

---

### ¬øC√≥mo se modifica la funci√≥n de coste?

Tomemos como ejemplo la **Regresi√≥n Lineal Regularizada con L2** (tambi√©n conocida como *Ridge Regression*).

La nueva funci√≥n de coste se define como:

$$J(Œ∏) = (1 / 2m) ‚àë*{i=1}^{m} (h\_Œ∏(x^{(i)}) - y^{(i)})¬≤ + (Œª / 2m) ‚àë*{j=1}^{n} Œ∏\_j¬≤$$

Donde:

* m es el n√∫mero de ejemplos.
* h\_Œ∏(x) es la predicci√≥n del modelo.
* y^{(i)} es el valor real para el ejemplo i.
* Œª ‚â• 0 es el **coeficiente de regularizaci√≥n**.
* La suma en el segundo t√©rmino excluye generalmente Œ∏‚ÇÄ (el sesgo/intercepto), ya que no suele penalizarse.

> Esto no es mas que la suma de nuestra funcion de coste mas la funcion de regularizacion
---

### ¬øC√≥mo afecta al Descenso de Gradiente?

La actualizaci√≥n de los par√°metros tambi√©n se modifica para incluir la penalizaci√≥n. Si antes actualiz√°bamos as√≠:


$$
\theta_j := \theta_j - \alpha \cdot \frac{\partial J}{\partial \theta_j}
$$

Con regularizaci√≥n L2, el gradiente se ajusta as√≠:


$$
\theta_j := \theta_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m} \theta_j \right]
$$


Esto significa que cada Œ∏\_j es "empujado" ligeramente hacia cero en cada paso, evitando que crezca demasiado.

> Nota: La penalizaci√≥n **no aplica a Œ∏‚ÇÄ**, as√≠ que su actualizaci√≥n se mantiene igual que antes.

---

### Beneficios de la Regularizaci√≥n

* **Reduce el riesgo de overfitting**, haciendo que el modelo generalice mejor.
* **Controla la complejidad** del modelo sin cambiar su arquitectura.
* **F√°cil de implementar**, ya que solo requiere ajustar la funci√≥n de coste y el gradiente.

### Que pasa si lmbda es 0?
Si `lmbda_reg = 0`, entonces el **t√©rmino de regularizaci√≥n** se anula completamente:

$$
\text{t√©rmino\_de\_regularizaci√≥n} = \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2 = 0
$$

Por tanto:

$$
\text{coste\_total} = \text{coste\_original} + 0 = \text{coste\_original}
$$

### ¬øQu√© significa esto?
Si el par√°metro de regularizaci√≥n Œª = 0, la funci√≥n de coste regularizada se convierte exactamente en la funci√≥n de coste original, sin regularizaci√≥n. Esto se debe a que el t√©rmino de penalizaci√≥n (por ejemplo, en regularizaci√≥n L2, la suma de los cuadrados de los par√°metros) se multiplica por Œª:

---

## Modificaci√≥n del modelo de Regresi√≥n Lineal con Regularizaci√≥n L2

**1. Paso 1: Modificar la funci√≥n de coste (MSE regularizado)**

Partimos de la funci√≥n de coste est√°ndar de la regresi√≥n lineal:

**Funci√≥n original (MSE):**

$$J(Œ∏) = (1/2m) ‚àë (hŒ∏(x‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ)¬≤$$

Con regularizaci√≥n L2, a√±adimos una penalizaci√≥n al tama√±o de los par√°metros (excepto el sesgo Œ∏‚ÇÄ):

**Funci√≥n de coste con regularizaci√≥n L2 (Ridge):**

$$J(Œ∏) = (1/2m) ‚àë (hŒ∏(x‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ)¬≤ + (Œª/2m) ‚àë\_{j=1}^n Œ∏‚±º¬≤$$

> Nota: La suma regularizada comienza desde j = 1 para **excluir el sesgo Œ∏‚ÇÄ**, ya que no queremos penalizarlo.


---

## **Paso 2: Modificar la Funci√≥n de Descenso de Gradiente (con Regularizaci√≥n L2)**

### **Paso A: Preparar el vector para la penalizaci√≥n**

La regularizaci√≥n L2 a√±ade una **penalizaci√≥n a los valores grandes de los par√°metros** para evitar que el modelo sobreajuste.
Sin embargo, **no debemos penalizar el par√°metro Œ∏‚ÇÄ** (el t√©rmino independiente o sesgo), ya que no est√° asociado a ninguna caracter√≠stica y su penalizaci√≥n podr√≠a afectar negativamente el entrenamiento.

Por eso, vamos a crear una **copia del vector `Œ∏` (theta)**, pero con el primer valor igual a cero.

En c√≥digo, esto ser√≠a:

```python
theta_para_penalizacion = theta.copy()
theta_para_penalizacion[0] = 0
```

Esto da como resultado un nuevo vector:

$$
\theta_{\text{penalizaci√≥n}} = 
\begin{bmatrix}
0 \\
\theta_1 \\
\theta_2 \\
\vdots \\
\theta_n
\end{bmatrix}
$$

### **Paso B: Calcular la penalizaci√≥n para el gradiente**

Ahora, vamos a calcular el **vector de penalizaci√≥n** que sumaremos al gradiente original.
Este vector se obtiene multiplicando cada elemento de `Œ∏_para_penalizacion` por un escalar que incluye el **par√°metro de regularizaci√≥n** Œª y el n√∫mero de ejemplos m:

$$
\text{penalizaci√≥n\_gradiente} = \frac{\lambda}{m} \cdot \theta_{\text{penalizaci√≥n}}
$$

Este vector tiene el mismo tama√±o que `Œ∏` y **solo penaliza los par√°metros distintos de Œ∏‚ÇÄ**.

---

## üêõ Proceso de Depuraci√≥n Completo: Problemas al Ejecutar el Modelo con Distintos Lambdas

### Contexto

Est√°bamos probando nuestro modelo de regresi√≥n lineal regularizada con distintos valores de `lambda`, con el siguiente bloque:

```python
for lmbda_reg in lmbda_valores:
    theta_calculado, historial_coste = descenso_gradiente(
        X_bias_scaled, y, theta_inicial.copy(), alpha, num_iteraciones, lmbda_reg
    )
```

El objetivo era observar c√≥mo cambiaban los par√°metros `theta` y la funci√≥n de coste con distintos grados de regularizaci√≥n. Pero **empezaron a ocurrir problemas graves**:

* La m√°quina se volv√≠a extremadamente lenta.
* El script nunca terminaba de ejecutarse.
* No hab√≠a errores expl√≠citos visibles al principio.

---

### üß© Etapa 1: TypeError en `num_iteraciones`

Error observado:

```
TypeError: 'float' object cannot be interpreted as an integer
```

Esto ocurri√≥ en la funci√≥n `descenso_gradiente`, en esta l√≠nea:

```python
for i in range(num_iteraciones):
```

**Hip√≥tesis inicial**: `num_iteraciones` no estaba llegando como entero, sino como `float`.

---

#### Paso 1.1: Verificar `num_iteraciones` ANTES del bucle

Agregamos este print en el script principal:

```python
print(f"DEBUG SCRIPT: Antes del bucle de lambdas, num_iteraciones = {num_iteraciones}, tipo = {type(num_iteraciones)}")
```

**Salida esperada** (si estuviera bien):

```
DEBUG SCRIPT: Antes del bucle de lambdas, num_iteraciones = 200, tipo = <class 'int'>
```

Pero eso estaba bien, as√≠ que fuimos m√°s adentro.

---

#### Paso 1.2: Verificar `num_iteraciones` DENTRO de la funci√≥n

Agregamos este print al inicio de `descenso_gradiente`:

```python
print(f"DEBUG GD: Al inicio de la funci√≥n, num_iteraciones = {num_iteraciones}, tipo = {type(num_iteraciones)}")
```

**Salida obtenida**:

```
DEBUG GD: Al inicio de la funci√≥n, num_iteraciones = 0.001, tipo = <class 'float'>
```

**Descubrimiento**: Est√°bamos pasando mal los argumentos en la llamada. Lo que estaba llegando como `num_iteraciones` en realidad era `alpha`.

---

#### ‚úîÔ∏è Soluci√≥n 1: Corregir el orden de los argumentos

La llamada correcta deb√≠a ser:

```python
theta_calculado, historial_coste = descenso_gradiente(
    X_bias_scaled, y, theta_inicial.copy(), alpha_real, num_iteraciones_real, lmbda_reg
)
```

Con esto, el error de tipo desapareci√≥.

---

### üß© Etapa 2: Explosi√≥n de Formas y Lentitud

Aunque el script ya no tiraba error, ahora ten√≠a s√≠ntomas distintos:

* El modelo se volv√≠a extremadamente lento.
* `theta` ten√≠a formas gigantes.
* `errores` explotaba en tama√±o.

---

#### Paso 2.1: Verificar formas en cada iteraci√≥n

Agregamos estos prints dentro de `descenso_gradiente`, despu√©s de calcular `predicciones` y `errores`:

```python
print(f"Iter {i+1} DEBUG GD: theta.shape = {theta.shape}")
print(f"Iter {i+1} DEBUG GD: predicciones.shape = {predicciones.shape}")
print(f"Iter {i+1} DEBUG GD: errores.shape = {errores.shape}")
```

**Salida obtenida (ejemplo con lambda=0):**

```
Iter 1 DEBUG GD: theta.shape = (9, 20640)
Iter 1 DEBUG GD: predicciones.shape = (20640, 1)
Iter 1 DEBUG GD: errores.shape = (20640, 20640)
```

**Algo estaba muy mal.**

---

#### Paso 2.2: Diagn√≥stico m√°s fino del error de dimensiones

Agregamos m√°s prints para analizar justo antes y despu√©s de calcular `errores`:

```python
predicciones = calcular_hipotesis(X_bias, theta)
print(f"Iter {i+1} DEBUG GD ANTES DE ERRORES: theta.shape={theta.shape}, predicciones.shape={predicciones.shape}, y.shape={y.shape}")
errores = predicciones - y
print(f"Iter {i+1} DEBUG GD DESPU√âS DE ERRORES: errores.shape={errores.shape}")
```

**Salida clave**:

```
Iter 1 DEBUG GD ANTES DE ERRORES: theta.shape=(9,1), predicciones.shape=(20640,1), y.shape=(20640,)
Iter 1 DEBUG GD DESPU√âS DE ERRORES: errores.shape=(20640,20640)
```

**Descubrimiento**: `y` ten√≠a forma `(20640,)` (vector de 1 dimensi√≥n), mientras que `predicciones` era `(20640,1)`. Python hizo *broadcasting* para hacer compatible la resta, creando una matriz de tama√±o `(20640,20640)`.

---

#### ‚úîÔ∏è Soluci√≥n 2: Forzar forma correcta de `y`

Al inicio de `descenso_gradiente`, agregamos:

```python
y = y.reshape(-1, 1)
```

**Resultado** tras el fix:

```
Iter 1 DEBUG GD ANTES DE ERRORES: theta.shape=(9,1), predicciones.shape=(20640,1), y.shape=(20640,1)
Iter 1 DEBUG GD DESPU√âS DE ERRORES: errores.shape=(20640,1)
```

‚úÖ Ahora todas las formas se manten√≠an correctas. La lentitud extrema desapareci√≥ y el entrenamiento se comport√≥ como se esperaba.

---

#### Paso 2.3 (Opcional): Verificar explosiones num√©ricas

A√∫n con formas correctas, se puede tener lentitud por problemas num√©ricos. Verificamos esto dentro de `calcular_coste`:

```python
print(f"CALC_COSTE: errores_cuadraticos.shape = {errores_cuadraticos.shape}")
print(f"CALC_COSTE: Primeros 3 errores_cuadraticos: {errores_cuadraticos[:3].T}")
print(f"CALC_COSTE: Hay NaNs? {np.isnan(errores_cuadraticos).any()}, Hay Infs? {np.isinf(errores_cuadraticos).any()}")
```

**Posible salida si hubiera overflows**:

```
CALC_COSTE: errores_cuadraticos.shape = (20640, 1)
CALC_COSTE: Primeros 3 errores_cuadraticos: [[inf inf inf]]
CALC_COSTE: Hay NaNs? False, Hay Infs? True
```

Pero en nuestro caso, tras corregir la forma de `y`, **no hubo problemas num√©ricos**.

---

### ‚úÖ Conclusi√≥n

Gracias a un proceso met√≥dico de **debugging con prints**, descubrimos dos errores graves:

1. Par√°metros mal pasados (`alpha` y `num_iteraciones` estaban invertidos).
2. Forma de `y` incorrecta, causando explosiones de matrices y lentitud.

Estas correcciones fueron **cr√≠ticas para el funcionamiento correcto del modelo**, y para que pudi√©ramos hacer las pruebas con m√∫ltiples valores de `lambda`.


## ‚úÖ CheckList de Buenas Pr√°cticas para Debugging y Modelado

### üìå Variables y Par√°metros

* [ ] Confirmar que `alpha`, `num_iteraciones`, `lambda` est√°n en el **orden correcto** al llamar funciones.
* [ ] Asegurarse de que `num_iteraciones` sea un `int`, no un `float`.

### üìè Formas de las Matrices

* [ ] Convertir `y` a forma `(m, 1)` antes de operaciones vectorizadas:

  ```python
  y = y.reshape(-1, 1)
  ```
* [ ] Verificar que `theta` tenga forma `(n, 1)` si `X` es `(m, n)`.

### üîç Verificaciones Intermedias (Debugging)

* [ ] Agregar prints de forma en puntos clave:

  ```python
  print(f"theta.shape = {theta.shape}")
  print(f"predicciones.shape = {predicciones.shape}")
  print(f"errores.shape = {errores.shape}")
  ```
* [ ] Usar prints con `.any()` para detectar `NaN` o `Inf`:

  ```python
  print(np.isnan(matriz).any(), np.isinf(matriz).any())
  ```

### üß™ Testeo Controlado

* [ ] Probar primero con un n√∫mero peque√±o de iteraciones (`num_iteraciones = 5 o 10`) y `lambda = 0` para validar la l√≥gica antes de entrenar completamente.

### ‚ö†Ô∏è Se√±ales de Error Com√∫n

| S√≠ntoma                         | Posible Causa                             |
| ------------------------------- | ----------------------------------------- |
| TypeError con `range()`         | `num_iteraciones` es `float`              |
| Errores gigantes `(m, m)`       | `y` tiene forma `(m,)` ‚Üí usar `.reshape`  |
| `theta` con forma rara `(n, m)` | Broadcasting incorrecto o errores previos |
| Script extremadamente lento     | Matrices gigantes por formas incorrectas  |
| `coste` devuelve `inf` o `nan`  | Overflow ‚Üí revisar `alpha` o escalado     |




## üìä **Resultados de la Experimentaci√≥n con Regularizaci√≥n L2 (Œª)**

### ¬øC√≥mo se comport√≥ Œ∏ 0(el intercepto) a medida que Œª cambiaba?

Realizamos experimentos utilizando un conjunto de datos de precios de casas en California para analizar c√≥mo afecta la regularizaci√≥n L2 en un modelo de regresi√≥n lineal.

### Regresion Lineal:
![alt text](<Regresion_Lineal.py/Coeficientes theta.png>)
### Regresion Logistica:
![alt text](<Regresion_Logisitica/Coeficientes ttheta en funcion del lmbda.png>) 




### üß™ **¬øQu√© me dice esto?**

En este an√°lisis, **no aplicamos penalizaci√≥n a Œ∏‚ÇÄ** (el t√©rmino que corresponde al valor inicial). Esto se debe a que cuando **Œª = 0**, no se agrega ninguna restricci√≥n, lo que significa que no se penaliza este t√©rmino.

Observamos en el gr√°fico c√≥mo los coeficientes (los valores multiplicados por las variables) cambian cuando **Œª** var√≠a. A medida que **Œª** aumenta, **los coeficientes tienden a hacerse m√°s peque√±os**, lo que significa que estamos "empujando" los coeficientes hacia **0**.

### üîé **¬øPor qu√© sucede esto?**

Esto ocurre por la **regularizaci√≥n L2**. Cuando aumentamos **Œª**:

* El modelo se hace m√°s simple, ya que reduce los valores de los coeficientes.
* Si **Œª** es grande, el modelo no confiar√° tanto en cada variable, evitando que alguna variable sea demasiado importante. Esto ayuda a **evitar el sobreajuste** (cuando el modelo "se ajusta demasiado" a los datos de entrenamiento).

### üìè **Funci√≥n de Coste con Regularizaci√≥n L2**

La funci√≥n de coste de la regresi√≥n lineal con regularizaci√≥n L2, tambi√©n conocida como **Ridge Regression**, es la siguiente:

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
$$

Donde:

* **J(Œ∏)** es la funci√≥n de coste, que mide qu√© tan bien se ajusta el modelo.
* **h‚Çú‚Çï‚Çê(x·µ¢)** es la predicci√≥n del modelo para los datos de entrada.
* **y·µ¢** es el valor real que queremos predecir.
* **Œª** es el par√°metro de regularizaci√≥n, que controla cu√°nto penalizamos a los coeficientes.
* **Œ∏‚±º** son los coeficientes de las caracter√≠sticas.
* **m** es el n√∫mero de ejemplos de entrenamiento.

### ‚öñÔ∏è **Resumen:**

* **Cuando Œª = 0:** No hay penalizaci√≥n, lo que puede hacer que el modelo se ajuste demasiado a los datos, llevando a un **sobreajuste**.

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2
$$

* **Cuando Œª es alto:** Los coeficientes se hacen m√°s peque√±os, favoreciendo un modelo m√°s simple, lo que ayuda a evitar el **sobreajuste**(overfitting) y mejora la generalizaci√≥n del modelo.

---


### Regresion Lineal:
![alt text](<Regresion_Lineal.py/Curvas de coste lmbda.png>)
### Regresion Logistica:
![alt text](<Regresion_Logisitica/Curvas coste de lmbda.png>)

---

## üìâ An√°lisis del Gr√°fico: *Curvas de Coste por Lambda*

Este gr√°fico nos muestra c√≥mo el par√°metro \$\lambda\$ (lambda, de regularizaci√≥n) afecta el **aprendizaje** de nuestro modelo de regresi√≥n lineal.

### 1. **Verificaci√≥n de la Convergencia**

Cada l√≠nea en el gr√°fico representa c√≥mo cambia el coste durante el entrenamiento, para un valor distinto de \$\lambda\$. Lo que buscamos es que el coste:

* Disminuya progresivamente.
* Se estabilice (indica que el modelo ha "convergido").

> ‚ö†Ô∏è **Nota**: Si el coste *no* baja o se comporta de forma rara para cierto \$\lambda\$, podr√≠a indicar que esa configuraci√≥n no est√° funcionando bien. Tal vez \$\lambda\$ es demasiado alto o interact√∫a mal con el valor de `alpha`.

---

### 2. **Comparaci√≥n del Error Final con Diferentes Valores de \$\lambda\$**

* **Cuando \$\lambda = 0\$ (sin regularizaci√≥n):**
  El modelo tiene total libertad para ajustarse a los datos de entrenamiento. Por eso, el coste final es usualmente **m√°s bajo**: el modelo "memoriza" los datos.

* **Cuando \$\lambda > 0\$ (con regularizaci√≥n):**
  A medida que aumentamos \$\lambda\$, el modelo debe **equilibrar dos objetivos**:

  1. Minimizar el error de predicci√≥n.
  2. Mantener los valores de los par√°metros \$\theta\_j\$ **peque√±os** (evitar que crezcan mucho).

  Esto suele llevar a un **coste de entrenamiento m√°s alto**, pero tambi√©n reduce el riesgo de *overfitting* (sobreajuste).

---

### ‚úÖ **Resumen del gr√°fico:**

* Verifica si el modelo est√° entrenando correctamente (convergencia).
* Muestra c√≥mo el modelo reacciona ante distintos niveles de regularizaci√≥n.
* Ayuda a detectar si un \$\lambda\$ **demasiado grande** est√° haciendo que el modelo sea **demasiado simple** (lo que llamamos *underfitting*).

---

## üéØ ¬øQu√© pasa con los coeficientes \$\theta\$ cuando usamos regularizaci√≥n?

### ¬øAlgunos coeficientes se reducen a cero o cerca de cero m√°s r√°pido que otros?

‚úÖ **S√≠**, cuando aumentamos \$\lambda\$, algunos coeficientes \$\theta\_j\$ se acercan a cero m√°s r√°pido que otros.

Esto pasa porque con **regularizaci√≥n fuerte**, el modelo trata de **penalizar m√°s** a ciertos coeficientes. Si ve que una variable no est√° aportando mucho, la "castiga" y empuja su \$\theta\$ hacia cero.

```
‚ùó Si un coeficiente se hace peque√±o o casi cero con una lambda alta, el modelo cree que esa variable no es tan importante para hacer predicciones.
```

---

### üîç ¬øQu√© implica esto?

* El modelo est√° buscando **simplicidad**: usar solo las variables que realmente ayudan.
* Si un \$\theta\$ baja r√°pido, es porque el modelo **conf√≠a menos** en esa variable.
* Es como una **selecci√≥n autom√°tica de caracter√≠sticas**: las menos √∫tiles se "apagan" solas.

---

### üî¨ Conexi√≥n con la teor√≠a

* En la regularizaci√≥n L2 (*Ridge*), el coste total incluye un t√©rmino adicional que penaliza los \$\theta\_j\$ grandes:

  $$
  J(\theta) = \text{Error cuadr√°tico} + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
  $$

* Como ves, se penaliza tener coeficientes grandes. Por eso, el modelo prefiere hacerlos peque√±os, **a menos que realmente sean necesarios para predecir bien**.

## üìä Resultados de la Experimentaci√≥n con Regularizaci√≥n L2 (Œª)
A continuaci√≥n se presentan los resultados obtenidos al aplicar regularizaci√≥n L2 a dos modelos: Regresi√≥n Lineal y Regresi√≥n Log√≠stica. Se analizaron los cambios en los coeficientes $\theta_j$ al variar el par√°metro de regularizaci√≥n $\lambda$, observando c√≥mo esto afecta tanto al entrenamiento como a la simplicidad del modelo.

üìå A. Modelo de Regresi√≥n Lineal
1. Estabilidad del Intercepto Œ∏‚ÇÄ
En este experimento, aunque el gr√°fico "Coeficientes theta.png" muestra √∫nicamente los coeficientes $\theta_1$ a $\theta_8$, al inspeccionar los arrays completos de theta_calculado, se observ√≥ que el valor de $\theta_0$ (el intercepto) se mantuvo relativamente estable.



Esto es coherente con la teor√≠a de la regularizaci√≥n L2, ya que $\theta_0$ no es penalizado en la funci√≥n de coste. Por lo tanto, su valor no se ve afectado significativamente por el aumento de $\lambda$.

1. Encogimiento de los Coeficientes $\theta_1$ a $\theta_8$
Del gr√°fico "Coeficientes theta.png", se observa que todos los coeficientes disminuyen en magnitud a medida que aumenta $\lambda$:

Por ejemplo, $\theta_1$ (l√≠nea azul) comienza en aproximadamente 0.81 cuando $\lambda$ es bajo, y se reduce hasta casi 0.05 cuando $\lambda = 1000$.

Otros coeficientes, como $\theta_5$ o $\theta_7$, bajan incluso m√°s r√°pido y tienden m√°s r√°pidamente a cero.

Este comportamiento refleja el efecto cl√°sico del "shrinkage" (encogimiento): la regularizaci√≥n L2 penaliza los coeficientes grandes, empuj√°ndolos hacia cero.
Esto sugiere que el modelo considera que algunas variables son menos importantes para la predicci√≥n, y por tanto sus $\theta_j$ son reducidos con m√°s fuerza. En otras palabras, el modelo autom√°ticamente "selecciona" cu√°les caracter√≠sticas conservar y cu√°les descartar, aunque en Ridge nunca llegan exactamente a cero.

üìå B. Modelo de Regresi√≥n Log√≠stica
1. Estabilidad del Intercepto Œ∏‚ÇÄ
De forma similar al modelo lineal, se observ√≥ que el valor de $\theta_0$ en la regresi√≥n log√≠stica se mantiene estable a pesar del aumento en $\lambda$:

Para $\lambda = 0$, $\theta_0 \approx 1.35$

Para $\lambda = 1000$, $\theta_0 \approx 1.31$

Esto nuevamente es esperado, ya que $\theta_0$ no es penalizado en la regularizaci√≥n L2.

2. Encogimiento de los Coeficientes $\theta_1$ a $\theta_8$
El gr√°fico "Coeficientes theta en funci√≥n del lambda.png" muestra c√≥mo los primeros 8 coeficientes disminuyen al aumentar $\lambda$.
Se decidi√≥ graficar solo estos primeros 8 de los 30 coeficientes disponibles en el dataset del c√°ncer de mama para facilitar la visualizaci√≥n.

Tendencia observada:

Cuando $\lambda$ es bajo, algunos $\theta_j$ comienzan con valores relativamente altos (entre 0.3 y 0.7).

A medida que $\lambda$ aumenta, todos estos coeficientes tienden hacia cero, aunque no todos con la misma rapidez.

A diferencia del modelo lineal, en este gr√°fico el encogimiento parece m√°s abrupto para algunos coeficientes espec√≠ficos, lo que puede deberse a que el modelo log√≠stico es m√°s sensible a la regularizaci√≥n por la naturaleza de su funci√≥n de coste (log loss).

### üîÑ Comparaci√≥n entre los dos modelos
Aunque ambos modelos usan el mismo principio de regularizaci√≥n L2, sus gr√°ficos de coeficientes lucen diferentes debido a varios factores:

Tipo de modelo: Lineal vs Log√≠stico.

Cantidad y tipo de variables: El modelo lineal us√≥ un dataset m√°s peque√±o con 8 caracter√≠sticas, mientras que el log√≠stico trabaj√≥ con 30.

Magnitud de los coeficientes: En la regresi√≥n log√≠stica, los coeficientes son m√°s peque√±os desde el inicio, lo que hace que el efecto visual del encogimiento sea m√°s notorio o abrupto.

Ambos modelos muestran el mismo comportamiento esencial:

A mayor $\lambda$, mayor penalizaci√≥n, lo que lleva a coeficientes m√°s peque√±os y, por ende, a modelos m√°s simples.

 ### A. Modelo de Regresi√≥n Lineal ‚Äî "Coeficientes theta.png"
üîπ 1. Estabilidad del Intercepto Œ∏‚ÇÄ
Aunque el gr√°fico "Coeficientes theta.png" muestra √∫nicamente los coeficientes Œ∏‚ÇÅ a Œ∏‚Çà, al observar los arrays de theta_calculado, se vio que el valor del intercepto Œ∏‚ÇÄ se mantuvo estable en todos los valores de Œª.
Por ejemplo, para Œª = 0, Œ∏‚ÇÄ ‚âà 2.07, y para Œª = 1000, Œ∏‚ÇÄ ‚âà 2.02.
Esto es coherente con lo esperado, ya que el intercepto no es penalizado por la regularizaci√≥n L2, por lo tanto no se ve afectado por el aumento de Œª.
üîπ 2. Encogimiento de los coeficientes Œ∏‚ÇÅ a Œ∏‚Çà
En el gr√°fico se observa c√≥mo los coeficientes disminuyen (efecto "shrinkage") al aumentar Œª.
Por ejemplo, el coeficiente Œ∏‚ÇÅ (l√≠nea azul) comienza en aproximadamente 0.81 cuando Œª es peque√±o, y disminuye hasta alrededor de 0.25 cuando Œª = 1000.

En contraste, coeficientes como Œ∏‚ÇÖ (l√≠nea morada) comienzan ya cerca de 0.05 y se mantienen pr√°cticamente planos, lo que indica que esa caracter√≠stica tiene una importancia muy baja y el modelo tiende a descartarla r√°pidamente.

Œ∏‚Çá (l√≠nea rosa) presenta una reducci√≥n m√°s pronunciada, bajando de aproximadamente 0.5 a casi 0.1, lo cual muestra que es una caracter√≠stica moderadamente importante, pero que pierde peso a medida que el modelo se simplifica.

En general, los coeficientes m√°s relevantes resisten m√°s el encogimiento, mientras que los menos informativos tienden r√°pidamente hacia cero. Esto ilustra c√≥mo la regularizaci√≥n act√∫a como un filtro autom√°tico de caracter√≠sticas.

### üìå B. Modelo de Regresi√≥n Log√≠stica ‚Äî "Coeficientes ttheta en funcion del lmbda.png"
üîπ 1. Estabilidad del Intercepto Œ∏‚ÇÄ
Para el modelo de regresi√≥n log√≠stica, tambi√©n se observ√≥ que el intercepto Œ∏‚ÇÄ se mantuvo estable.
Por ejemplo, para Œª = 0, Œ∏‚ÇÄ ‚âà 1.35, y para Œª = 1000, Œ∏‚ÇÄ ‚âà 1.31.
Al igual que en la regresi√≥n lineal, esto es esperable porque el intercepto no es penalizado por la regularizaci√≥n L2.

üîπ 2. Encogimiento de los coeficientes Œ∏‚ÇÅ a Œ∏‚Çà
En este gr√°fico se muestran solo los primeros 8 coeficientes (de un total de 30 del dataset de c√°ncer de mama), por claridad visual.

Cuando Œª es peque√±o (a la izquierda del gr√°fico, valores de log(Œª) cercanos a -3), los coeficientes Œ∏‚ÇÅ a Œ∏‚Çà toman valores entre aproximadamente -0.4 y -0.1, es decir, la mayor√≠a empiezan en valores negativos moderados.

A medida que Œª aumenta, todos los coeficientes disminuyen su magnitud y tienden hacia cero, mostrando el cl√°sico efecto de "shrinkage".
Algunos, como **Œ∏‚ÇÉ o Œ∏






