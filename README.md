# My-AI-Mission

# Misi√≥n 1: √Ålgebra Lineal con Python Puro

Breve descripci√≥n del proyecto: Implementaci√≥n de operaciones b√°sicas de matrices (suma, multiplicaci√≥n por escalar, multiplicaci√≥n de matrices, traspuesta) usando √∫nicamente Python puro, junto con explicaciones de los conceptos fundamentales involucrados.

## üìÇ C√≥digo Python (`Algebra_lineal.py`)

Este archivo contiene las funciones desarrolladas para realizar las operaciones matriciales solicitadas: suma de matrices, multiplicaci√≥n por escalar, multiplicaci√≥n de matrices y trasposici√≥n.

## üìò Explicaciones Conceptuales

### 1. Vectores y Combinaciones Lineales

- **Vector:** Una lista ordenada de n√∫meros (componentes) que representa una magnitud con direcci√≥n en un espacio.
- **Combinaci√≥n Lineal:** Es una operaci√≥n en la que cada vector se multiplica por un escalar y luego se suman los vectores resultantes.

### 2. Multiplicaci√≥n de Matrices

* **¬øC√≥mo funciona?:** Cada elemento de la matriz resultante se calcula mediante el producto punto de una fila de la primera matriz y una columna de la segunda matriz (se multiplican los elementos correspondientes y se suman los resultados).
* **¬øPor qu√© importan las dimensiones?:** Son cruciales para determinar si la multiplicaci√≥n es posible y cu√°l ser√° el tama√±o del resultado.
    * **Condici√≥n:** Para multiplicar A (m x n) por B (p x q), es necesario que `n = p` (el n√∫mero de columnas de A debe ser igual al n√∫mero de filas de B).
    * **Tama√±o del Resultado:** Si la condici√≥n se cumple, la matriz resultado ser√° de tama√±o m x q (filas de A x columnas de B).
    * **Raz√≥n:** La condici√≥n `n = p` es necesaria para poder realizar la operaci√≥n fila-por-columna (producto punto).
      
### 3. Traspuesta de una Matriz

- **¬øC√≥mo se obtiene?:** Se intercambian filas por columnas. Es decir, la fila \( i \) de la matriz original se convierte en la columna \( i \) de la traspuesta.
  
- **¬øPara qu√© sirve?:** Facilita reorganizar datos, simplificar f√≥rmulas matem√°ticas y es muy usada en √°reas como Machine Learning para manipular vectores, pesos y operaciones matriciales..

### ‚ùì ¬øPor qu√© la multiplicaci√≥n de matrices *no* es conmutativa en general (\( AB \neq BA \))?

Las matrices representan transformaciones (como rotaciones o escalados), y aplicar una transformaci√≥n seguida de otra no necesariamente da el mismo resultado si se invierte el orden. Adem√°s:

- Puede que \( A \times B \) sea posible pero \( B \times A \) no, debido a la incompatibilidad de dimensiones.
- Incluso si ambas multiplicaciones son posibles, el tama√±o del resultado puede ser diferente.
- Y aun si el tama√±o coincide, el contenido generalmente **no ser√° el mismo**.

### ‚ùì ¬øCu√°l es la intuici√≥n geom√©trica detr√°s de la traspuesta?

- **Perspectiva:** Se puede ver como un cambio de enfoque: si las filas representan personas y las columnas caracter√≠sticas, al trasponer la matriz, ahora las filas representan caracter√≠sticas y las columnas personas que comparten esas caracter√≠sticas.
  
- **Reflejo:** Visualmente, es como reflejar la matriz sobre su **diagonal principal**, intercambiando filas por columnas.


# Preguntas de la Misi√≥n 1:

## ¬øPor qu√© la multiplicaci√≥n de matrices NO es conmutativa en general (AB != BA)?

**Porque** las matrices representan transformaciones, representan rotaciones, etc. **b√°sicamente** cuando multiplicas estas aplicando una **transformaci√≥n**, por lo tanto no puede ser conmutativa. **Tambi√©n** podemos pensar A\*B sea posible pero tal vez B\*A no, **porque** tal vez no sean compatibles (columna de la primera con filas de la segunda). **Aun as√≠**, en el caso de que sea posible A\*B y B\*A, puede que el tama√±o resultante no sea el mismo.

### ¬øCu√°l es la intuici√≥n geom√©trica (si la hay) detr√°s de la traspuesta?

* **Perspectiva:** Podemos pensarlo como un cambio de enfoque. Por ejemplo, si las filas representan personas y las columnas caracter√≠sticas, al trasponer la matriz, las filas pasar√≠an a representar las caracter√≠sticas, y las columnas a las personas que las poseen.
* **Reflejo:** Tambi√©n puede visualizarse como reflejar la matriz en su diagonal principal, intercambiando filas por columnas.


# Misi√≥n 2

## ¬øQu√© es PCA?

PCA, por sus siglas **(An√°lisis de Componentes Principales)**, es una t√©cnica que se usa para reducir la cantidad de variables en un conjunto de datos, sin perder demasiada informaci√≥n.

## ¬øPara qu√© sirve?

* Nos permite **visualizar datos complejos.** Por ejemplo, podemos pasar de 4 dimensiones a 2 y graficarlos.
* **Tambi√©n** podemos eliminar variables que no aportan mucho o que no son tan importantes.

## ¬øC√≥mo se elige cu√°ntos componentes usar (el valor de $k$)?

Al hacer PCA para reducir dimensiones (por ejemplo, de 4 a $k$), la gran pregunta es: ¬øcu√°ntos componentes ($k$) debemos conservar para quedarnos con la informaci√≥n m√°s importante sin perder demasiado? Aqu√≠ entran los conceptos de **varianza explicada** y **varianza acumulada**.

* **Varianza Explicada:** Cada componente principal (CP) "explica" un cierto porcentaje de la variaci√≥n total de los datos. El primer CP explica la mayor parte, el segundo un poco menos, y as√≠ sucesivamente. Este porcentaje est√° directamente relacionado con el tama√±o de su valor propio (eigenvalue) o su valor singular al cuadrado ($s^2$) comparado con la suma total de todos ellos. Es como preguntarse: "¬øCu√°nto de la 'historia completa' de los datos me cuenta esta direcci√≥n principal?"

* **Varianza Acumulada:** Es simplemente ir sumando los porcentajes de varianza explicada de los primeros componentes. Por ejemplo, la varianza acumulada por los 2 primeros CP es (Varianza del CP1) + (Varianza del CP2).

**M√©todos para elegir $k$:**

1.  **Umbral de Varianza Acumulada (El m√°s com√∫n):**
    * Decidimos qu√© porcentaje de la varianza total original queremos conservar (un valor t√≠pico es entre 90% y 99%, por ejemplo, 95%).
    * Calculamos la varianza explicada acumulada al usar 1 componente, luego 2, luego 3...
    * Elegimos el **menor n√∫mero $k$** de componentes cuya varianza acumulada **alcance o supere** nuestro umbral (ej: el primer $k$ que explique al menos el 95% de la varianza).

2.  **M√©todo del "Codo" (Visual):**
    * Se grafica la varianza explicada por cada componente (ordenados de mayor a menor).
    * Se busca un punto en el gr√°fico donde la curva "se dobla" como un codo y empieza a aplanarse. El "codo" sugiere el punto donde a√±adir m√°s componentes ya no aporta una cantidad significativa de informaci√≥n nueva. El valor de $k$ se elige en ese codo.

## Pasos clave de mi implementaci√≥n

1.  **Cargamos** el dataset de Iris, que tiene 4 variables por flor.
2.  **Centramos** los datos, restando la media de cada caracter√≠stica.
3.  **Calculamos** la matriz de covarianza:
    * Aqu√≠ calculamos la matriz de covarianza, que **b√°sicamente** nos dice c√≥mo se mueven las variables entre s√≠. Es como una tabla que nos muestra si dos variables tienden a aumentar o disminuir juntas. Es clave para ver las relaciones entre las caracter√≠sticas.
4.  **Calculamos** los valores propios y vectores propios:
    * Los valores propios nos dicen **cu√°nta** "importancia" tiene cada direcci√≥n de variaci√≥n.
    * Los vectores propios nos indican en qu√© direcci√≥n ocurren esas variaciones.
5.  **Ordenamos** los vectores propios de mayor a menor seg√∫n la varianza que explican. As√≠ sabemos qu√© direcciones son las m√°s importantes y capturan la mayor parte de la informaci√≥n.
6.  **Seleccionamos** los dos primeros componentes principales para proyectar los datos, y as√≠ poder visualizarlos.
7.  **Aqu√≠ utilizamos** una alternativa usando SVD (Descomposici√≥n en Valores Singulares):
    * Como alternativa, usamos SVD, que es otra forma de descomponer los datos. En lugar de calcular la matriz de covarianza, directamente obtenemos las direcciones principales usando SVD. Los vectores fila de `Vh` nos dan las direcciones principales.
8.  **Proyecci√≥n final alternativa usando SVD**:
    * Proyectamos los datos usando los primeros dos componentes principales que conseguimos a trav√©s de SVD. Esto tambi√©n nos reduce la dimensionalidad y nos permite ver los datos en 2D.
9.  **Visualizaci√≥n**:
    * Finalmente, **graficamos** los datos proyectados en 2D, usando los dos componentes principales seleccionados. Esto nos da una visualizaci√≥n m√°s sencilla de los datos, para poder ver patrones o relaciones.

## Conexi√≥n: Matriz de Covarianza y Varianza

Pensemos en los **n√∫meros** de la diagonal principal (de arriba a la izquierda hacia abajo a la derecha), estos son la **varianza**, te dicen **cu√°nto var√≠a** cada variable por s√≠ sola; si es grande, cambia mucho entre los datos.
Los **n√∫meros** que quedan fuera de la diagonal son la **covarianza** entre pares de variables, estas nos dicen **c√≥mo var√≠an** juntas dos **caracter√≠sticas**.

* Si el n√∫mero es positivo y grande, cuando una sube, la otra tambi√©n tiende a subir.
* Si es negativo, cuando una sube, la otra baja.
* Si es cerca de cero, no hay mucha relaci√≥n.

### ¬øPor qu√© nos ayudan a entender la estructura de datos?

**Porque** nos dan pista de **c√≥mo est√°n** distribuidos los datos y sus relaciones.

* Si una varianza es muy grande, quiere decir que esa variable es muy dispersa, cambia mucho entre muestra y muestra. **Tal vez** sea importante.
* Si una covarianza fuera de la diagonal es grande y positiva, significa que esas dos variables **est√°n** relacionadas, se mueven ‚Äújuntas‚Äù. Eso es √∫til porque **podr√≠amos** reducir dimensiones, ya que **est√°n** diciendo cosas parecidas.

### B√ÅSICAMENTE

* Qu√© variables **cambian mucho** (varianzas).
* Qu√© variables **est√°n** conectadas entre s√≠ (covarianzas).

## Conexi√≥n: Eigenvectores/Eigenvalores y Varianza

Los **vectores propios** son como flechas que te dicen hacia d√≥nde se **extienden** los datos, o sea, por donde se dispersan. Los **valores propios** nos dicen **cu√°nta variaci√≥n** hay en la **direcci√≥n** de su **vector propio** correspondiente.

* Si el valor propio es grande, hay mucha varianza (los datos **est√°n** muy esparcidos en esa direcci√≥n).
* Si es peque√±o, hay poca varianza (los datos **est√°n** m√°s concentrados).

El **v√≠nculo** es: los vectores propios de la matriz de covarianza nos dan las **direcciones** donde la varianza es m√°xima, y los valores propios nos dicen **cu√°nta** varianza hay en cada una de esas direcciones.
En resumen: Los **vectores propios** te dicen por d√≥nde se **est√°n** moviendo m√°s los datos. Los **valores propios** te dicen **cu√°nto** se **est√°n** moviendo por esas direcciones.

## Conexi√≥n: SVD y Varianza

Cuando usamos `U, s, Vh = np.linalg.svd(X_centrado)` estamos haciendo algo muy parecido a lo que hicimos con la matriz de covarianza, pero m√°s directo y m√°s estable.
* La U nos dice **c√≥mo** se ven los datos originales sobre las nuevas direcciones (`Vh`).
* Las filas de `Vh` son las mismas direcciones principales que **hab√≠amos** encontrado con los vectores propios, o sea, por d√≥nde m√°s se esparcen los datos.
* Los valores de `s` (valores singulares) **est√°n** ligados a la varianza:
    * Si haces $s^2$ (s al cuadrado), eso te da una idea de **cu√°nta** varianza hay en cada direcci√≥n.

## En resumen (SVD):

* `Vh`: Hacia d√≥nde mirar (las direcciones principales).
* `s`: **Cu√°nta** importancia tiene cada direcci√≥n (relacionado con la varianza a trav√©s de $s^2$).
* `U`: **C√≥mo** cada punto del dataset se ve desde esas nuevas direcciones.

SVD te da otra forma (m√°s precisa y directa) de encontrar esas direcciones principales importantes (`Vh`) y **cu√°nta** info hay en cada una ($s^2$). Es como hacer PCA, pero sin tener que **calcular** la matriz de covarianza.

## C√≥mo Elegir $k$ (N√∫mero de Componentes)

Cuando hacemos PCA necesitamos preguntarnos "**¬øcu√°ntas direcciones** necesito para obtener lo **m√°s** importante?". **Aqu√≠** entramos en los **conceptos** de **varianza explicada** y **varianza acumulada**.

* **Varianza explicada:** Cada componente principal (esas nuevas direcciones que PCA encuentra) explica una parte de la variaci√≥n total que hay en tus datos. Es como decir: "**¬øCu√°nto** de la info original me **est√°** mostrando esta **direcci√≥n**?"
* **Varianza acumulada:** Es la suma de la varianza explicada de los primeros $k$ componentes. Nos ayuda a saber **cu√°nta informaci√≥n** estamos obteniendo si usamos solo $k$ componentes.

## ¬øC√≥mo saber cu√°ntos componentes necesito para conservar, por ejemplo, el 95% de la info (varianza)?

Primero, **calcul√°s** la varianza que explica cada componente, y **despu√©s** vas sumando una por una (eso se llama varianza acumulada).

* **Si us√°s los `valores_propios` (eigenvalues):**
    * **Sum√°s** todos los valores propios, eso te da la varianza total.
    * Para cada componente, **divid√≠s** su valor propio entre la varianza total; eso te da el porcentaje de varianza que explica ese componente.
    * Vas sumando esos porcentajes hasta que llegues o pases el umbral del 95%.
* **Si us√°s los `s` de SVD (valores singulares):**
    * **Elev√°s** al cuadrado cada n√∫mero de `s` ($s^2$), eso te dice **cu√°nta** "varianza" (informaci√≥n) tiene cada componente.
    * **Sum√°s** todos esos cuadrados; eso te da la informaci√≥n total que hay en los datos.
    * **Despu√©s**, para cada componente, **divid√≠s** su valor al cuadrado ($s^2$) entre la suma total ‚Üí as√≠ ves qu√© porcentaje de info aporta ese componente.
    * Vas sumando esos porcentajes, uno por uno, hasta que la suma llegue al 95%.

## Puedes explicar con una analog√≠a simple o geom√©trica qu√© representan los componentes principales? ¬øC√≥mo se relaciona la p√©rdida de informaci√≥n con la reducci√≥n de dimensiones?

**Componentes principales:**
Los componentes principales pueden entenderse como nuevas direcciones o ejes que nos permiten describir los datos de manera m√°s compacta.
Una forma de verlo es imaginar que las fotos representan informaci√≥n. Lo que los componentes principales buscan es organizar esta informaci√≥n de manera que no se pierda demasiado detalle.

En t√©rminos visuales, podr√≠amos pensar en dibujar una l√≠nea imaginaria que pase por el punto de mayor dispersi√≥n de los datos. Esta l√≠nea representar√≠a el primer componente principal, la direcci√≥n con mayor varianza. Es decir, la l√≠nea que captura la mayor parte de la variaci√≥n en los datos.

Despu√©s, podemos dibujar una segunda l√≠nea que debe ser ortogonal a la primera. Esta segunda l√≠nea captura la mayor cantidad restante de informaci√≥n, es decir, la segunda mayor varianza. As√≠ sucesivamente para cada componente principal.

**C√≥mo se relaciona la p√©rdida de informaci√≥n con la reducci√≥n de dimensiones:**
Reducir dimensiones es comparable a resumir una historia: retienes lo m√°s importante, pero inevitablemente se pierde parte de la informaci√≥n.
Siguiendo la analog√≠a de los datos como una nube de puntos, el primer componente principal captura la mayor parte de la informaci√≥n contenida en los datos. Sin embargo, como se descartan los componentes restantes, se pierde informaci√≥n.
Esta informaci√≥n perdida corresponde a la varianza explicada por los componentes principales que decidimos no conservar.

# Explicaciones Tarea 3

## Descripci√≥n de las operaciones realizadas con Pandas

En la primera parte del script, **utilic√©** la **librer√≠a** Pandas para realizar varias operaciones de **manipulaci√≥n** y **an√°lisis** sobre el conjunto de datos de **Iris**. A **continuaci√≥n**, se **describen** las **principales** operaciones que **realic√©**:

* **Carga de datos:** **Utilic√©** el **m√©todo** `pd.read_csv()` para cargar el archivo `Iris.csv` en un DataFrame llamado `df`. Este DataFrame contiene las **caracter√≠sticas** de las flores de **Iris**.

* **Visualizaci√≥n de las primeras y √∫ltimas filas:** **Us√©** `df.head()` para mostrar las primeras 5 filas del DataFrame y obtener una vista **r√°pida** de los primeros registros. **Tambi√©n** **utilic√©** `df.tail()` para ver las **√∫ltimas** 5 filas y verificar **c√≥mo** **terminan** los datos.

* **Resumen del DataFrame:** Con `df.info()`, **obtuve** un resumen general del DataFrame, incluyendo el **n√∫mero** de entradas, el **tipo** de datos de cada columna y la cantidad de valores no nulos.

* **Estad√≠sticas descriptivas:** **Utilic√©** `df.describe()` para obtener **estad√≠sticas** descriptivas de las columnas **num√©ricas** del DataFrame, como la media, **desviaci√≥n** **est√°ndar**, **m√≠nimos**, **m√°ximos** y percentiles. Posteriormente, **redonde√©** los resultados a tres decimales con `.round(3)`.

* **Conteo de categor√≠as:** Para saber **cu√°ntas** veces **aparece** cada valor en la columna `Species`, **utilic√©** `df['Species'].value_counts()`, lo que me **dio** la **distribuci√≥n** de las especies en el conjunto de datos.

* **Selecci√≥n de filas y columnas por posici√≥n:** **Us√©** `df.iloc[-1, [1, 3]]` para seleccionar la **√∫ltima** fila y las columnas en las **posiciones** 1 y 3 (correspondientes a `SepalWidthCm` y `PetalWidthCm`).

* **Selecci√≥n de filas y columnas por etiquetas:** Con `df.loc[149, ['SepalWidthCm', 'PetalWidthCm']]`, **seleccion√©** la fila con el **√≠ndice** 149 y las columnas `SepalWidthCm` y `PetalWidthCm` por sus nombres.

* **Filtrado de datos con condiciones combinadas:** **Apliqu√©** un filtro combinado con `df[(df['Species'] == 'Iris-setosa') & (df['SepalLengthCm'] < 5)]` para seleccionar las filas donde la **especie** es `Iris-setosa` y la **longitud** del **s√©palo** es menor a 5.

Estas **operaciones** son fundamentales para explorar y entender los datos antes de pasar a las siguientes etapas del **an√°lisis** o **modelado**.

## Naive Bayes

Podemos pensar en Naive Bayes como un detective que trata de resolver un caso: descubrir qu√© tipo de flor es una nueva flor que encontr√≥, bas√°ndose en la evidencia (sus medidas) y su experiencia previa.

### Pasos del Algoritmo (Implementaci√≥n)

1.  **Cargar Datos:** Es muy importante empezar importando los datos necesarios, en este caso, usando `load_iris()`. A este resultado le damos un nombre (ej: `datos`). Accedemos a los datos de dos maneras principales:
    * `.data` (guardado en `$X$`): Contiene las caracter√≠sticas num√©ricas (las 4 medidas de cada flor).
    * `.target` (guardado en `$Y$`): Contiene las etiquetas de clase (0, 1 o 2) para cada flor.

2.  **Separar Datos por Clase:** Tomamos la matriz `$X$` y, usando las etiquetas `$Y$`, la separamos en tres grupos: uno para cada clase (0, 1 y 2). As√≠ tenemos `X_clase0`, `X_clase1`, `X_clase2`.

3.  **Calcular Estad√≠sticas ("Entrenamiento"):** Una vez separados los datos por clase, calculamos para cada una:
    * **Media (`np.mean(..., axis=0)`):** El valor promedio de cada una de las 4 caracter√≠sticas para esa clase espec√≠fica.
    * **Desviaci√≥n Est√°ndar (`np.std(..., axis=0)`):** Cu√°nto var√≠an o se dispersan las medidas de cada caracter√≠stica alrededor de su media, para esa clase.
    * **Probabilidad Previa (Prior):** Qu√© tan com√∫n es cada especie en el conjunto total de datos (calculado como: n√∫mero de flores de la clase / n√∫mero total de flores).

4.  **Agrupar Estad√≠sticas:** Guardamos todas las medias, desviaciones est√°ndar y priors calculados en listas (`lista_medias`, `lista_stds`, `lista_priors`) para usarlas f√°cilmente en la predicci√≥n.

5.  **Funci√≥n `gaussian_pdf`:** Definimos una funci√≥n auxiliar importante. Esta calcula la Densidad de Probabilidad Gaussiana: dado un valor `$x$`, una media `$mu$` y una desviaci√≥n est√°ndar `$std$`, nos dice qu√© tan "probable" o "t√≠pico" es ese valor `$x$` si perteneciera a una distribuci√≥n normal (Campana de Gauss) con esa `$mu$` y `$std$`. Es como preguntarle a la campana: "¬øQu√© altura ten√©s en este punto `$x$`?".

6.  **Funci√≥n `predecir_clases_nb` (Predicci√≥n):** Esta es la funci√≥n principal. Recibe una flor nueva (`flor_nueva`) y las listas de estad√≠sticas. Para decidir la clase:
    * Crea una lista vac√≠a (`posteriors`) para guardar los "scores".
    * Usa un bucle `for` para recorrer cada clase posible (0, 1, 2).
    * **Dentro del bucle:**
        * Obtiene las estad√≠sticas (`media_actual`, `std_actual`, `prior_actual`) de la clase actual.
        * Calcula el **Likelihood**: Usa `gaussian_pdf` para obtener la probabilidad de cada una de las 4 caracter√≠sticas de la `flor_nueva` seg√∫n la media y `$std$` de la clase actual. Luego, multiplica estas 4 probabilidades (¬°la asunci√≥n "naive"!) usando `np.prod` para obtener la probabilidad total de observar esas caracter√≠sticas si la flor fuera de esta clase. (Nota: en el c√≥digo final usamos logaritmos para evitar problemas num√©ricos, sumando $\log(\text{PDFs})$ en lugar de multiplicar PDFs).
        * Calcula el **Score Posterior**: Multiplica el `likelihood` por el `prior_actual` (o suma sus logaritmos).
        * Guarda este `posterior_actual` en la lista `posteriors`.
    * **Despu√©s del bucle:** Compara los 3 scores guardados en `posteriors` y elige el **√≠ndice** (0, 1 o 2) del score m√°s alto usando `np.argmax`.
    * Devuelve ese √≠ndice como la clase predicha.

7.  **Probar Nuestro Clasificador:**
    * Hacemos un bucle que recorre todas las flores del dataset original (`$X$`).
    * Para cada flor, llamamos a `predecir_clases_nb` para obtener su predicci√≥n.
    * Guardamos todas las predicciones.
    * Calculamos la **Precisi√≥n (Accuracy)**: comparamos nuestras predicciones con las etiquetas reales (`$Y$`) y vemos el porcentaje de aciertos.

8.  **Comparar con Scikit-learn:**
    * Usamos la implementaci√≥n `GaussianNB` de Scikit-learn, la entrenamos (`fit`) y predecimos (`predict`) con los mismos datos `$X$` e `$Y$`.
    * Calculamos su precisi√≥n para tener una referencia y ver si nuestro modelo manual da resultados similares.


## Conceptos Clave Relacionados

### Teorema de Bayes (Regla General)

* **¬øQu√© es?:** Es una f√≥rmula matem√°tica para actualizar una probabilidad o creencia inicial (`Prior`) bas√°ndonos en nueva evidencia (`Datos`) para obtener una probabilidad final (`Posterior`).
* **Idea:** $P(\text{Clase} | \text{Datos}) \propto P(\text{Datos} | \text{Clase}) \times P(\text{Clase})$ (El posterior es proporcional al likelihood por el prior).
* **En resumen:** Nos dice c√≥mo combinar lo que ya sab√≠amos con la nueva evidencia.

### Naive Bayes (El Algoritmo de Clasificaci√≥n)

* **¬øQu√© es?:** Un m√©todo de clasificaci√≥n que **usa** el Teorema de Bayes para decidir a qu√© clase pertenece una nueva muestra.
* **La parte "Naive" (Ingenua):** Su caracter√≠stica principal es que **asume (ingenuamente)** que todas las caracter√≠sticas (las 4 medidas de la flor) son **independientes entre s√≠** dada la clase. Esto simplifica much√≠simo el c√°lculo del Likelihood ($P(\text{Datos}|\text{Clase})$), permitiendo multiplicar las probabilidades individuales de cada caracter√≠stica (o sumar sus logaritmos).
* **En resumen:** Aplica Bayes con una simplificaci√≥n clave (independencia) para clasificar.

### Campana de Gauss (Distribuci√≥n Normal)

* **¬øQu√© es?:** Una forma matem√°tica muy com√∫n (la curva en forma de campana) que describe c√≥mo se distribuyen muchos datos num√©ricos alrededor de una media, con una cierta dispersi√≥n (desviaci√≥n est√°ndar).
* **En Gaussian Naive Bayes:** Hacemos la **suposici√≥n** de que las caracter√≠sticas num√©ricas *dentro de cada clase* siguen esta distribuci√≥n Gaussiana. Esto nos permite usar la f√≥rmula de la Campana de Gauss (nuestra funci√≥n `gaussian_pdf`) para calcular las probabilidades $P(\text{caracter√≠stica}_k | \text{Clase})$ que necesitamos para el Likelihood.
* **En resumen:** Es el modelo de probabilidad que usamos para las caracter√≠sticas num√©ricas en esta versi√≥n espec√≠fica de Naive Bayes.

### Resumen de la Relaci√≥n

El **Teorema de Bayes** nos da el marco general. El algoritmo **Naive Bayes** lo aplica para clasificar, a√±adiendo la suposici√≥n "naive" de independencia. La **Campana de Gauss** es la herramienta que usamos en *Gaussian* Naive Bayes para calcular las probabilidades de las caracter√≠sticas num√©ricas dentro de ese marco.

# Explicaciones Tarea 4

## Regresi√≥n Lineal

### ¬øQu√© es?

La **regresi√≥n lineal** es una t√©cnica estad√≠stica que busca la relaci√≥n entre una variable cuantitativa ($Y$) y una o m√°s variables predictoras ($X$).
El **objetivo** es **predecir valores num√©ricos continuos**, bas√°ndose en la suposici√≥n de que existe una relaci√≥n lineal entre las variables explicativas ($X$) y la variable objetivo ($Y$).

---

### ¬øQu√© tipo de resultado produce?

**Variable cuantitativa continua:**
No clasifica en categor√≠as, sino que entrega un valor num√©rico que puede tomar cualquier valor dentro de un rango.

---

### Interpretaci√≥n de los coeficientes

Los coeficientes del modelo ($\theta$) indican c√≥mo cambia $Y$ ante variaciones en $X$.

---

## Funci√≥n Hip√≥tesis ($h_\theta(X) = X \theta$)

Ecuaci√≥n fundamental en la regresi√≥n lineal: $h_\theta(X) = X \theta$. Define la relaci√≥n entre las variables predictoras ($X$) y la variable objetivo ($Y$).

### Objetivo

Predecir valores num√©ricos continuos bas√°ndose en una combinaci√≥n lineal de caracter√≠sticas de entrada y coeficientes asociados.

### Elementos

1.  **$X$**: Matriz de caracter√≠sticas (inputs)
    * Cada fila representa una observaci√≥n (ejemplo de entrenamiento)
    * Cada columna corresponde a una caracter√≠stica (variable predictora)
    * Se agrega una columna de unos para incluir el intercepto ($\theta_0$)

2.  **$\theta$**: Vector de par√°metros (coeficientes)
    * Contiene los pesos que el modelo aprende durante el entrenamiento
    * Incluye el intercepto ($\theta_0$)
    * $\theta_1, \theta_2, \dots$ indican la influencia de cada caracter√≠stica

3.  **$h_\theta(X)$**: Predicci√≥n del modelo
    * Resultado del producto matricial $X\theta$
    * Cada valor $h_\theta(x^{(i)})$ es la predicci√≥n para la observaci√≥n $i$

---

## Funci√≥n de Coste (MSE)

**F√≥rmula:**
$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$

Mide el **promedio del error cuadr√°tico** entre las predicciones del modelo y los valores reales en todo el conjunto de datos.

* **$m$**: N√∫mero de observaciones
* **$h_\theta(x^{(i)})$**: Predicci√≥n para la observaci√≥n $i$
* **$y^{(i)}$**: Valor real para la observaci√≥n $i$

---

### ¬øPor qu√© se eleva al cuadrado la diferencia?

1.  **Evita errores negativos:** Las diferencias se vuelven positivas.
2.  **Penaliza errores grandes:** Un error de 2 pesa m√°s (4) que uno de 1 (1).
3.  **Facilita la optimizaci√≥n:** La funci√≥n cuadr√°tica es convexa y garantiza un m√≠nimo global.

---

## ¬øPor qu√© queremos minimizar $J(\theta)$?

Minimizar $J(\theta)$ significa ajustar los par√°metros $\theta$ para que las predicciones sean lo m√°s cercanas posible a los valores reales.

### M√©todos comunes:

* **M√≠nimos Cuadrados Ordinarios (OLS):** Soluci√≥n anal√≠tica.
* **Descenso de Gradiente (GD):** M√©todo iterativo.

---

## ¬øQu√© implica un $J(\theta)$ peque√±o?

* **Buen ajuste:** Las predicciones est√°n cerca de los valores reales.
* **Alta precisi√≥n:** El modelo generaliza bien.
* **Menor incertidumbre:** Los errores (residuos) tienen baja variabilidad.

---

# Descenso de Gradiente (Gradient Descent)

El **descenso de gradiente** busca minimizar $J(\theta)$, ajustando iterativamente los par√°metros $\theta$ para reducir el error.

### En resumen:

Es un m√©todo que permite a un modelo **aprender** los mejores valores de los par√°metros $\theta$, optimizando la predicci√≥n. El resultado del entrenamiento es el vector `theta_final`, que contiene los coeficientes √≥ptimos encontrados por el modelo.

---

## Regla de Actualizaci√≥n

$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)$ (Esta es la regla impl√≠cita que se describe)

### Componentes

**$\alpha$ (Tasa de Aprendizaje):**

* Controla el tama√±o del paso en cada iteraci√≥n.
* $\alpha$ alto ‚Üí puede hacer que el algoritmo no converja.
* $\alpha$ bajo ‚Üí puede hacer que la convergencia sea muy lenta.

**Derivada Parcial ($\frac{\partial}{\partial \theta_j} J(\theta)$):**

* Mide la direcci√≥n de mayor aumento de $J(\theta)$
* Al restarla, el modelo se mueve en direcci√≥n descendente (hacia el m√≠nimo).

---

## Gradiente Vectorizado

**F√≥rmula:**
$\nabla J(\theta) = \frac{1}{m} X^T (X\theta - y)$

### T√©rminos

1.  **$X\theta$**: Vector de predicciones para todas las observaciones.
2.  **$X\theta - y$**: Vector de errores residuales.
3.  **$X^T$**: Multiplicaci√≥n por la transpuesta de $X$ pondera los errores por cada caracter√≠stica.
4.  **$\frac{1}{m}$**: Promedia el gradiente sobre el conjunto de datos.

---

## ¬øQu√© representa el resultado?

El vector $\nabla J(\theta)$ nos dice:

* **Direcci√≥n:**
    * Si $\frac{\partial J}{\partial \theta_j} > 0$ = disminuir $\theta_j$
    * Si $\frac{\partial J}{\partial \theta_j} < 0$ = aumentar $\theta_j$

* **Magnitud:**
    * Cu√°nto influye ese $\theta_j$ en el error total

---

## Resumen

* **Objetivo:** Minimizar $J(\theta)$ ajustando $\theta$ para reducir errores
* **Tasa de Aprendizaje ($\alpha$):** Controla la velocidad de convergencia
* **Gradiente Vectorizado:** Forma eficiente de calcular el ajuste de todos los coeficientes a la vez

# üìò Pasos del Algoritmo de Regresi√≥n Lineal (California Housing)

## 1. üóÇÔ∏è Importaci√≥n de Datos

Importamos el dataset **California Housing** y las funciones necesarias de **NumPy**.

* `.data`: contiene las **caracter√≠sticas** (features), en forma de matriz ($X$).
* `.target`: contiene los **valores a predecir** (precios promedio de casas) ($Y$).

Se ajust√≥ la forma de $Y$ usando `np.reshape` para trabajar con matrices columna, sin alterar los datos.

---

## 2. ‚öñÔ∏è Normalizaci√≥n (Estandarizaci√≥n)

Calculamos la **media ($\mu$)** y **desviaci√≥n est√°ndar ($\sigma$)** de los datos originales para escalar las caracter√≠sticas. Esto es clave porque:

* Las variables originales tienen diferentes escalas.
* Sin escalar, el descenso de gradiente puede ser lento o ineficaz.
* El escalado mejora la velocidad y estabilidad del entrenamiento.

**üîß Sin este paso, el modelo devolv√≠a `null` en `theta_final`, sin importar los hiperpar√°metros.**

---

## 3. ‚ûï Agregar Columna de Unos (Bias)

Se a√±adi√≥ una columna de unos a `X_scaled` para permitir que el modelo aprenda un **t√©rmino independiente** ($\theta_0$), haciendo que la recta **no tenga que pasar por el origen (0,0)**.
Esto da **flexibilidad** al modelo.

---

## 4. üîÆ Funci√≥n de Hip√≥tesis

Se implement√≥ la funci√≥n `calcular_hipotesis(X, theta)`:

* Predice valores continuos.
* Es la base de la f√≥rmula de **regresi√≥n lineal**:
    $h(\theta) = X \theta$

---

## 5. ‚ùå Funci√≥n de Coste (MSE)

Se implement√≥ la funci√≥n de coste: **Error Cuadr√°tico Medio (Mean Squared Error)**. $J(\theta)$.

Mide el **promedio del error al cuadrado** entre predicciones ($h_\theta(x^{(i)})$) y valores reales ($y^{(i)}$).

üìâ El objetivo es **minimizarla**:

* **MSE alta** ‚Üí el modelo se equivoca mucho.
* **MSE baja** ‚Üí el modelo est√° aprendiendo bien.

---

## 6. üîÅ Descenso de Gradiente

Se implement√≥ el **descenso de gradiente** para minimizar el error:

* Calcula predicciones, errores y gradiente ($\nabla J(\theta)$) en cada iteraci√≥n.
* Actualiza $\theta$ con la regla de aprendizaje:
    $\theta := \theta - \alpha \nabla J(\theta)$

Se definieron:

* `theta_inicial`: vector de ceros
* `$\alpha$` (tasa de aprendizaje)
* `n_iteraciones`

Tambi√©n se grafic√≥ el **historial de coste** para visualizar la convergencia del algoritmo.
![alt text](<Regresion_Lineal.py/Grafico Historial de coste.png>)
**La gr√°fica mostr√≥ que el coste convergi√≥ de forma estable a un valor aproximado de X.XX despu√©s de unas YYY iteraciones.** (<- **¬°RECUERDA REEMPLAZAR X.XX e YYY con tus valores!**)

---

## 7. ü§ñ Funci√≥n de Predicci√≥n

La funci√≥n `predecir(X_nuevos, theta_final, mu, sigma)` permite usar el modelo entrenado con **nuevos datos**:

1.  Escala los nuevos datos con $\mu$ y $\sigma$ del entrenamiento.
2.  A√±ade la columna de unos (bias).
3.  Aplica la f√≥rmula de regresi√≥n lineal ($h_\theta(X) = X \theta$) para predecir precios.

---

## ‚úÖ Conclusi√≥n

Este modelo permite predecir el precio promedio de casas en California usando regresi√≥n lineal multivariable, correctamente entrenada y escalada.
Con el descenso de gradiente y la MSE como gu√≠a, podemos ajustar $\theta$ hasta encontrar una soluci√≥n eficiente y precisa.

## Punto 1: Consolidar el Aprendizaje üß†

Dependiendo del valor de **alpha**, podemos observar cu√°nto tiempo tarda en **converger** el algoritmo.

* Si el **alpha es muy peque√±o**, el descenso de gradiente avanza muy lento.
* Si el **alpha es muy grande**, el algoritmo puede **omitir el aprendizaje** o incluso **divergir** (los valores crecen en lugar de estabilizarse).

üìà
*Figura 1*
![alt text](Regresion_Lineal.py/Figure_1.png)

Gracias a la comparaci√≥n de los valores en la gr√°fica, podemos encontrar un **alpha ideal**:


üìâ
*Figura 2*
![alt text](Regresion_Lineal.py/Figure_2.png)


Sin el **escalado**, el descenso de gradiente tarda m√°s o simplemente **no converge** (pueden aparecer datos 'null').
En cambio, cuando **escalamos las caracter√≠sticas**:

* Las variables tienen una magnitud parecida.
* El algoritmo avanza mejor.
* Se pueden usar valores de alpha m√°s grandes sin que se vuelva inestable.
* Todo converge de forma m√°s r√°pida y eficiente.


*Gr√°fico del historial de coste*
![alt text](<Regresion_Lineal.py/Grafico Historial de coste.png>)


### ‚úÖ En resumen:

* Escalar las caracter√≠sticas mejora la **eficiencia del algoritmo**.
* Elegir bien el valor de **alpha** hace que el modelo **converja m√°s r√°pido** sin salirse de control.

##  Punto 2: Evaluar el N√∫mero de Iteraciones ‚è±Ô∏è

Con distintos valores de alpha, podemos observar que, aproximadamente a partir de las 2500 iteraciones, las curvas comienzan a aplanarse. Esto indica que el algoritmo empieza a converger, ya que el coste deja de disminuir significativamente.

En mi experimento utilic√© 4000 iteraciones como n√∫mero total. Eleg√≠ este valor porque, al probar varios valores de alpha, quer√≠a asegurarme de observar con claridad en qu√© punto cada curva se aplanaba por completo. Esto me permiti√≥ identificar con mayor precisi√≥n cu√°ndo el algoritmo realmente comenzaba a converger en cada caso.

![alt text](Regresion_Lineal.py/Figure_1.2.png)



# Regresi√≥n Lineal: Ecuaci√≥n Normal vs Descenso de Gradiente  

## Estructura de la Matriz **X** y el Vector **y**  
### **Matriz X (Dise√±o)**  
- **Contenido**:  
  - Columna de **unos (1)** para el intercepto (`Œ∏‚ÇÄ`).  
  - Columnas de **caracter√≠sticas** (`X‚ÇÅ, X‚ÇÇ, ..., X‚Çô`).  
- **Dimensiones**:  
  `m √ó (n+1)`  *(m observaciones, n caracter√≠sticas)*  

### **Vector y (Objetivo)**  
- **Contenido**: Valores reales a predecir.  
- **Dimensiones**:  
  `m √ó 1`  

## üîç **Comparaciones Clave**  
### üìä Resultados Experimentales  
| **Escenario**            | Diferencia (Error) | Comparaci√≥n V√°lida |  
|--------------------------|--------------------|--------------------|  
| Theta GD (escalado) vs Theta EN (**sin escalar**) | ‚âà 111              | ‚ùå No (escalas distintas) |  
| Theta GD (escalado) vs Theta EN (**escalado**)    | ‚âà 9.9              | ‚úÖ S√≠               |  

### ‚ùì **Interpretaci√≥n**  
1. **Diferencia ‚âà 111**:  
   - Ilustra c√≥mo el **escalado afecta los valores absolutos de `Œ∏`**.  
   - **No es v√°lida t√©cnicamente** (comparar `Œ∏` en escalas diferentes no tiene sentido matem√°tico).  

2. **Diferencia ‚âà 9.9**:  
   - Muestra que el Descenso de Gradiente (**GD**) **no convergi√≥ totalmente** por falta de iteraciones.  

---

## üßÆ **Ecuaci√≥n Normal: F√≥rmula e Implementaci√≥n**  
### **F√≥rmula Anal√≠tica**  


Œ∏ = (X·µó X)‚Åª¬π X·µó y



### **Pasos de Implementaci√≥n**  
1. Calcular `X·µó X`.  
2. Invertir la matriz resultante.  
3. Multiplicar por `X·µó y`.  

## ‚öñÔ∏è **Pros y Contras**  
| **M√©todo**           | **Ecuaci√≥n Normal**                              | **Descenso de Gradiente**                     |  
|----------------------|--------------------------------------------------|-----------------------------------------------|  
| **Ventajas**         | - Soluci√≥n exacta en 1 paso.<br>- Sin hiperpar√°metros.<br>- No requiere escalado. | - Escalable a grandes `n`.<br>- Funciona incluso si `X·µó X` es singular. |  
| **Desventajas**      | - Coste `O(n¬≥)` (lento para `n > 10‚Å¥`).<br>- Falla si `X·µó X` no es invertible. | - Necesita ajustar `Œ±` e iteraciones.<br>- Requiere escalado para converger bien. |  

---

## üöÄ **¬øCu√°ndo Usar Cada M√©todo?**  
| **Criterio**               | **Ecuaci√≥n Normal**          | **Descenso de Gradiente**       |  
|----------------------------|------------------------------|---------------------------------|  
| **N√∫mero de caracter√≠sticas** | `n < 10‚Å¥`                 | `n ‚â• 10‚Å¥`                      |  
| **Estabilidad matricial**  | Evitar si `X·µó X` es singular | Funciona siempre               |  
| **Recursos computacionales** | Adecuado para CPU/GPU moderadas | Ideal para clusters distribuidos |  

---

**Notas Finales**:  
- Usar `np.linalg.pinv` en lugar de `inv` para manejar matrices singulares.  
- El escalado en GD es **cr√≠tico** para convergencia r√°pida y estable.  



