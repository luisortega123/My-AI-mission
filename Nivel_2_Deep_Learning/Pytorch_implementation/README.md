# üöÄ Implementaci√≥n de Redes Neuronales con PyTorch

> "De NumPy manual a PyTorch autom√°tico: transformando c√°lculos complejos en l√≠neas elegantes de c√≥digo."

---

<details>
  <summary><strong>TL;DR: Resumen y Analog√≠a (Haz clic para expandir)</strong></summary>
  
  ### Resumen Clave üìù
  Estamos evolucionando nuestro modelo de red neuronal `run_moon` desde implementaci√≥n manual con NumPy hacia PyTorch. PyTorch nos permite automatizar c√°lculos de gradientes y operaciones tensoriales de forma m√°s intuitiva y eficiente, utilizando la herramienta **autograd** para el seguimiento autom√°tico de operaciones.

  ### Analog√≠a para Entenderlo Mejor üí°
  Imagina que antes constru√≠as una casa ladrillo por ladrillo (NumPy manual), calculando cada √°ngulo y medida a mano. Ahora tienes una m√°quina constructora inteligente (PyTorch) que no solo coloca los ladrillos autom√°ticamente, sino que tambi√©n **recuerda** cada paso del proceso para poder deshacerlo o mejorarlo despu√©s (autograd). ¬°Es como tener un asistente de construcci√≥n con memoria fotogr√°fica!

</details>

---

## üìñ Explicaci√≥n Completa y Sencilla

### üéØ El Gran Salto: De NumPy a PyTorch

PyTorch revoluciona nuestro enfoque anterior al ofrecer **operaciones tensoriales** similares a NumPy, pero espec√≠ficamente dise√±adas para redes neuronales. La gran ventaja es que PyTorch puede realizar autom√°ticamente todos esos c√°lculos de gradientes que antes hac√≠amos manualmente.

### üîß La Magia de Autograd

**Autograd** es el coraz√≥n de PyTorch. Su funci√≥n principal es:

* **Seguimiento autom√°tico** de todas las operaciones matem√°ticas
* **Construcci√≥n del grafo computacional** necesario para calcular gradientes
* **Diferenciaci√≥n autom√°tica** sin intervenci√≥n manual

Para activar esta magia, necesitamos el atributo especial:

```python
tensor.requires_grad = True
```

### üèóÔ∏è Configuraci√≥n de Nuestros Componentes

Para que todo funcione correctamente, establecemos que:

**Par√°metros principales:**
* `W1, b1, W2, b2` ‚Üí tensores con `requires_grad=True`
* `X, Y` (datos) ‚Üí tambi√©n convertidos a tensores

### üé® Construyendo Nuestra Clase de Red Neuronal

Definimos nuestra **clase blueprint** para construir la red:

```python
class MyNeuralNetwork(nn.Module):
```

Esta clase hereda de `nn.Module`, que es la **base fundamental** para todas las redes en PyTorch.

### üî® El Constructor: Funci√≥n `__init__`

La funci√≥n especial `__init__` se ejecuta autom√°ticamente al crear un objeto:

```python
def __init__(self, n_x, n_h, n_y):
```

**Par√°metros de la arquitectura:**
* `n_x` ‚Üí n√∫mero de entradas (features de input)
* `n_h` ‚Üí neuronas en la capa oculta
* `n_y` ‚Üí n√∫mero de salidas (predicciones finales)

### ‚ö° Inicializaci√≥n Correcta

```python
super().__init__()
```

Esta l√≠nea **inicializa correctamente** la clase base `nn.Module`, asegurando que todas las funcionalidades de PyTorch est√©n disponibles.

### üß† Definiendo las Capas de la Red

**Primera capa lineal:**
```python
self.layer_1 = nn.Linear(n_x, n_h)
```

Esto reemplaza todas las operaciones matriciales manuales que hac√≠amos antes. PyTorch maneja autom√°ticamente la inicializaci√≥n de pesos y las multiplicaciones matriz-vector.

**Funci√≥n de activaci√≥n no lineal:**
```python
self.activation_1 = nn.Tanh()
```

La **no linealidad es crucial** - sin ella, m√∫ltiples capas lineales se reducir√≠an a una sola transformaci√≥n lineal, limitando dr√°sticamente las capacidades del modelo.

**Segunda capa lineal:**
```python
self.layer_2 = nn.Linear(n_h, n_y)
```

Esta capa final toma las `n_h` salidas de la capa oculta y produce las `n_y` predicciones finales.

## ‚ú® Puntos Finales

* **Automatizaci√≥n inteligente:** PyTorch maneja autom√°ticamente c√°lculos complejos que antes requer√≠an implementaci√≥n manual
* **Autograd = superpoder:** El seguimiento autom√°tico de gradientes revoluciona el entrenamiento de redes neuronales
* **Arquitectura modular:** La herencia de `nn.Module` proporciona una estructura robusta y extensible para construir redes complejas



## üß† Definir la funci√≥n `forward`

Despu√©s de definir nuestra red neuronal, el siguiente paso es crear el m√©todo `forward`. Esta funci√≥n se encarga de pasar los datos de entrada `x` a trav√©s de las capas del modelo.

### ‚öôÔ∏è ¬øQu√© hace el m√©todo `forward`?

1. üß© **Capa oculta**  
   Aplica una transformaci√≥n lineal a los datos y luego una funci√≥n de activaci√≥n no lineal (por ejemplo, `ReLU`).

2. üéØ **Capa de salida**  
   Por ahora **no** aplicamos ninguna funci√≥n de activaci√≥n aqu√≠. M√°s adelante usaremos `BCEWithLogitsLoss`, que **ya incluye** la activaci√≥n `sigmoid` autom√°ticamente.



## üõ†Ô∏è Crear una instancia del modelo

A continuaci√≥n, creamos un objeto (instancia) de nuestra red neuronal personalizada:

```python
model = MyNeuralNetwork(n_x=input_size, n_h=hidden_size, n_y=output_size)
print(model)
```

‚úÖ Esto inicializa nuestra red con la arquitectura definida y nos permite ver su estructura.


## üìö ¬øQu√© necesitamos para entrenar el modelo?

Antes de comenzar a entrenar, asegur√©monos de tener todos los componentes clave. A continuaci√≥n, los explicamos de forma sencilla:

1. üß† **El Modelo (`MyNeuralNetwork`)**

   > Es el "cerebro" del sistema. Ya lo construimos.

2. üìä **Los Datos (`X` e `Y`)**

   > Son el "libro de texto". El modelo los usar√° para aprender.

3. ‚ùó **Funci√≥n de P√©rdida (Loss Function)**

   > Es como un "examen". Le dice al modelo qu√© tan equivocadas son sus predicciones.
   > En este caso, usamos `BCEWithLogitsLoss`, que combina:

   * üåÄ `Sigmoid`: convierte la salida en una probabilidad.
   * üìâ `Binary Cross-Entropy`: mide el error de clasificaci√≥n binaria.

4. üßÆ **Optimizador (Optimizer)**

   > Es el "tutor". Ayuda al modelo a mejorar ajustando sus pesos en cada iteraci√≥n.

5. üîÅ **Bucle de Entrenamiento (Training Loop)**

   > Es el "horario de estudio". Repite muchas veces el proceso de:

   * Aprender üß†
   * Evaluarse ‚ùó
   * Corregirse üîß

üöÄ ¬°Con todo esto listo, ya podemos pasar al entrenamiento de la red neuronal!

# üöÄ IMPLEMENTACI√ìN

En esta secci√≥n, vamos a construir y entrenar nuestra red neuronal utilizando PyTorch, junto con algunas librer√≠as adicionales como NumPy y Matplotlib para el manejo de datos y visualizaci√≥n.

---

## üîß Importaciones necesarias

Primero importamos las librer√≠as esenciales:

- `torch`: Para manejar tensores y construir el modelo.
- `numpy`: Para operaciones num√©ricas.
- `matplotlib`: Para graficar resultados.
- Nuestra clase `MyNeuralNetwork` desde el script donde definimos la arquitectura y funciones de entrenamiento.

De esta manera, tenemos listo el ‚Äúplano de construcci√≥n‚Äù de nuestra red neuronal.

---

## üß© Crear la instancia del modelo

Creamos nuestro modelo con:

```python
model = MyNeuralNetwork(n_x=input_size, n_h=hidden_size, n_y=output_size)
```

¬øPor qu√© hacemos esto?
Porque este objeto contiene toda la l√≥gica y estructura de la red, y ser√° la base para el entrenamiento. A diferencia de usar solo NumPy, PyTorch facilita el entrenamiento con menos c√≥digo y de forma m√°s eficiente.

---

## üìä Preparar los datos

Usamos el conjunto de datos `make_moons` de `sklearn`.

Para evitar errores en las operaciones matriciales, hacemos un `reshape` a las etiquetas para que tengan forma `(1, n)`.

Luego convertimos los datos de NumPy a tensores de PyTorch, ya que PyTorch trabaja internamente con tensores para realizar c√°lculos r√°pidos y eficientes:

```python
X = torch.from_numpy(X).float()
Y = torch.from_numpy(Y).float()
```

Finalmente, renombramos las variables para mejorar la claridad en el c√≥digo.

---

## ‚öôÔ∏è Definir funci√≥n de p√©rdida y optimizador

* Usamos la funci√≥n de p√©rdida **BCEWithLogitsLoss** (Binary Cross-Entropy con logits).

  > Esta funci√≥n es ideal para clasificaci√≥n binaria y ya incluye internamente la activaci√≥n `sigmoid`, por lo que no necesitamos aplicarla manualmente.

* Para optimizar, usamos el optimizador **Adam** con tasa de aprendizaje 0.01, que ajusta los pesos para minimizar la p√©rdida.

---

## üîÑ Bucle de entrenamiento

```python
epochs = 1000
for epoch in range(epochs):
    optimizer.zero_grad()                 # 1Ô∏è‚É£ Limpiar gradientes del paso anterior
    y_pred = model(X_train)               # 2Ô∏è‚É£ Forward pass: obtener predicciones
    loss = criterion(y_pred, y_train)    # 3Ô∏è‚É£ Calcular p√©rdida
    loss.backward()                      # 4Ô∏è‚É£ Backpropagation: calcular gradientes
    optimizer.step()                     # 5Ô∏è‚É£ Actualizar pesos

    # Mostrar la p√©rdida cada 100 √©pocas
    if epoch % 100 == 0:
        print(f"Loss (epoch {epoch}): {loss.item():.4f}")
```

### Paso a paso del loop:

* üîπ **`zero_grad()`**: Limpiamos los gradientes acumulados para evitar sumas incorrectas.
* üîπ **Forward pass**: Pasamos los datos por el modelo para obtener las predicciones.
* üîπ **Calcular p√©rdida**: Comparamos las predicciones con los valores reales.
* üîπ **Backpropagation**: Calculamos c√≥mo cambiar cada peso para reducir la p√©rdida.
* üîπ **Actualizar pesos**: El optimizador ajusta los pesos con base en los gradientes.
* üîπ **Monitoreo**: Imprimimos la p√©rdida peri√≥dicamente para seguir el progreso.

---

## üîÆ Funci√≥n de predicci√≥n

```python
def predict(x):
    model.eval()               # Cambiar a modo evaluaci√≥n (sin calcular gradientes)
    with torch.no_grad():      # Evitar c√°lculo de gradientes para optimizar memoria y velocidad
        logits = model(x)      
        probs = torch.sigmoid(logits)     # Convertir logits en probabilidades
        prediction = (probs >= 0.5).float()  # Umbral para clasificar en 0 o 1
    model.train()              # Volver a modo entrenamiento
    return prediction
```

> **¬øPor qu√© usar `torch.no_grad()`?**
> Durante el entrenamiento PyTorch guarda informaci√≥n para calcular gradientes. En predicciones esto no es necesario y consumir√≠a recursos extra, por eso desactivamos el c√°lculo de gradientes.

---

## üé® Funci√≥n para graficar la frontera de decisi√≥n

```python
def plot_decision_boundary(predict_fn, X, y):
    # 1Ô∏è‚É£ Crear una malla fina que cubra todo el espacio de caracter√≠sticas
    # 2Ô∏è‚É£ Predecir la clase para cada punto en la malla usando predict_fn
    # 3Ô∏è‚É£ Graficar la frontera de decisi√≥n coloreando seg√∫n la clase
    # 4Ô∏è‚É£ Dibujar los puntos originales para comparar con la clasificaci√≥n
```

Esta funci√≥n permite visualizar c√≥mo el modelo divide el espacio de caracter√≠sticas entre las distintas clases.

---

# üí° Abstracci√≥n vs. Control Manual en Redes Neuronales


## üèóÔ∏è Definici√≥n de capas: `nn.Linear` vs. creaci√≥n manual de matrices `W` y `b`

- **Control Manual con NumPy:**  
  - Inicializas t√∫ mismo las matrices de pesos `W` y vectores de sesgo `b` (por ejemplo, con ceros, valores aleatorios o distribuciones espec√≠ficas).  
  - Escribes manualmente todas las operaciones (multiplicaci√≥n matricial, sumas, activaciones).  
  - Ventaja: comprensi√≥n profunda de **c√≥mo funciona** una red neuronal.  
  - Desventaja: mucho c√≥digo y posibilidad de errores humanos, especialmente al escalar a redes grandes.

- **Con `nn.Linear` en PyTorch:**  
  - Es un m√≥dulo de alto nivel que **abstracta y automatiza** la creaci√≥n y actualizaci√≥n de pesos y sesgos.  
  - Maneja internamente las operaciones matriciales y la inicializaci√≥n adecuada.  
  - Ideal para proyectos reales porque reduce la complejidad y minimiza errores en operaciones complejas.  
  - El rango de error es mucho menor y facilita trabajar con redes neuronales profundas y grandes.

---

## üîÑ C√°lculo de Gradientes: funci√≥n manual `backward()` vs. `loss.backward()`

- **Funci√≥n `backward()` con NumPy:**  
  - Implementas t√∫ mismo el c√°lculo de derivadas parciales de la funci√≥n de p√©rdida con respecto a cada par√°metro (pesos y sesgos).  
  - Esto puede ser complicado y propenso a errores, sobre todo con operaciones m√°s complejas o muchas capas.  
  - El c√≥digo es extenso y dif√≠cil de mantener.

- **`loss.backward()` en PyTorch:**  
  - PyTorch usa **autograd**: calcula autom√°ticamente los gradientes de manera eficiente y segura.  
  - Profesional y optimizado para modelos reales y grandes.  
  - Reduce dr√°sticamente errores y permite enfocarse en dise√±o y experimentaci√≥n en lugar de c√°lculos manuales.

---

## üîÑ El "Ecosistema": Coexistencia de NumPy/Sklearn y PyTorch

- **NumPy y Sklearn:**  
  - Herramientas maduras y estables para manipulaci√≥n, preprocesamiento, generaci√≥n y evaluaci√≥n de datos.  
  - Permiten, por ejemplo, normalizar datos, hacer splits de entrenamiento/prueba, crear datasets sint√©ticos, etc.

- **PyTorch:**  
  - Fuerte en modelado, c√°lculo autom√°tico de gradientes, entrenamiento y despliegue de redes neuronales.

- **Uso conjunto:**  
  - Preparas y procesas datos con NumPy/Sklearn.  
  - Luego conviertes datos a tensores con `torch.from_numpy()` para alimentar a PyTorch.  
  - Esto reutiliza librer√≠as especializadas y evita reinventar la rueda.

- **Cuidado:**  
  - La conversi√≥n constante entre formatos puede provocar errores sutiles (tipos de datos, dimensiones, CPU vs GPU).  
  - En producci√≥n, intenta minimizar el cruce de frameworks para evitar bugs y mejorar rendimiento.

---

## üìà Curva de Aprendizaje: Mi experiencia personal

- Aprender NumPy y construir redes neuronales **desde cero** te da una base s√≥lida para entender **por qu√©** y **c√≥mo** funcionan internamente.  
- Es fundamental para construir conocimiento profundo y evitar ‚Äúcajas negras‚Äù.  
- Pero para **implementaciones pr√°cticas y profesionales**, PyTorch es una ventaja enorme: automatiza c√°lculos, optimizaci√≥n y reduce errores.  
- Adem√°s, ahorra much√≠simo tiempo, especialmente en proyectos reales con redes complejas.

---

# üîë **Conclusi√≥n:**  
El control manual es excelente para aprender y entender, pero la abstracci√≥n que ofrece PyTorch es clave para eficiencia, escalabilidad y confiabilidad en proyectos reales. Lo ideal es combinar ambos enfoques seg√∫n la etapa de aprendizaje o desarrollo en la que te encuentres.
