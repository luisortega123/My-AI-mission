# Diagn√≥stico y Control del Modelo: Overfitting y Regularizaci√≥n

## ¬øQu√© es el Overfitting (Sobreajuste)?

El **overfitting** ocurre cuando un modelo aprende *demasiado bien* los datos con los que fue entrenado. No solo aprende los **patrones generales**, sino tambi√©n las **particularidades, errores o ruido** de esos datos. Como consecuencia, **pierde capacidad para generalizar** a nuevos datos: **memoriza** en lugar de *entender*.

> üìå **Definici√≥n simple**: El modelo rinde bien en los datos de entrenamiento, pero falla con datos nuevos porque ha memorizado en lugar de aprender.

---

## Causas Comunes del Overfitting

| Causa                            | Explicaci√≥n                                                                                      | Ejemplo                                                                                          |
| -------------------------------- | ------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------ |
| **Modelo demasiado complejo**    | Tiene demasiados par√°metros o flexibilidad en relaci√≥n con la cantidad/simplicidad de los datos. | Ajustar un polinomio de grado 10 a 15 puntos que siguen una l√≠nea recta.                         |
| **Pocos datos de entrenamiento** | No hay suficiente informaci√≥n para aprender patrones generalizables. El modelo ajusta el ruido.  | Con solo 5 ejemplos, el modelo puede "pasar por todos los puntos", pero fallar con datos nuevos. |
| **Ruido en los datos**           | El modelo aprende errores o anomal√≠as como si fueran patrones reales.                            | Datos mal etiquetados o con errores que el modelo intenta memorizar.                             |
| **Entrenamiento excesivo**       | Aun si el modelo es adecuado, entrenarlo demasiado tiempo hace que memorice.                     | Despu√©s de muchas √©pocas, el modelo deja de aprender y empieza a copiar el entrenamiento.        |

---

## Sobre la cantidad de datos

* Si **tienes pocos datos** y un **modelo muy complejo**, este podr√≠a *ajustarse perfectamente* a esos pocos puntos.
* Pero eso no implica que **haya aprendido bien**.
* Al llegar nuevos datos, ese ajuste perfecto puede resultar **muy pobre**.

> üéØ **Conclusi√≥n**: Con pocos datos, un modelo complejo **no tiene suficiente evidencia** para distinguir entre **se√±al** (patr√≥n general) y **ruido** (casualidades del conjunto de entrenamiento).

---

## üß† Resumen de las causas del Overfitting

* Modelo demasiado complejo para los datos o la tarea.
* Conjunto de datos de entrenamiento muy peque√±o.
* Presencia excesiva de ruido en los datos.
* Entrenamiento durante demasiadas iteraciones (√©pocas).

---

## üß© Underfitting (Subajuste)

El **underfitting** ocurre cuando un modelo es **demasiado simple** para captar la complejidad real de los datos de entrenamiento. Como resultado:

* **No aprende bien** los patrones presentes.
* **Comete muchos errores**, incluso con los datos con los que fue entrenado.
* Falla en generalizar a nuevos datos porque **ni siquiera ha logrado aprender los datos originales**.

> üìå **Definici√≥n simple**: El modelo no est√° aprendiendo ni siquiera los patrones de entrenamiento, y por eso comete errores altos *en todo*.

---

## ¬øC√≥mo se ve el underfitting?

| Tipo de error                  | Resultado | ¬øPor qu√© ocurre?                                                                     |
| ------------------------------ | --------- | ------------------------------------------------------------------------------------ |
| **Error en entrenamiento**     | **Alto**  | El modelo no logra ajustarse a los patrones presentes en los datos.                  |
| **Error en prueba/validaci√≥n** | **Alto**  | Si no entendi√≥ los datos de entrenamiento, dif√≠cilmente podr√° entender datos nuevos. |

> ‚ùó El rendimiento es pobre de forma consistente, tanto en entrenamiento como en validaci√≥n.

---

## Causas Comunes del Underfitting

| Causa                                  | Explicaci√≥n                                                                                       | Ejemplo                                                           |
| -------------------------------------- | ------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **Modelo demasiado simple**            | Tiene pocos par√°metros o una estructura r√≠gida que no puede capturar la complejidad de los datos. | Usar una l√≠nea recta para datos que tienen una forma curva.       |
| **Datos de entrada poco informativos** | Las variables (features) no contienen suficiente informaci√≥n relevante.                           | Predecir precios de casas solo con el n√∫mero de ventanas.         |
| **Entrenamiento insuficiente**         | El modelo no tuvo suficiente tiempo o ciclos de entrenamiento para aprender los patrones.         | Cortar el entrenamiento antes de que el error baje lo suficiente. |

---

## üß† Resumen

* El underfitting es lo **opuesto** al overfitting.
* El modelo **no aprende bien** ni siquiera los datos de entrenamiento.
* Puede deberse a una arquitectura demasiado simple, mala calidad de datos o entrenamiento insuficiente.
* Los **errores ser√°n altos en todas las fases**: tanto en entrenamiento como en prueba.


# üìö Bias-Variance Tradeoff (Compromiso Sesgo-Varianza)



## üéØ ¬øQu√© es el Bias-Variance Tradeoff?

Es el equilibrio que buscamos entre dos fuentes de error en los modelos de Machine Learning:

* **Sesgo (Bias)**: Error por suposiciones demasiado simplistas.
* **Varianza (Variance)**: Error por sensibilidad excesiva a los datos de entrenamiento.
```
Nuestro objetivo es **minimizar el error total** que un modelo comete en datos que nunca ha visto antes.
```


## Manifestaci√≥n Pr√°ctica del Tradeoff Sesgo-Varianza

## üîç 1. Comparaci√≥n de Errores: ¬øSesgo o Varianza?

Cuando entrenas un modelo, puedes calcular:

* **Error de entrenamiento**: el error que comete el modelo sobre los datos con los que fue entrenado.
* **Error de validaci√≥n/prueba**: el error que comete sobre datos que nunca ha visto.

Comparar estos errores te permite diagnosticar lo siguiente:

### üî¥ A. Alto Sesgo (Underfitting)

* **Error de entrenamiento: Alto**
* **Error de validaci√≥n: Similarmente alto**
* **Diferencia entre ambos: Peque√±a**

üëâ Esto indica que el modelo es **demasiado simple** para capturar los patrones del problema. No aprende bien ni siquiera los datos de entrenamiento. Necesitas un modelo m√°s complejo.

### üîµ B. Alta Varianza (Overfitting)

* **Error de entrenamiento: Bajo**
* **Error de validaci√≥n: Alto**
* **Diferencia entre ambos: Grande**

üëâ Esto indica que el modelo se ha **ajustado demasiado** a los datos de entrenamiento (incluso al ruido). No generaliza bien. Puede requerir regularizaci√≥n, simplificaci√≥n, o m√°s datos.

### ‚úÖ C. Buen Equilibrio (Sweet Spot)

* **Error de entrenamiento: Bajo**
* **Error de validaci√≥n: Tambi√©n bajo (ligeramente m√°s alto)**
* **Diferencia: Peque√±a**

üëâ El modelo ha aprendido lo suficiente **sin sobreajustarse**. Est√° generalizando bien.

---

## üìà 2. Curvas de Aprendizaje T√≠picas

Las curvas de aprendizaje muestran c√≥mo cambian los errores a medida que:

* Aumentas el **tama√±o del conjunto de entrenamiento**, o
* Aumentas la **complejidad del modelo**.

Aqu√≠ c√≥mo se ven t√≠picamente en cada caso:

### üî¥ A. Alto Sesgo (Underfitting)

**Curvas vs tama√±o del entrenamiento:**

* Ambas curvas (entrenamiento y validaci√≥n) est√°n **altas** y cercanas entre s√≠.
* A medida que se agregan m√°s datos, **no hay gran mejora**.
* **El modelo no mejora con m√°s datos porque es demasiado simple.**

üìä Ejemplo gr√°fico (mental):

```
Error
  |
  |       --------------------   ‚Üê entrenamiento
  |       --------------------   ‚Üê validaci√≥n
  |
  +----------------------------> tama√±o de los datos
```

---

### üîµ B. Alta Varianza (Overfitting)

**Curvas vs tama√±o del entrenamiento:**

* **Error de entrenamiento muy bajo**
* **Error de validaci√≥n mucho m√°s alto**
* Hay una **gran brecha (gap)** entre ambos.
* A medida que se a√±aden m√°s datos, la brecha puede **disminuir**.

üìä Ejemplo gr√°fico:

```
Error
  |
  |  \                         ‚Üê entrenamiento (muy bajo)
  |    \     ________          ‚Üê validaci√≥n (alto, pero bajando con m√°s datos)
  |
  +---------------------------> tama√±o de los datos
```

---

### ‚úÖ C. Buen Equilibrio (Sweet Spot)

**Curvas vs tama√±o del entrenamiento:**

* El error de entrenamiento es bajo, pero no perfecto.
* El error de validaci√≥n es un poco m√°s alto, pero **ambos convergen**.
* Es se√±al de que el modelo est√° generalizando bien.

üìä Ejemplo gr√°fico:

```
Error
  |
  |   \         
  |    \______                     ‚Üê entrenamiento
  |           \_______            ‚Üê validaci√≥n
  |
  +-----------------------------> tama√±o de los datos
```

---



# Estrategias Generales para Combatir el Underfitting y el Overfitting

En el entrenamiento de modelos de machine learning, uno de los principales desaf√≠os es encontrar el equilibrio entre **subajuste (underfitting)** y **sobreajuste (overfitting)**. A continuaci√≥n, se presentan estrategias pr√°cticas y razonadas para abordar cada caso.

---

## ¬øC√≥mo combatir el UNDERFITTING?

El underfitting ocurre cuando un modelo es demasiado simple para capturar los patrones subyacentes de los datos. Algunas estrategias efectivas incluyen:

### 1. Aumentar la complejidad del modelo

* **Elegir un modelo m√°s expresivo**:

  * Si usas regresi√≥n lineal, prueba con regresi√≥n polin√≥mica (a√±adiendo t√©rminos como x¬≤, x¬≥, etc.).
    üëâ *¬øQu√© hiperpar√°metro controlar√≠as aqu√≠?* El grado del polinomio.
  * Si usas √°rboles de decisi√≥n, permite que crezcan m√°s profundos.
  * Considera modelos m√°s complejos como redes neuronales o SVM con kernel no lineal.

### 2. Ingenier√≠a de caracter√≠sticas (Feature Engineering)

* Agrega nuevas caracter√≠sticas relevantes.
* Introduce combinaciones de variables (interacciones).
* Aseg√∫rate de incluir representaciones adecuadas del dominio del problema.

### 3. Asegurar entrenamiento suficiente

* Aumenta el n√∫mero de √©pocas o iteraciones.
* Verifica que el algoritmo haya tenido oportunidad de converger.

### 4. Ajustar la regularizaci√≥n

* Si est√°s aplicando regularizaci√≥n (por ejemplo, con par√°metro Œª), revisa que **no sea excesiva**.
  Un Œª demasiado alto puede hacer que el modelo sea demasiado simple.
  üëâ *Reducir Œª puede permitirle aprender m√°s patrones reales.*

---

## ¬øC√≥mo combatir el OVERFITTING?

El overfitting ocurre cuando el modelo aprende demasiado bien los datos de entrenamiento, incluyendo el ruido o las particularidades del conjunto, y falla al generalizar. Estas estrategias ayudan a evitarlo:

### 1. Regularizaci√≥n

* Penaliza los valores grandes de los par√°metros del modelo (Œ∏) para evitar que se ajusten demasiado a los datos.

  * **L1 (Lasso)**: puede llevar a modelos m√°s escuetos (sparse).
  * **L2 (Ridge)**: reduce gradualmente todos los pesos.

  üëâ *¬øC√≥mo ayuda esto?* Reduce la complejidad efectiva del modelo sin cambiar su estructura base.

### 2. Selecci√≥n de caracter√≠sticas o reducci√≥n de dimensionalidad

* Elimina variables irrelevantes o ruidosas.
* Aplica t√©cnicas como **PCA (An√°lisis de Componentes Principales)** o m√©todos de selecci√≥n automatizada para reducir la dimensionalidad.

### 3. Early Stopping (Detenci√≥n Temprana)

* Monitorea el error en el conjunto de validaci√≥n durante el entrenamiento.
* Si el error de validaci√≥n comienza a aumentar mientras el error de entrenamiento sigue bajando, det√©n el entrenamiento.
  üëâ *Esto previene que el modelo se "memorice" los datos.*

### 4. M√°s datos de entrenamiento

* Cuantos m√°s ejemplos diversos tengas, m√°s robusto ser√° el modelo.
* Ayuda a reducir el sesgo inducido por un conjunto peque√±o o no representativo.

### 5. Filtrar o limpiar datos (con cuidado)

* Identifica y elimina outliers si est√°n claramente afectando el modelo.
  ‚ö†Ô∏è *Hazlo solo si puedes justificarlo bien*, ya que podr√≠as introducir sesgo si te excedes.

### 6. M√©todos de ensamblaje (Ensemble Methods)

* Combina m√∫ltiples modelos para obtener predicciones m√°s estables y precisas:

  * **Bagging** (como Random Forests) reduce la varianza.
  * **Boosting** (como XGBoost) puede mejorar el sesgo y la varianza a la vez.

---


### Comparaci√≥n de Estrategias contra Underfitting y Overfitting

| Categor√≠a                                | Combatir Underfitting                                                               | Combatir Overfitting                                                |
| ---------------------------------------- | ----------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| **Complejidad del Modelo**               | Aumentar complejidad (p. ej., redes m√°s grandes, polinomios, √°rboles m√°s profundos) | Reducir complejidad (modelos m√°s simples, limitar profundidad)      |
| **Caracter√≠sticas**                      | A√±adir o transformar caracter√≠sticas relevantes                                     | Eliminar caracter√≠sticas irrelevantes o ruidosas                    |
| **Entrenamiento**                        | Aumentar n√∫mero de iteraciones/√©pocas                                               | Early stopping (detener cuando se sobreajusta al set de validaci√≥n) |
| **Regularizaci√≥n**                       | Reducir regularizaci√≥n (bajar Œª)                                                    | A√±adir o aumentar regularizaci√≥n L1 / L2                            |
| **Datos**                                | No suele ser la primera opci√≥n, pero ayuda                                          | A√±adir m√°s datos de entrenamiento                                   |
| **Dimensionalidad**                      | No aplica directamente                                                              | Reducci√≥n de dimensionalidad (PCA, selecci√≥n de caracter√≠sticas)    |
| **T√©cnicas avanzadas**                   | ‚Äî                                                                                   | T√©cnicas de ensamblaje (Bagging, Boosting)                          |
| **Aumento de Datos (Data Augmentation)** | ‚Äî                                                                                   | √ötil para generalizar mejor (im√°genes, texto, audio)                |



## üß† Tipos de Bias en Machine Learning

| Concepto                 | ¬øQu√© es?                                  | ¬øD√≥nde aparece?        |
| ------------------------ | ----------------------------------------- | ---------------------- |
| **Bias como intercepto** | Columna de unos ‚Üí par√°metro Œ≤‚ÇÄ            | Modelos lineales, RN   |
| **Bias como sesgo**      | Suposiciones err√≥neas ‚Üí error sistem√°tico | Bias-Variance Tradeoff |

---

## üîç 1. **Bias como Par√°metro (Intercepto)**

* Se refiere al t√©rmino independiente en modelos lineales:

  Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + Œ≤‚ÇÇX‚ÇÇ + ‚Ä¶

* Es un **par√°metro aprendido** por el modelo.

* Se introduce agregando una **columna de unos** a la matriz de entrada X.

---

## üîç 2. **Bias como Error Sistem√°tico**

* Error causado por suposiciones r√≠gidas (por ejemplo, que todo es lineal).
* Se define como la **diferencia entre la predicci√≥n promedio del modelo y la realidad**.
* Es una **medida de error te√≥rico**, no un par√°metro.

---

## üìà ¬øQu√© ocurre con Sesgo y Varianza?

| Tipo de Modelo           | Sesgo | Varianza | Resultado             |
| ------------------------ | ----- | -------- | --------------------- |
| Muy simple               | Alto  | Bajo     | Underfitting          |
| Muy complejo             | Bajo  | Alto     | Overfitting           |
| Equilibrado (sweet spot) | Medio | Medio    | Generalizaci√≥n √≥ptima |

---

## üìâ Error Total

El error total en un modelo puede expresarse como:

**Error total = Sesgo¬≤ + Varianza + Error irreducible**

* **Sesgo¬≤**: Error por suposiciones err√≥neas (underfitting).
* **Varianza**: Error por sobreajuste al conjunto de entrenamiento (overfitting).
* **Error irreducible**: Ruido inherente al problema. No se puede eliminar.

---

## ‚öñÔ∏è El Compromiso

* Reducir **sesgo** suele **aumentar varianza**.
* Reducir **varianza** suele **aumentar sesgo**.
* El punto √≥ptimo (üí° *sweet spot*) es donde el error total es m√≠nimo y el modelo **generaliza bien**.

---

## üõ†Ô∏è ¬øC√≥mo controlar la complejidad?

T√∫ eliges la complejidad del modelo con las siguientes "perillas":

* **Tipo de modelo**: lineal vs red neuronal, √°rbol de decisi√≥n, etc.
* **Hiperpar√°metros**:

  * Grado del polinomio
  * Profundidad del √°rbol
  * Capas y neuronas en redes
* **Regularizaci√≥n**: penaliza la complejidad (controla el overfitting).

---

## üìä ¬øC√≥mo encontrar el sweet spot?

1. **Divisi√≥n de Datos**:

   * Entrenamiento: aprende los par√°metros.
   * Validaci√≥n: elige el mejor modelo/hiperpar√°metro.
   * Prueba: eval√∫a el modelo final.

2. **Curvas de Aprendizaje**:

   * Gr√°fica de error de entrenamiento y validaci√≥n al aumentar la complejidad.
   * El sweet spot suele estar donde el error de validaci√≥n es m√≠nimo.

3. **Validaci√≥n Cruzada (Cross-Validation)**:

   * Eval√∫a el rendimiento de forma m√°s robusta.
   * Recomendado para seleccionar hiperpar√°metros con mayor confianza.

---

## üß© Conclusi√≥n

* El **bias-variance tradeoff** es uno de los conceptos m√°s fundamentales para entender por qu√© un modelo no est√° funcionando bien.
* **No hay una f√≥rmula m√°gica** para saber cu√°nta complejidad es ideal: lo descubrimos **experimentando** y validando.
* Tu tarea como modelador es ajustar esa complejidad para que el modelo **aprenda lo suficiente pero no memorice**.


---


## Regularizaci√≥n: Previniendo el Sobreajuste sin Perder Capacidad de Aprendizaje

### Formula

$$\text{T√©rmino de Regularizaci√≥n L2} = \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2$$

### ¬øQu√© es la Regularizaci√≥n?

La **regularizaci√≥n** es una t√©cnica fundamental que modifica la funci√≥n de coste de un modelo para reducir su complejidad y evitar el sobreajuste (*overfitting*). Lo hace penalizando los **par√°metros grandes** del modelo (Œ∏), lo que tiende a producir modelos m√°s simples y generalizables.

---

### ¬øPor qu√© se necesita?

* Queremos minimizar el **error de predicci√≥n** en los datos de entrenamiento.
* Pero si el modelo es demasiado complejo (por ejemplo, tiene par√°metros Œ∏ muy grandes), puede **memorizar los datos** en lugar de aprender patrones generales.
* Esto causa **sobreajuste**, es decir, bajo error en entrenamiento pero alto error en datos nuevos.

La regularizaci√≥n combate esto a√±adiendo una **penalizaci√≥n por complejidad** directamente en la funci√≥n de coste.

---

### ¬øC√≥mo se modifica la funci√≥n de coste?

Tomemos como ejemplo la **Regresi√≥n Lineal Regularizada con L2** (tambi√©n conocida como *Ridge Regression*).

La nueva funci√≥n de coste se define como:

$$J(Œ∏) = (1 / 2m) ‚àë*{i=1}^{m} (h\_Œ∏(x^{(i)}) - y^{(i)})¬≤ + (Œª / 2m) ‚àë*{j=1}^{n} Œ∏\_j¬≤$$

Donde:

* m es el n√∫mero de ejemplos.
* h\_Œ∏(x) es la predicci√≥n del modelo.
* y^{(i)} es el valor real para el ejemplo i.
* Œª ‚â• 0 es el **coeficiente de regularizaci√≥n**.
* La suma en el segundo t√©rmino excluye generalmente Œ∏‚ÇÄ (el sesgo/intercepto), ya que no suele penalizarse.

> Esto no es mas que la suma de nuestra funcion de coste mas la funcion de regularizacion
---

### ¬øC√≥mo afecta al Descenso de Gradiente?

La actualizaci√≥n de los par√°metros tambi√©n se modifica para incluir la penalizaci√≥n. Si antes actualiz√°bamos as√≠:


$$
\theta_j := \theta_j - \alpha \cdot \frac{\partial J}{\partial \theta_j}
$$

Con regularizaci√≥n L2, el gradiente se ajusta as√≠:


$$
\theta_j := \theta_j - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m} \theta_j \right]
$$


Esto significa que cada Œ∏\_j es "empujado" ligeramente hacia cero en cada paso, evitando que crezca demasiado.

> Nota: La penalizaci√≥n **no aplica a Œ∏‚ÇÄ**, as√≠ que su actualizaci√≥n se mantiene igual que antes.

---

### Beneficios de la Regularizaci√≥n

* **Reduce el riesgo de overfitting**, haciendo que el modelo generalice mejor.
* **Controla la complejidad** del modelo sin cambiar su arquitectura.
* **F√°cil de implementar**, ya que solo requiere ajustar la funci√≥n de coste y el gradiente.

### Que pasa si lmbda es 0?
Si `lmbda_reg = 0`, entonces el **t√©rmino de regularizaci√≥n** se anula completamente:

$$
\text{t√©rmino\_de\_regularizaci√≥n} = \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2 = 0
$$

Por tanto:

$$
\text{coste\_total} = \text{coste\_original} + 0 = \text{coste\_original}
$$

### ¬øQu√© significa esto?
Si el par√°metro de regularizaci√≥n Œª = 0, la funci√≥n de coste regularizada se convierte exactamente en la funci√≥n de coste original, sin regularizaci√≥n. Esto se debe a que el t√©rmino de penalizaci√≥n (por ejemplo, en regularizaci√≥n L2, la suma de los cuadrados de los par√°metros) se multiplica por Œª:

---
# Pasos de Modificacion del algoritmo


## ‚öôÔ∏è Modificaci√≥n del Modelo de Regresi√≥n Lineal con Regularizaci√≥n L2

### üìå Paso 1: Modificar la Funci√≥n de Coste

Comenzamos con la **funci√≥n de coste est√°ndar** utilizada en regresi√≥n lineal, conocida como **Error Cuadr√°tico Medio (MSE)**. Esta funci√≥n mide qu√© tan lejos est√°n las predicciones del modelo respecto a los valores reales:

### üìâ Funci√≥n de Coste Original (MSE)

$$
J(Œ∏) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_Œ∏(x^{(i)}) - y^{(i)} \right)^2
$$

Aqu√≠:

* $m$ es el n√∫mero de ejemplos de entrenamiento.
* $h_Œ∏(x^{(i)})$ es la predicci√≥n del modelo para el ejemplo $i$.
* $y^{(i)}$ es el valor real para ese ejemplo.
* $Œ∏$ representa los par√°metros del modelo (tambi√©n llamados coeficientes o pesos).

---

### üõ°Ô∏è Agregando Regularizaci√≥n L2 (Ridge)

Para evitar que los coeficientes del modelo se vuelvan demasiado grandes (lo que podr√≠a llevar a **sobreajuste**), agregamos un t√©rmino de **penalizaci√≥n** que castiga los valores grandes de $Œ∏$, excepto el t√©rmino de sesgo $Œ∏_0$.

### üßÆ Funci√≥n de Coste con Regularizaci√≥n L2

$$
J(Œ∏) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_Œ∏(x^{(i)}) - y^{(i)} \right)^2 + \frac{Œª}{2m} \sum_{j=1}^{n} Œ∏_j^2
$$

#### Donde:

* $Œª$ (lambda) es el **par√°metro de regularizaci√≥n**. Controla cu√°nto penalizamos los coeficientes grandes.
* El segundo t√©rmino **no incluye** a $Œ∏_0$, ya que este representa el sesgo base del modelo y **no deber√≠a penalizarse**.
* La suma de la regularizaci√≥n va desde $j = 1$ hasta $n$, dejando fuera $j = 0$.

---

üîç **¬øPor qu√© no penalizamos $Œ∏_0$?**

Porque $Œ∏_0$ act√∫a como el punto de partida del modelo (es el valor que predice cuando todas las variables son cero). Penalizarlo no contribuye a controlar la complejidad del modelo y podr√≠a llevar a un ajuste sub√≥ptimo.


---

## üéØ ¬øPor qu√© no se penaliza Œ∏‚ÇÄ en la regularizaci√≥n L2?

Cuando aplicamos **regularizaci√≥n L2 (Ridge)** en un modelo de regresi√≥n lineal, es importante entender **por qu√© el t√©rmino de intercepci√≥n $Œ∏_0$** ‚Äîtambi√©n conocido como el **sesgo o bias**‚Äî **no se incluye en la penalizaci√≥n**.

### üß† ¬øQu√© es Œ∏‚ÇÄ?

* $Œ∏_0$ es el valor que predice el modelo **cuando todas las variables de entrada son cero**.
* Es como el punto de partida o el "ajuste base" del modelo.

---

### üß™ ¬øC√≥mo se implementa esto?

En la **funci√≥n de coste**, solo se penalizan los coeficientes que est√°n asociados a las **caracter√≠sticas de entrada**, es decir, desde $Œ∏_1$ hasta $Œ∏_n$:

$$
J(Œ∏) = \frac{1}{2m} \sum_{i=1}^{m} \left(h_Œ∏(x^{(i)}) - y^{(i)}\right)^2 + \frac{Œª}{2m} \sum_{j=1}^{n} Œ∏_j^2
$$

üëâ Observa que la **suma del segundo t√©rmino empieza en $j = 1$**, **no en 0**. Por eso, $Œ∏_0$ queda **fuera de la penalizaci√≥n**.

---

### üîÑ ¬øY en la actualizaci√≥n del gradiente?

Al actualizar los par√°metros durante el entrenamiento (por ejemplo, con gradiente descendente), el **t√©rmino de penalizaci√≥n**:

$$
\frac{Œª}{m}Œ∏_j
$$

...**tambi√©n se aplica solamente a $Œ∏_1, Œ∏_2, ..., Œ∏_n$**.
Esto se refleja en el c√≥digo, donde usualmente se escribe algo como `theta[1:] += ...`.

---

### ‚öñÔ∏è ¬øPor qu√© no penalizamos $Œ∏_0$?

* ‚úÖ **Porque no representa una variable de entrada**, sino un desplazamiento general en todas las predicciones.
* ‚úÖ Penalizarlo **no ayuda a prevenir el sobreajuste** causado por variables complejas o irrelevantes.
* ‚ùå De hecho, podr√≠a llevar a un **modelo sub√≥ptimo**, limitando la capacidad del modelo para ajustarse correctamente a los datos.

---

### üßµ En resumen:

| Elemento             | ¬øSe penaliza en L2? | ¬øPor qu√©?                                                             |
| -------------------- | ------------------- | --------------------------------------------------------------------- |
| $Œ∏_0$ (intercepci√≥n) | ‚ùå No                | No es una caracter√≠stica; penalizarlo puede da√±ar el ajuste base.     |
| $Œ∏_j$, $j ‚â• 1$       | ‚úÖ S√≠                | Representan caracter√≠sticas del modelo que pueden causar sobreajuste. |

---

‚úÖ **El objetivo principal de la regularizaci√≥n L2 es controlar la complejidad del modelo penalizando solo los coeficientes asociados a las variables de entrada**, no al sesgo general.

---

## **Paso 2: Modificar la Funci√≥n de Descenso de Gradiente (con Regularizaci√≥n L2)**

### **Paso A: Preparar el vector para la penalizaci√≥n**

La regularizaci√≥n L2 a√±ade una **penalizaci√≥n a los valores grandes de los par√°metros** para evitar que el modelo sobreajuste.
Sin embargo, **no debemos penalizar el par√°metro Œ∏‚ÇÄ** (el t√©rmino independiente o sesgo), ya que no est√° asociado a ninguna caracter√≠stica y su penalizaci√≥n podr√≠a afectar negativamente el entrenamiento.

Por eso, vamos a crear una **copia del vector `Œ∏` (theta)**, pero con el primer valor igual a cero.

En c√≥digo, esto ser√≠a:

```python
theta_para_penalizacion = theta.copy()
theta_para_penalizacion[0] = 0
```

Esto da como resultado un nuevo vector:

$$
\theta_{\text{penalizaci√≥n}} = 
\begin{bmatrix}
0 \\
\theta_1 \\
\theta_2 \\
\vdots \\
\theta_n
\end{bmatrix}
$$

### **Paso B: Calcular la penalizaci√≥n para el gradiente**

Ahora, vamos a calcular el **vector de penalizaci√≥n** que sumaremos al gradiente original.
Este vector se obtiene multiplicando cada elemento de `Œ∏_para_penalizacion` por un escalar que incluye el **par√°metro de regularizaci√≥n** Œª y el n√∫mero de ejemplos m:

$$
\text{penalizaci√≥n\_gradiente} = \frac{\lambda}{m} \cdot \theta_{\text{penalizaci√≥n}}
$$

Este vector tiene el mismo tama√±o que `Œ∏` y **solo penaliza los par√°metros distintos de Œ∏‚ÇÄ**.

---

## üêõ Proceso de Depuraci√≥n Completo: Problemas al Ejecutar el Modelo con Distintos Lambdas

### Contexto

Est√°bamos probando nuestro modelo de regresi√≥n lineal regularizada con distintos valores de `lambda`, con el siguiente bloque:

```python
for lmbda_reg in lmbda_valores:
    theta_calculado, historial_coste = descenso_gradiente(
        X_bias_scaled, y, theta_inicial.copy(), alpha, num_iteraciones, lmbda_reg
    )
```

El objetivo era observar c√≥mo cambiaban los par√°metros `theta` y la funci√≥n de coste con distintos grados de regularizaci√≥n. Pero **empezaron a ocurrir problemas graves**:

* La m√°quina se volv√≠a extremadamente lenta.
* El script nunca terminaba de ejecutarse.
* No hab√≠a errores expl√≠citos visibles al principio.

---

### üß© Etapa 1: TypeError en `num_iteraciones`

Error observado:

```
TypeError: 'float' object cannot be interpreted as an integer
```

Esto ocurri√≥ en la funci√≥n `descenso_gradiente`, en esta l√≠nea:

```python
for i in range(num_iteraciones):
```

**Hip√≥tesis inicial**: `num_iteraciones` no estaba llegando como entero, sino como `float`.

---

#### Paso 1.1: Verificar `num_iteraciones` ANTES del bucle

Agregamos este print en el script principal:

```python
print(f"DEBUG SCRIPT: Antes del bucle de lambdas, num_iteraciones = {num_iteraciones}, tipo = {type(num_iteraciones)}")
```

**Salida esperada** (si estuviera bien):

```
DEBUG SCRIPT: Antes del bucle de lambdas, num_iteraciones = 200, tipo = <class 'int'>
```

Pero eso estaba bien, as√≠ que fuimos m√°s adentro.

---

#### Paso 1.2: Verificar `num_iteraciones` DENTRO de la funci√≥n

Agregamos este print al inicio de `descenso_gradiente`:

```python
print(f"DEBUG GD: Al inicio de la funci√≥n, num_iteraciones = {num_iteraciones}, tipo = {type(num_iteraciones)}")
```

**Salida obtenida**:

```
DEBUG GD: Al inicio de la funci√≥n, num_iteraciones = 0.001, tipo = <class 'float'>
```

**Descubrimiento**: Est√°bamos pasando mal los argumentos en la llamada. Lo que estaba llegando como `num_iteraciones` en realidad era `alpha`.

---

#### ‚úîÔ∏è Soluci√≥n 1: Corregir el orden de los argumentos

La llamada correcta deb√≠a ser:

```python
theta_calculado, historial_coste = descenso_gradiente(
    X_bias_scaled, y, theta_inicial.copy(), alpha_real, num_iteraciones_real, lmbda_reg
)
```

Con esto, el error de tipo desapareci√≥.

---

### üß© Etapa 2: Explosi√≥n de Formas y Lentitud

Aunque el script ya no tiraba error, ahora ten√≠a s√≠ntomas distintos:

* El modelo se volv√≠a extremadamente lento.
* `theta` ten√≠a formas gigantes.
* `errores` explotaba en tama√±o.

---

#### Paso 2.1: Verificar formas en cada iteraci√≥n

Agregamos estos prints dentro de `descenso_gradiente`, despu√©s de calcular `predicciones` y `errores`:

```python
print(f"Iter {i+1} DEBUG GD: theta.shape = {theta.shape}")
print(f"Iter {i+1} DEBUG GD: predicciones.shape = {predicciones.shape}")
print(f"Iter {i+1} DEBUG GD: errores.shape = {errores.shape}")
```

**Salida obtenida (ejemplo con lambda=0):**

```
Iter 1 DEBUG GD: theta.shape = (9, 20640)
Iter 1 DEBUG GD: predicciones.shape = (20640, 1)
Iter 1 DEBUG GD: errores.shape = (20640, 20640)
```

**Algo estaba muy mal.**

---

#### Paso 2.2: Diagn√≥stico m√°s fino del error de dimensiones

Agregamos m√°s prints para analizar justo antes y despu√©s de calcular `errores`:

```python
predicciones = calcular_hipotesis(X_bias, theta)
print(f"Iter {i+1} DEBUG GD ANTES DE ERRORES: theta.shape={theta.shape}, predicciones.shape={predicciones.shape}, y.shape={y.shape}")
errores = predicciones - y
print(f"Iter {i+1} DEBUG GD DESPU√âS DE ERRORES: errores.shape={errores.shape}")
```

**Salida clave**:

```
Iter 1 DEBUG GD ANTES DE ERRORES: theta.shape=(9,1), predicciones.shape=(20640,1), y.shape=(20640,)
Iter 1 DEBUG GD DESPU√âS DE ERRORES: errores.shape=(20640,20640)
```

**Descubrimiento**: `y` ten√≠a forma `(20640,)` (vector de 1 dimensi√≥n), mientras que `predicciones` era `(20640,1)`. Python hizo *broadcasting* para hacer compatible la resta, creando una matriz de tama√±o `(20640,20640)`.

---

#### ‚úîÔ∏è Soluci√≥n 2: Forzar forma correcta de `y`

Al inicio de `descenso_gradiente`, agregamos:

```python
y = y.reshape(-1, 1)
```

**Resultado** tras el fix:

```
Iter 1 DEBUG GD ANTES DE ERRORES: theta.shape=(9,1), predicciones.shape=(20640,1), y.shape=(20640,1)
Iter 1 DEBUG GD DESPU√âS DE ERRORES: errores.shape=(20640,1)
```

‚úÖ Ahora todas las formas se manten√≠an correctas. La lentitud extrema desapareci√≥ y el entrenamiento se comport√≥ como se esperaba.

---

#### Paso 2.3 (Opcional): Verificar explosiones num√©ricas

A√∫n con formas correctas, se puede tener lentitud por problemas num√©ricos. Verificamos esto dentro de `calcular_coste`:

```python
print(f"CALC_COSTE: errores_cuadraticos.shape = {errores_cuadraticos.shape}")
print(f"CALC_COSTE: Primeros 3 errores_cuadraticos: {errores_cuadraticos[:3].T}")
print(f"CALC_COSTE: Hay NaNs? {np.isnan(errores_cuadraticos).any()}, Hay Infs? {np.isinf(errores_cuadraticos).any()}")
```

**Posible salida si hubiera overflows**:

```
CALC_COSTE: errores_cuadraticos.shape = (20640, 1)
CALC_COSTE: Primeros 3 errores_cuadraticos: [[inf inf inf]]
CALC_COSTE: Hay NaNs? False, Hay Infs? True
```

Pero en nuestro caso, tras corregir la forma de `y`, **no hubo problemas num√©ricos**.

---

### ‚úÖ Conclusi√≥n

Gracias a un proceso met√≥dico de **debugging con prints**, descubrimos dos errores graves:

1. Par√°metros mal pasados (`alpha` y `num_iteraciones` estaban invertidos).
2. Forma de `y` incorrecta, causando explosiones de matrices y lentitud.

Estas correcciones fueron **cr√≠ticas para el funcionamiento correcto del modelo**, y para que pudi√©ramos hacer las pruebas con m√∫ltiples valores de `lambda`.


## ‚úÖ CheckList de Buenas Pr√°cticas para Debugging y Modelado

### üìå Variables y Par√°metros

* [ ] Confirmar que `alpha`, `num_iteraciones`, `lambda` est√°n en el **orden correcto** al llamar funciones.
* [ ] Asegurarse de que `num_iteraciones` sea un `int`, no un `float`.

### üìè Formas de las Matrices

* [ ] Convertir `y` a forma `(m, 1)` antes de operaciones vectorizadas:

  ```python
  y = y.reshape(-1, 1)
  ```
* [ ] Verificar que `theta` tenga forma `(n, 1)` si `X` es `(m, n)`.

### üîç Verificaciones Intermedias (Debugging)

* [ ] Agregar prints de forma en puntos clave:

  ```python
  print(f"theta.shape = {theta.shape}")
  print(f"predicciones.shape = {predicciones.shape}")
  print(f"errores.shape = {errores.shape}")
  ```
* [ ] Usar prints con `.any()` para detectar `NaN` o `Inf`:

  ```python
  print(np.isnan(matriz).any(), np.isinf(matriz).any())
  ```

### üß™ Testeo Controlado

* [ ] Probar primero con un n√∫mero peque√±o de iteraciones (`num_iteraciones = 5 o 10`) y `lambda = 0` para validar la l√≥gica antes de entrenar completamente.

### ‚ö†Ô∏è Se√±ales de Error Com√∫n

| S√≠ntoma                         | Posible Causa                             |
| ------------------------------- | ----------------------------------------- |
| TypeError con `range()`         | `num_iteraciones` es `float`              |
| Errores gigantes `(m, m)`       | `y` tiene forma `(m,)` ‚Üí usar `.reshape`  |
| `theta` con forma rara `(n, m)` | Broadcasting incorrecto o errores previos |
| Script extremadamente lento     | Matrices gigantes por formas incorrectas  |
| `coste` devuelve `inf` o `nan`  | Overflow ‚Üí revisar `alpha` o escalado     |




## üìä **Resultados de la Experimentaci√≥n con Regularizaci√≥n L2 (Œª)**

### ¬øC√≥mo se comport√≥ Œ∏ 0(el intercepto) a medida que Œª cambiaba?

Realizamos experimentos utilizando un conjunto de datos de precios de casas en California para analizar c√≥mo afecta la regularizaci√≥n L2 en un modelo de regresi√≥n lineal.

### Regresion Lineal:
![alt text](<Regresion_Lineal.py/Coeficientes theta.png>)
### Regresion Logistica:
![alt text](<Regresion_Logisitica/Coeficientes ttheta en funcion del lmbda.png>) 




### üß™ **¬øQu√© me dice esto?**

En este an√°lisis, **no aplicamos penalizaci√≥n a Œ∏‚ÇÄ** (el t√©rmino que corresponde al valor inicial). Esto se debe a que cuando **Œª = 0**, no se agrega ninguna restricci√≥n, lo que significa que no se penaliza este t√©rmino.

Observamos en el gr√°fico c√≥mo los coeficientes (los valores multiplicados por las variables) cambian cuando **Œª** var√≠a. A medida que **Œª** aumenta, **los coeficientes tienden a hacerse m√°s peque√±os**, lo que significa que estamos "empujando" los coeficientes hacia **0**.

### üîé **¬øPor qu√© sucede esto?**

Esto ocurre por la **regularizaci√≥n L2**. Cuando aumentamos **Œª**:

* El modelo se hace m√°s simple, ya que reduce los valores de los coeficientes.
* Si **Œª** es grande, el modelo no confiar√° tanto en cada variable, evitando que alguna variable sea demasiado importante. Esto ayuda a **evitar el sobreajuste** (cuando el modelo "se ajusta demasiado" a los datos de entrenamiento).

### üìè **Funci√≥n de Coste con Regularizaci√≥n L2**

La funci√≥n de coste de la regresi√≥n lineal con regularizaci√≥n L2, tambi√©n conocida como **Ridge Regression**, es la siguiente:

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
$$

Donde:

* **J(Œ∏)** es la funci√≥n de coste, que mide qu√© tan bien se ajusta el modelo.
* **h‚Çú‚Çï‚Çê(x·µ¢)** es la predicci√≥n del modelo para los datos de entrada.
* **y·µ¢** es el valor real que queremos predecir.
* **Œª** es el par√°metro de regularizaci√≥n, que controla cu√°nto penalizamos a los coeficientes.
* **Œ∏‚±º** son los coeficientes de las caracter√≠sticas.
* **m** es el n√∫mero de ejemplos de entrenamiento.

### ‚öñÔ∏è **Resumen:**

* **Cuando Œª = 0:** No hay penalizaci√≥n, lo que puede hacer que el modelo se ajuste demasiado a los datos, llevando a un **sobreajuste**.

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^2
$$

* **Cuando Œª es alto:** Los coeficientes se hacen m√°s peque√±os, favoreciendo un modelo m√°s simple, lo que ayuda a evitar el **sobreajuste**(overfitting) y mejora la generalizaci√≥n del modelo.

---


### Regresion Lineal:
![alt text](<Regresion_Lineal.py/Curvas de coste lmbda.png>)
### Regresion Logistica:
![alt text](<Regresion_Logisitica/Curvas coste de lmbda.png>)

---

## üìâ An√°lisis del Gr√°fico: *Curvas de Coste por Lambda*

Este gr√°fico nos muestra c√≥mo el par√°metro \$\lambda\$ (lambda, de regularizaci√≥n) afecta el **aprendizaje** de nuestro modelo de regresi√≥n lineal.

### 1. **Verificaci√≥n de la Convergencia**

Cada l√≠nea en el gr√°fico representa c√≥mo cambia el coste durante el entrenamiento, para un valor distinto de \$\lambda\$. Lo que buscamos es que el coste:

* Disminuya progresivamente.
* Se estabilice (indica que el modelo ha "convergido").

> ‚ö†Ô∏è **Nota**: Si el coste *no* baja o se comporta de forma rara para cierto \$\lambda\$, podr√≠a indicar que esa configuraci√≥n no est√° funcionando bien. Tal vez \$\lambda\$ es demasiado alto o interact√∫a mal con el valor de `alpha`.

---

### 2. **Comparaci√≥n del Error Final con Diferentes Valores de \$\lambda\$**

* **Cuando \$\lambda = 0\$ (sin regularizaci√≥n):**
  El modelo tiene total libertad para ajustarse a los datos de entrenamiento. Por eso, el coste final es usualmente **m√°s bajo**: el modelo "memoriza" los datos.

* **Cuando \$\lambda > 0\$ (con regularizaci√≥n):**
  A medida que aumentamos \$\lambda\$, el modelo debe **equilibrar dos objetivos**:

  1. Minimizar el error de predicci√≥n.
  2. Mantener los valores de los par√°metros \$\theta\_j\$ **peque√±os** (evitar que crezcan mucho).

  Esto suele llevar a un **coste de entrenamiento m√°s alto**, pero tambi√©n reduce el riesgo de *overfitting* (sobreajuste).

---

### ‚úÖ **Resumen del gr√°fico:**

* Verifica si el modelo est√° entrenando correctamente (convergencia).
* Muestra c√≥mo el modelo reacciona ante distintos niveles de regularizaci√≥n.
* Ayuda a detectar si un \$\lambda\$ **demasiado grande** est√° haciendo que el modelo sea **demasiado simple** (lo que llamamos *underfitting*).

---

## üéØ ¬øQu√© pasa con los coeficientes \$\theta\$ cuando usamos regularizaci√≥n?

### ¬øAlgunos coeficientes se reducen a cero o cerca de cero m√°s r√°pido que otros?

‚úÖ **S√≠**, cuando aumentamos \$\lambda\$, algunos coeficientes \$\theta\_j\$ se acercan a cero m√°s r√°pido que otros.

Esto pasa porque con **regularizaci√≥n fuerte**, el modelo trata de **penalizar m√°s** a ciertos coeficientes. Si ve que una variable no est√° aportando mucho, la "castiga" y empuja su \$\theta\$ hacia cero.

```
‚ùó Si un coeficiente se hace peque√±o o casi cero con una lambda alta, el modelo cree que esa variable no es tan importante para hacer predicciones.
```

---

### üîç ¬øQu√© implica esto?

* El modelo est√° buscando **simplicidad**: usar solo las variables que realmente ayudan.
* Si un \$\theta\$ baja r√°pido, es porque el modelo **conf√≠a menos** en esa variable.
* Es como una **selecci√≥n autom√°tica de caracter√≠sticas**: las menos √∫tiles se "apagan" solas.

---

### üî¨ Conexi√≥n con la teor√≠a

* En la regularizaci√≥n L2 (*Ridge*), el coste total incluye un t√©rmino adicional que penaliza los \$\theta\_j\$ grandes:

  $$
  J(\theta) = \text{Error cuadr√°tico} + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
  $$

* Como ves, se penaliza tener coeficientes grandes. Por eso, el modelo prefiere hacerlos peque√±os, **a menos que realmente sean necesarios para predecir bien**.

## üìä Resultados de la Experimentaci√≥n con Regularizaci√≥n L2 (Œª)
A continuaci√≥n se presentan los resultados obtenidos al aplicar regularizaci√≥n L2 a dos modelos: Regresi√≥n Lineal y Regresi√≥n Log√≠stica. Se analizaron los cambios en los coeficientes $\theta_j$ al variar el par√°metro de regularizaci√≥n $\lambda$, observando c√≥mo esto afecta tanto al entrenamiento como a la simplicidad del modelo.

üìå A. Modelo de Regresi√≥n Lineal
1. Estabilidad del Intercepto Œ∏‚ÇÄ
En este experimento, aunque el gr√°fico "Coeficientes theta.png" muestra √∫nicamente los coeficientes $\theta_1$ a $\theta_8$, al inspeccionar los arrays completos de theta_calculado, se observ√≥ que el valor de $\theta_0$ (el intercepto) se mantuvo relativamente estable.



Esto es coherente con la teor√≠a de la regularizaci√≥n L2, ya que $\theta_0$ no es penalizado en la funci√≥n de coste. Por lo tanto, su valor no se ve afectado significativamente por el aumento de $\lambda$.

1. Encogimiento de los Coeficientes $\theta_1$ a $\theta_8$
Del gr√°fico "Coeficientes theta.png", se observa que todos los coeficientes disminuyen en magnitud a medida que aumenta $\lambda$:

Por ejemplo, $\theta_1$ (l√≠nea azul) comienza en aproximadamente 0.81 cuando $\lambda$ es bajo, y se reduce hasta casi 0.05 cuando $\lambda = 1000$.

Otros coeficientes, como $\theta_5$ o $\theta_7$, bajan incluso m√°s r√°pido y tienden m√°s r√°pidamente a cero.

Este comportamiento refleja el efecto cl√°sico del "shrinkage" (encogimiento): la regularizaci√≥n L2 penaliza los coeficientes grandes, empuj√°ndolos hacia cero.
Esto sugiere que el modelo considera que algunas variables son menos importantes para la predicci√≥n, y por tanto sus $\theta_j$ son reducidos con m√°s fuerza. En otras palabras, el modelo autom√°ticamente "selecciona" cu√°les caracter√≠sticas conservar y cu√°les descartar, aunque en Ridge nunca llegan exactamente a cero.

üìå B. Modelo de Regresi√≥n Log√≠stica
1. Estabilidad del Intercepto Œ∏‚ÇÄ
De forma similar al modelo lineal, se observ√≥ que el valor de $\theta_0$ en la regresi√≥n log√≠stica se mantiene estable a pesar del aumento en $\lambda$:

Para $\lambda = 0$, $\theta_0 \approx 1.35$

Para $\lambda = 1000$, $\theta_0 \approx 1.31$

Esto nuevamente es esperado, ya que $\theta_0$ no es penalizado en la regularizaci√≥n L2.

2. Encogimiento de los Coeficientes $\theta_1$ a $\theta_8$
El gr√°fico "Coeficientes theta en funci√≥n del lambda.png" muestra c√≥mo los primeros 8 coeficientes disminuyen al aumentar $\lambda$.
Se decidi√≥ graficar solo estos primeros 8 de los 30 coeficientes disponibles en el dataset del c√°ncer de mama para facilitar la visualizaci√≥n.

Tendencia observada:

Cuando $\lambda$ es bajo, algunos $\theta_j$ comienzan con valores relativamente altos (entre 0.3 y 0.7).

A medida que $\lambda$ aumenta, todos estos coeficientes tienden hacia cero, aunque no todos con la misma rapidez.

A diferencia del modelo lineal, en este gr√°fico el encogimiento parece m√°s abrupto para algunos coeficientes espec√≠ficos, lo que puede deberse a que el modelo log√≠stico es m√°s sensible a la regularizaci√≥n por la naturaleza de su funci√≥n de coste (log loss).

### üîÑ Comparaci√≥n entre los dos modelos
Aunque ambos modelos usan el mismo principio de regularizaci√≥n L2, sus gr√°ficos de coeficientes lucen diferentes debido a varios factores:

Tipo de modelo: Lineal vs Log√≠stico.

Cantidad y tipo de variables: El modelo lineal us√≥ un dataset m√°s peque√±o con 8 caracter√≠sticas, mientras que el log√≠stico trabaj√≥ con 30.

Magnitud de los coeficientes: En la regresi√≥n log√≠stica, los coeficientes son m√°s peque√±os desde el inicio, lo que hace que el efecto visual del encogimiento sea m√°s notorio o abrupto.

Ambos modelos muestran el mismo comportamiento esencial:

A mayor $\lambda$, mayor penalizaci√≥n, lo que lleva a coeficientes m√°s peque√±os y, por ende, a modelos m√°s simples.

 ### A. Modelo de Regresi√≥n Lineal ‚Äî "Coeficientes theta.png"
üîπ 1. Estabilidad del Intercepto Œ∏‚ÇÄ
Aunque el gr√°fico "Coeficientes theta.png" muestra √∫nicamente los coeficientes Œ∏‚ÇÅ a Œ∏‚Çà, al observar los arrays de theta_calculado, se vio que el valor del intercepto Œ∏‚ÇÄ se mantuvo estable en todos los valores de Œª.
Por ejemplo, para Œª = 0, Œ∏‚ÇÄ ‚âà 2.07, y para Œª = 1000, Œ∏‚ÇÄ ‚âà 2.02.
Esto es coherente con lo esperado, ya que el intercepto no es penalizado por la regularizaci√≥n L2, por lo tanto no se ve afectado por el aumento de Œª.
üîπ 2. Encogimiento de los coeficientes Œ∏‚ÇÅ a Œ∏‚Çà
En el gr√°fico se observa c√≥mo los coeficientes disminuyen (efecto "shrinkage") al aumentar Œª.
Por ejemplo, el coeficiente Œ∏‚ÇÅ (l√≠nea azul) comienza en aproximadamente 0.81 cuando Œª es peque√±o, y disminuye hasta alrededor de 0.25 cuando Œª = 1000.

En contraste, coeficientes como Œ∏‚ÇÖ (l√≠nea morada) comienzan ya cerca de 0.05 y se mantienen pr√°cticamente planos, lo que indica que esa caracter√≠stica tiene una importancia muy baja y el modelo tiende a descartarla r√°pidamente.

Œ∏‚Çá (l√≠nea rosa) presenta una reducci√≥n m√°s pronunciada, bajando de aproximadamente 0.5 a casi 0.1, lo cual muestra que es una caracter√≠stica moderadamente importante, pero que pierde peso a medida que el modelo se simplifica.

En general, los coeficientes m√°s relevantes resisten m√°s el encogimiento, mientras que los menos informativos tienden r√°pidamente hacia cero. Esto ilustra c√≥mo la regularizaci√≥n act√∫a como un filtro autom√°tico de caracter√≠sticas.

### üìå B. Modelo de Regresi√≥n Log√≠stica ‚Äî "Coeficientes ttheta en funcion del lmbda.png"
üîπ 1. Estabilidad del Intercepto Œ∏‚ÇÄ
Para el modelo de regresi√≥n log√≠stica, tambi√©n se observ√≥ que el intercepto Œ∏‚ÇÄ se mantuvo estable.
Por ejemplo, para Œª = 0, Œ∏‚ÇÄ ‚âà 1.35, y para Œª = 1000, Œ∏‚ÇÄ ‚âà 1.31.
Al igual que en la regresi√≥n lineal, esto es esperable porque el intercepto no es penalizado por la regularizaci√≥n L2.

üîπ 2. Encogimiento de los coeficientes Œ∏‚ÇÅ a Œ∏‚Çà
En este gr√°fico se muestran solo los primeros 8 coeficientes (de un total de 30 del dataset de c√°ncer de mama), por claridad visual.

Cuando Œª es peque√±o (a la izquierda del gr√°fico, valores de log(Œª) cercanos a -3), los coeficientes Œ∏‚ÇÅ a Œ∏‚Çà toman valores entre aproximadamente -0.4 y -0.1, es decir, la mayor√≠a empiezan en valores negativos moderados.

A medida que Œª aumenta, todos los coeficientes disminuyen su magnitud y tienden hacia cero, mostrando el cl√°sico efecto de "shrinkage".
Algunos, como **Œ∏‚ÇÉ o Œ∏



# Resumen: Diagn√≥stico y Control del Modelo ‚Äì Overfitting, Underfitting y Regularizaci√≥n

El objetivo principal al entrenar un modelo de Machine Learning es que **generalice bien** a datos nuevos, no vistos durante el entrenamiento. Dos problemas comunes que impiden esto son el overfitting y el underfitting, ambos relacionados con el **compromiso sesgo-varianza**. La **regularizaci√≥n** es una t√©cnica clave para controlar estos problemas, especialmente el overfitting.

## 1. Overfitting (Sobreajuste)

### ¬øQu√© es?
Ocurre cuando un modelo aprende *demasiado bien* los datos de entrenamiento, capturando no solo los patrones generales sino tambi√©n el **ruido, errores o particularidades** de ese conjunto espec√≠fico. Como resultado, **memoriza** en lugar de *entender*, llevando a un excelente rendimiento en los datos de entrenamiento pero un **bajo rendimiento en datos nuevos**.

### Caracter√≠sticas:
- Error de entrenamiento: Muy bajo.
- Error de validaci√≥n/prueba: Alto.
- Implica **Alta Varianza**: el modelo es muy sensible a peque√±as fluctuaciones en los datos de entrenamiento.

### Causas Comunes:
- **Modelo demasiado complejo** en relaci√≥n a la cantidad/simplicidad de los datos (e.g., un polinomio de alto grado para datos lineales).
- **Pocos datos de entrenamiento**: insuficientes para aprender patrones generalizables.
- **Ruido en los datos**: el modelo intenta ajustarse a estos errores.
- **Entrenamiento excesivo**: el modelo empieza a memorizar tras muchas iteraciones.

## 2. Underfitting (Subajuste)

### ¬øQu√© es?
Ocurre cuando un modelo es **demasiado simple** para captar la complejidad real de los datos. No aprende bien los patrones ni siquiera de los datos de entrenamiento.

### Caracter√≠sticas:
- Error de entrenamiento: Alto.
- Error de validaci√≥n/prueba: Alto.
- Implica **Alto Sesgo**: el modelo hace suposiciones demasiado simplistas sobre los datos.

### Causas Comunes:
- **Modelo demasiado simple** (e.g., una l√≠nea recta para datos con forma curva).
- **Datos de entrada (features) poco informativos**.
- **Entrenamiento insuficiente**: el modelo no tuvo tiempo de aprender.

## 3. El Compromiso Sesgo-Varianza (Bias-Variance Tradeoff)

Este es un concepto fundamental que describe el equilibrio necesario entre dos fuentes de error:

-   **Sesgo (Bias):** Error debido a suposiciones incorrectas o demasiado simplistas en el modelo. Un alto sesgo causa underfitting.
-   **Varianza (Variance):** Error debido a la sensibilidad del modelo a las peque√±as fluctuaciones en los datos de entrenamiento. Una alta varianza causa overfitting.

El **error total** de un modelo se puede descomponer (te√≥ricamente) como:
$$Error_{total} = Sesgo^2 + Varianza + Error\ Irreducible$$
Donde el **Error Irreducible** es el ruido inherente al problema que ning√∫n modelo puede eliminar.

-   Reducir el sesgo (aumentando la complejidad del modelo) tiende a aumentar la varianza.
-   Reducir la varianza (simplificando el modelo o con m√°s datos) tiende a aumentar el sesgo (si se simplifica demasiado).
-   **Objetivo:** Encontrar el "sweet spot" o punto √≥ptimo donde el error total en datos no vistos es m√≠nimo, logrando un buen equilibrio.

### Diagn√≥stico:
Se compara el error del modelo en el conjunto de entrenamiento y en el de validaci√≥n/prueba:
-   **Alto Sesgo (Underfitting):** Error de entrenamiento alto, error de validaci√≥n alto (similares).
-   **Alta Varianza (Overfitting):** Error de entrenamiento bajo, error de validaci√≥n alto (gran diferencia).
-   **Buen Equilibrio:** Error de entrenamiento bajo, error de validaci√≥n bajo (y cercano al de entrenamiento).

Las **curvas de aprendizaje** (error vs. tama√±o del conjunto de entrenamiento o vs. complejidad del modelo) tambi√©n ayudan a visualizar estos problemas.

## 4. Estrategias para Combatir Underfitting y Overfitting

| Problema                        | Estrategias para Combatir                                                                                                                                                                                                                                                           |
| :------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Underfitting** (Alto Sesgo)   | 1. **Aumentar la complejidad del modelo** (polinomios de mayor grado, redes neuronales m√°s profundas/anchas).<br>2. **Ingenier√≠a de caracter√≠sticas** (a√±adir features relevantes, crear interacciones).<br>3. **Entrenar m√°s tiempo** (m√°s √©pocas/iteraciones).<br>4. **Reducir la regularizaci√≥n** (si se est√° aplicando de forma excesiva). |
| **Overfitting** (Alta Varianza) | 1. **Regularizaci√≥n** (L1, L2 - ver abajo).<br>2. **Conseguir m√°s datos de entrenamiento**.<br>3. **Reducir la complejidad del modelo** (modelos m√°s simples, menos features).<br>4. **Selecci√≥n de caracter√≠sticas / Reducci√≥n de dimensionalidad** (PCA).<br>5. **Early Stopping** (detener el entrenamiento cuando el error de validaci√≥n empieza a subir).<br>6. **M√©todos de Ensamblaje** (Bagging como Random Forests, Boosting como XGBoost).<br>7. **Aumento de Datos (Data Augmentation)**. |

## 5. Regularizaci√≥n L2 (Ridge Regression)

La regularizaci√≥n es una t√©cnica para prevenir el overfitting al a√±adir un t√©rmino de penalizaci√≥n a la funci√≥n de coste del modelo. Esta penalizaci√≥n desalienta que los par√°metros (coeficientes $\theta_j$) del modelo tomen valores demasiado grandes.

### Funci√≥n de Coste con Regularizaci√≥n L2:
Se modifica la funci√≥n de coste original (e.g., Error Cuadr√°tico Medio para regresi√≥n lineal) a√±adiendo el t√©rmino de regularizaci√≥n:
$$J(\theta) = \underbrace{\frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2}_{\text{Coste Original (MSE)}} + \underbrace{\frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2}_{\text{T√©rmino de Regularizaci√≥n L2}}$$
Donde:
-   $m$: n√∫mero de ejemplos de entrenamiento.
-   $h_\theta(x^{(i)})$: predicci√≥n del modelo para el ejemplo $i$.
-   $y^{(i)}$: valor real para el ejemplo $i$.
-   $\theta_j$: par√°metros (coeficientes) del modelo.
-   $\lambda$ (lambda): **par√°metro de regularizaci√≥n**. Controla la magnitud de la penalizaci√≥n. Un $\lambda$ m√°s alto implica una penalizaci√≥n mayor.
-   La suma del t√©rmino de regularizaci√≥n $\sum_{j=1}^{n} \theta_j^2$ va desde $j=1$ hasta $n$. **Importante: $\theta_0$ (el t√©rmino de intercepci√≥n o sesgo del modelo) no se penaliza.**

### ¬øPor qu√© no se penaliza $\theta_0$?
$\theta_0$ representa el desplazamiento base del modelo y no est√° asociado a la complejidad de c√≥mo se ajusta a las caracter√≠sticas de entrada. Penalizarlo no ayuda a controlar el overfitting y podr√≠a llevar a un ajuste sub√≥ptimo.

### Actualizaci√≥n del Gradiente con Regularizaci√≥n L2 (para Descenso de Gradiente):
La actualizaci√≥n para cada $\theta_j$ (donde $j \ge 1$) se modifica:
$$\theta_j := \theta_j - \alpha \left[ \left( \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \right) + \frac{\lambda}{m} \theta_j \right]$$
Para $\theta_0$ (si $j=0$):
$$\theta_0 := \theta_0 - \alpha \left[ \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \right]$$
(Asumiendo $x_0^{(i)} = 1$. El t√©rmino $\frac{\lambda}{m} \theta_j$ no se aplica a $\theta_0$).
El efecto es que, en cada paso, $\theta_j$ (para $j \ge 1$) se reduce proporcionalmente a su valor actual, adem√°s del ajuste por el error ("shrinkage" o encogimiento).

### Efecto de $\lambda$:
-   **Si $\lambda = 0$:** No hay regularizaci√≥n. La funci√≥n de coste es la original. Alto riesgo de overfitting si el modelo es complejo.
-   **Si $\lambda > 0$:** Los coeficientes $\theta_j$ (para $j \ge 1$) tienden a disminuir. Cuanto mayor es $\lambda$, m√°s peque√±os se vuelven los coeficientes, llevando a un modelo m√°s simple.
-   **Si $\lambda$ es muy grande:** Puede llevar a underfitting, ya que los coeficientes pueden volverse demasiado peque√±os, perdiendo informaci√≥n √∫til.
-   El valor de $\theta_0$ se mantiene relativamente estable ya que no es afectado por la penalizaci√≥n de $\lambda$.

## 6. Interpretaci√≥n de Resultados Experimentales con Regularizaci√≥n

### Gr√°ficos de Coeficientes ($\theta_j$) vs. $\lambda$:
-   Muestran que a medida que $\lambda$ aumenta, la magnitud de los coeficientes $\theta_j$ (para $j \ge 1$) disminuye, tendiendo hacia cero.
-   $\theta_0$ (intercepto) permanece relativamente constante.
-   Coeficientes asociados a caracter√≠sticas menos relevantes tienden a disminuir m√°s r√°pidamente.

### Gr√°ficos de Coste vs. Iteraciones (para diferentes $\lambda$):
-   Permiten verificar la convergencia del entrenamiento para cada valor de $\lambda$.
-   El coste de entrenamiento final con $\lambda > 0$ suele ser un poco m√°s alto que con $\lambda = 0$, ya que el modelo no se ajusta tan agresivamente al ruido del entrenamiento.
-   Un $\lambda$ √≥ptimo es aquel que minimiza el error en un conjunto de validaci√≥n, no necesariamente el que da el menor coste de entrenamiento.

---

## üìå ¬øQu√© hace `train_test_split` en regresi√≥n lineal?

Divide tus datos en:

* **Conjunto de entrenamiento (`X_train`, `y_train`)** ‚Üí se usa para ajustar los coeficientes del modelo (las "Œ∏").
* **Conjunto de prueba (`X_test`, `y_test`)** ‚Üí se usa para evaluar c√≥mo de bien generaliza el modelo a datos nuevos.

Esto evita que eval√∫es el modelo sobre los mismos datos con los que fue entrenado, lo que podr√≠a ocultar problemas como el **overfitting**.

---

## üìä M√©tricas comunes en regresi√≥n lineal

En vez de precisi√≥n o matriz de confusi√≥n (que son para clasificaci√≥n), usamos m√©tricas que miden **error num√©rico**:

### 1. **Mean Squared Error (MSE)**

Promedio del cuadrado de los errores:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2
$$

M√°s sensible a errores grandes.

---

### 2. **Mean Absolute Error (MAE)**

Promedio del valor absoluto de los errores:

$$
MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

Menos sensible a valores extremos.

---

### 3. **R¬≤ Score (Coeficiente de determinaci√≥n)**

Indica qu√© tan bien se explica la variaci√≥n de los datos con el modelo (0 a 1, donde 1 es perfecto).

$$
R^2 = 1 - \frac{SSE}{SST}
$$

---

## üí¨ Entonces, en resumen:

> ‚ÄúEn regresi√≥n lineal, `train_test_split` permite evaluar el modelo en un conjunto independiente usando m√©tricas como MSE, MAE, RMSE y R¬≤, lo cual es esencial para validar su capacidad de generalizaci√≥n.‚Äù

---

## Implementaci√≥n de `train_test_split` en regresi√≥n log√≠stica

Gracias a la funci√≥n `train_test_split`, podemos evaluar el rendimiento real del modelo con m√©tricas de clasificaci√≥n confiables, al medirlo sobre un conjunto de prueba independiente.

Cuando entrenas un modelo, necesitas evaluar su desempe√±o en datos nuevos, no solo en los que ha visto (entrenado). Si evaluaras el modelo en los mismos datos de entrenamiento:

* Obtendr√≠as una visi√≥n irrealmente optimista del rendimiento del modelo.
* Podr√≠as estar midiendo **overfitting** (el modelo aprendi√≥ los datos de memoria).

`train_test_split` separa el conjunto original en:

* `X_train`, `y_train`: para entrenar el modelo.
* `X_test`, `y_test`: para evaluar el modelo.

üß† **¬øQu√© permite esta divisi√≥n?**
Gracias a ella, puedes:

* Medir c√≥mo generaliza el modelo a datos que no ha visto.
* Calcular m√©tricas como:
  ‚úÖ **Accuracy**
  üì¶ **Matriz de Confusi√≥n**
  üéØ **Precision**
  üîç **Recall**
  ‚öñÔ∏è **F1 Score**

---

## üßÆ 1. Matriz de Confusi√≥n

Es una tabla que resume c√≥mo de bien ha hecho las predicciones un modelo de clasificaci√≥n, bas√°ndose en las etiquetas verdaderas y las predichas.

### üì¶ Estructura (para clasificaci√≥n binaria):

|                        | Predicho: 0 | Predicho: 1 |
| ---------------------- | ----------- | ----------- |
| **Real: 0** (Negativo) | TN (‚úî)      | FP (‚ùå)      |
| **Real: 1** (Positivo) | FN (‚ùå)      | TP (‚úî)      |

* **TP** (True Positives): casos positivos predichos correctamente.
* **TN** (True Negatives): casos negativos predichos correctamente.
* **FP** (False Positives): negativos clasificados err√≥neamente como positivos.
* **FN** (False Negatives): positivos clasificados err√≥neamente como negativos.

---

## üìä 2. Accuracy (Exactitud)

Es el porcentaje de predicciones correctas (positivas y negativas).

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

üü¢ √ötil cuando las clases est√°n **balanceadas** (mismo n√∫mero de positivos y negativos).

---

## üéØ 3. Precision (Precisi√≥n)

Mide cu√°ntos de los casos que predijiste como positivos **realmente lo son**.

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

‚úÖ √ötil cuando los **falsos positivos son costosos** (por ejemplo, diagnosticar err√≥neamente una enfermedad grave).

---

## üîç 4. Recall (Sensibilidad o Tasa de Verdaderos Positivos)

Mide cu√°ntos de los casos **positivos reales** has detectado correctamente.

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

‚úÖ Muy importante cuando los **falsos negativos son peligrosos** (por ejemplo, no detectar un c√°ncer cuando s√≠ lo hay).

---

## ‚öñÔ∏è 5. F1 Score

Es el **promedio arm√≥nico** de precisi√≥n y recall. Equilibra ambos.

$$
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

‚úÖ √ötil cuando quieres un equilibrio entre precisi√≥n y recall, especialmente importante con clases desbalanceadas.

---

## üß† Ejemplo sencillo:

Sup√≥n que:

* Tienes 100 pacientes.
* 10 tienen c√°ncer (positivos), 90 no (negativos).
* Tu modelo detecta 8 de esos 10 correctamente, pero se equivoca en 5 casos sin c√°ncer (falsos positivos).

Entonces:

* TP = 8
* FN = 2
* FP = 5
* TN = 85

Ahora puedes calcular:

* Accuracy = (8 + 85) / 100 = 93%
* Precision = 8 / (8 + 5) = 61.5%
* Recall = 8 / (8 + 2) = 80%
* F1 Score ‚âà 69.6%

---

### ‚úÖ Tabla comparativa de modelos de regresi√≥n

| Modelo                                     | Dataset            | MSE (Test) | MAE (Test) | R¬≤ (Test)  | Hiperpar√°metros / Notas                   |
| ------------------------------------------ | ------------------ | ---------- | ---------- | ---------- | ----------------------------------------- |
| Regresi√≥n Lineal (Descenso Gradiente)      | California Housing | 0.5559     | 0.5332     | 0.5758     | Œ±=0.1, Iter=2500, Œª=0.01, Datos escalados |
| Regresi√≥n Lineal (Ecuaci√≥n Normal)         | California Housing | 0.5559     | 0.5332     | 0.5758     | Datos escalados                           |
| SVR (scikit-learn, kernel='linear', C=0.1) | California Housing | 0.5795     | 0.5123     | 0.5578     | C=0.1, Datos escalados                    |
| SVR (scikit-learn, kernel='rbf', C=3, Œ≥=1) | California Housing | **0.3127** | **0.3711** | **0.7614** | Mejor resultado. Datos escalados          |

---

### üèÜ Mejor modelo: **SVR con kernel RBF (C=3, Œ≥=1)**

* Supera ampliamente a la regresi√≥n lineal en MSE, MAE y R¬≤.
* Presenta un muy buen ajuste sin indicios evidentes de overfitting.
* Requiere mayor tiempo de c√≥mputo, justificado por la mejora en desempe√±o.

---

### Interpretaci√≥n de Resultados - Modelos de Regresi√≥n

En este an√°lisis sobre el dataset **California Housing**, comparamos cuatro enfoques para regresi√≥n:

* **Regresi√≥n Lineal (Descenso Gradiente y Ecuaci√≥n Normal):**
  Ambos m√©todos ofrecen resultados pr√°cticamente id√©nticos, con MSE ‚âà 0.556, MAE ‚âà 0.533 y un coeficiente de determinaci√≥n \$R^2\$ ‚âà 0.58. Esto refleja un rendimiento moderado, adecuado para una predicci√≥n lineal b√°sica pero con espacio para mejoras.

* **SVR con Kernel Lineal:**
  Este modelo muestra un desempe√±o comparable, ligeramente inferior en \$R^2\$ (0.56) y errores similares (MSE ‚âà 0.58, MAE ‚âà 0.51). Indica que la inclusi√≥n del margen suave del SVM con kernel lineal no aporta una ventaja significativa frente a la regresi√≥n lineal tradicional.

* **SVR con Kernel RBF (Mejor Modelo):**
  El modelo con kernel RBF logra un desempe√±o notablemente superior: reduce el MSE a 0.313, el MAE a 0.371 y aumenta el \$R^2\$ a 0.76. Esto evidencia que captura relaciones no lineales presentes en los datos, mejorando considerablemente la precisi√≥n sin sobreajustar.

**Conclusi√≥n:**
El SVR con kernel RBF es el modelo m√°s efectivo para este problema, demostrando la importancia de considerar modelos no lineales cuando la relaci√≥n entre variables no es simple. El mayor costo computacional se justifica por la mejora significativa en la capacidad predictiva.

---



| Modelo                | Dataset       | Accuracy (Test) | Precision (Test) | Recall (Test) | F1-Score (Test) | Par√°metros Clave                  |
| --------------------- | ------------- | --------------- | ---------------- | ------------- | --------------- | --------------------------------- |
| Regresi√≥n Log√≠stica   | Breast Cancer | 0.9649          | 0.9718           | 0.9718        | 0.9718          | alpha=0.1, iter=2500, lambda=0.01 |
| SVC (kernel='linear') | Breast Cancer | 0.9649          | 0.9589           | 0.9859        | 0.9722          | C=0.1                             |


---

## Interpretaci√≥n de Resultados: Modelos de Clasificaci√≥n en Breast Cancer

| M√©trica   | Regresi√≥n Log√≠stica (Test) | SVC (Test) |
| --------- | -------------------------- | ---------- |
| Accuracy  | 96.49%                     | 96.49%     |
| Precision | 97.18%                     | 95.89%     |
| Recall    | 97.18%                     | 98.59%     |
| F1-Score  | 97.18%                     | 97.22%     |

---

### 1. Exactitud (Accuracy)

Ambos modelos obtienen una exactitud id√©ntica (96.49%), lo que significa que los dos clasifican correctamente la mayor√≠a de las muestras del conjunto de prueba.

### 2. Precisi√≥n (Precision)

* **Regresi√≥n Log√≠stica** presenta una precisi√≥n ligeramente mayor (97.18%) que SVC (95.89%).
* Esto indica que cuando la regresi√≥n log√≠stica predice un caso positivo (por ejemplo, tumor maligno), lo hace con mayor confianza y menos falsos positivos.

### 3. Sensibilidad / Recall

* **SVC** tiene un recall mayor (98.59% vs 97.18%), lo que indica que detecta un porcentaje m√°s alto de casos positivos reales.
* Esto es especialmente importante en diagn√≥sticos m√©dicos, donde es preferible minimizar falsos negativos.

### 4. F1-Score

* Ambos modelos tienen un F1-score muy alto y casi igual, indicando un excelente balance entre precisi√≥n y recall.
* SVC tiene una ligera ventaja con 97.22% frente al 97.18% de regresi√≥n log√≠stica, aunque la diferencia es m√≠nima.

---
Claro, aqu√≠ tienes una interpretaci√≥n detallada para la tabla de regresi√≥n, siguiendo el estilo de la interpretaci√≥n que hice para clasificaci√≥n:

---




### Conclusi√≥n general:

* **Ambos modelos funcionan excepcionalmente bien para el problema de clasificaci√≥n del c√°ncer de mama.**
* Si priorizas detectar todos los casos positivos (menos falsos negativos), **SVC es la mejor opci√≥n** gracias a su mayor recall.
* Si te importa m√°s evitar falsos positivos (mayor precisi√≥n), la **regresi√≥n log√≠stica es ligeramente mejor**.
* La decisi√≥n final puede depender del contexto cl√≠nico y del costo asociado a los errores de clasificaci√≥n.

---



## Resumen de  M√©tricas de Evaluaci√≥n en Clasificaci√≥n Binaria: Precision, Recall y F1-Score

| M√©trica   | Se enfoca en...                           | Qu√© quiere evitar                 |
| --------- | ----------------------------------------- | --------------------------------- |
| Precision | Predicciones positivas correctas          | Falsos positivos                  |
| Recall    | Positivos reales correctamente detectados | Falsos negativos                  |
| F1        | Balance entre Precision y Recall          | Cuando uno de los dos falla mucho |
